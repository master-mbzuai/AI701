{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is for creating dictionary of mapping between fine labels ranging from 0 to 99 to labels from 0 to 4\n",
    "# first I divide by parent category, then in each of the 20 parents I map fine labels from 0 to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets.utils import download_url, check_integrity\n",
    "import torchvision.transforms as tt\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "\n",
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "\n",
    "    meta = {\n",
    "        \"filename\": \"batches.meta\",\n",
    "        \"key\": \"label_names\",\n",
    "        \"md5\": \"5ff9c542aee3614f3951f8cda6e48888\",\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False, coarse=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "        self.coarse = coarse\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        if self.train:\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            self.train_coarse_labels = []\n",
    "            for fentry in self.train_list:\n",
    "                f = fentry[0]\n",
    "                file = os.path.join(self.root, self.base_folder, f)\n",
    "                fo = open(file, 'rb')\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(fo)\n",
    "                else:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                self.train_data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.train_labels += entry['labels']\n",
    "                else:\n",
    "                    self.train_labels += entry['fine_labels']\n",
    "                    if self.coarse:\n",
    "                        self.train_coarse_labels += entry['coarse_labels']\n",
    "                fo.close()\n",
    "\n",
    "            self.train_data = np.concatenate(self.train_data)\n",
    "            self.train_data = self.train_data.reshape((50000, 3, 32, 32))\n",
    "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "        else:\n",
    "            f = self.test_list[0][0]\n",
    "            file = os.path.join(self.root, self.base_folder, f)\n",
    "            fo = open(file, 'rb')\n",
    "            if sys.version_info[0] == 2:\n",
    "                entry = pickle.load(fo)\n",
    "            else:\n",
    "                entry = pickle.load(fo, encoding='latin1')\n",
    "            self.test_data = entry['data']\n",
    "\n",
    "            if 'labels' in entry:\n",
    "                self.test_labels = entry['labels']\n",
    "            else:\n",
    "                self.test_labels = entry['fine_labels']\n",
    "                if self.coarse:\n",
    "                    self.test_coarse_labels = entry['coarse_labels']\n",
    "            fo.close()\n",
    "            self.test_data = self.test_data.reshape((10000, 3, 32, 32))\n",
    "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "            if self.coarse:\n",
    "                coarse_target = self.train_coarse_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "            if self.coarse:\n",
    "                coarse_target = self.test_coarse_labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        if not self.coarse:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img, target, coarse_target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        root = self.root\n",
    "        download_url(self.url, root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        cwd = os.getcwd()\n",
    "        tar = tarfile.open(os.path.join(root, self.filename), \"r:gz\")\n",
    "        os.chdir(root)\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        os.chdir(cwd)\n",
    "\n",
    "\n",
    "class CIFAR100(CIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tt.Compose([tt.RandomCrop(32, padding=4,padding_mode='reflect'), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         #tt.Normalize(mean,std,inplace=True) \n",
    "                         ]\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = CIFAR100('./data', train=True,\n",
    "                 transform=transform_train,\n",
    "                 download=True, coarse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "100\n",
      "dict_keys([11, 15, 4, 14, 1, 5, 18, 3, 10, 17, 2, 9, 8, 16, 6, 12, 19, 7, 13, 0])\n"
     ]
    }
   ],
   "source": [
    "demo_loader = torch.utils.data.DataLoader(train_data, batch_size=1200)\n",
    "\n",
    "demo_batch = next(iter(demo_loader))\n",
    "img, fine_labels, course_labels = demo_batch\n",
    "fine_labels=fine_labels.numpy()\n",
    "course_labels=course_labels.numpy()\n",
    "\n",
    "parent_to_child_class={}\n",
    "\n",
    "for fine, coarse in zip(fine_labels, course_labels):\n",
    "    if coarse not in parent_to_child_class:\n",
    "        parent_to_child_class[coarse]=[fine]\n",
    "    elif fine not in parent_to_child_class[coarse]:\n",
    "        parent_to_child_class[coarse].append(fine)\n",
    "        parent_to_child_class[coarse].sort()\n",
    "\n",
    "print(len(parent_to_child_class.keys()))\n",
    "\n",
    "count=0\n",
    "for x in parent_to_child_class.values():\n",
    "    count+=len(x)\n",
    "print(count)\n",
    "\n",
    "print(parent_to_child_class.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(11, [15, 19, 21, 31, 38]), (15, [27, 29, 44, 78, 93]), (4, [0, 51, 53, 57, 83]), (14, [2, 11, 35, 46, 98]), (1, [1, 32, 67, 73, 91]), (5, [22, 39, 40, 86, 87]), (18, [8, 13, 48, 58, 90]), (3, [9, 10, 16, 28, 61]), (10, [23, 33, 49, 60, 71]), (17, [47, 52, 56, 59, 96]), (2, [54, 62, 70, 82, 92]), (9, [12, 17, 37, 68, 76]), (8, [3, 42, 43, 88, 97]), (16, [36, 50, 65, 74, 80]), (6, [5, 20, 25, 84, 94]), (12, [34, 63, 64, 66, 75]), (19, [41, 69, 81, 85, 89]), (7, [6, 7, 14, 18, 24]), (13, [26, 45, 77, 79, 99]), (0, [4, 30, 55, 72, 95])])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_to_child_class.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 30, 55, 72, 95]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_to_child_class[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[15, 19, 21, 31, 38]\n",
      "Original values list is : [15, 19, 21, 31, 38]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "15\n",
      "[27, 29, 44, 78, 93]\n",
      "Original values list is : [27, 29, 44, 78, 93]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "4\n",
      "[0, 51, 53, 57, 83]\n",
      "Original values list is : [0, 51, 53, 57, 83]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "14\n",
      "[2, 11, 35, 46, 98]\n",
      "Original values list is : [2, 11, 35, 46, 98]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "1\n",
      "[1, 32, 67, 73, 91]\n",
      "Original values list is : [1, 32, 67, 73, 91]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "5\n",
      "[22, 39, 40, 86, 87]\n",
      "Original values list is : [22, 39, 40, 86, 87]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "18\n",
      "[8, 13, 48, 58, 90]\n",
      "Original values list is : [8, 13, 48, 58, 90]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "3\n",
      "[9, 10, 16, 28, 61]\n",
      "Original values list is : [9, 10, 16, 28, 61]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "10\n",
      "[23, 33, 49, 60, 71]\n",
      "Original values list is : [23, 33, 49, 60, 71]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "17\n",
      "[47, 52, 56, 59, 96]\n",
      "Original values list is : [47, 52, 56, 59, 96]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "2\n",
      "[54, 62, 70, 82, 92]\n",
      "Original values list is : [54, 62, 70, 82, 92]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "9\n",
      "[12, 17, 37, 68, 76]\n",
      "Original values list is : [12, 17, 37, 68, 76]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "8\n",
      "[3, 42, 43, 88, 97]\n",
      "Original values list is : [3, 42, 43, 88, 97]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "16\n",
      "[36, 50, 65, 74, 80]\n",
      "Original values list is : [36, 50, 65, 74, 80]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "6\n",
      "[5, 20, 25, 84, 94]\n",
      "Original values list is : [5, 20, 25, 84, 94]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "12\n",
      "[34, 63, 64, 66, 75]\n",
      "Original values list is : [34, 63, 64, 66, 75]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "19\n",
      "[41, 69, 81, 85, 89]\n",
      "Original values list is : [41, 69, 81, 85, 89]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "7\n",
      "[6, 7, 14, 18, 24]\n",
      "Original values list is : [6, 7, 14, 18, 24]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "13\n",
      "[26, 45, 77, 79, 99]\n",
      "Original values list is : [26, 45, 77, 79, 99]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "0\n",
      "[4, 30, 55, 72, 95]\n",
      "Original values list is : [4, 30, 55, 72, 95]\n",
      "New value list is : [0, 1, 2, 3, 4]\n",
      "{11: {15: 0, 19: 1, 21: 2, 31: 3, 38: 4}, 15: {27: 0, 29: 1, 44: 2, 78: 3, 93: 4}, 4: {0: 0, 51: 1, 53: 2, 57: 3, 83: 4}, 14: {2: 0, 11: 1, 35: 2, 46: 3, 98: 4}, 1: {1: 0, 32: 1, 67: 2, 73: 3, 91: 4}, 5: {22: 0, 39: 1, 40: 2, 86: 3, 87: 4}, 18: {8: 0, 13: 1, 48: 2, 58: 3, 90: 4}, 3: {9: 0, 10: 1, 16: 2, 28: 3, 61: 4}, 10: {23: 0, 33: 1, 49: 2, 60: 3, 71: 4}, 17: {47: 0, 52: 1, 56: 2, 59: 3, 96: 4}, 2: {54: 0, 62: 1, 70: 2, 82: 3, 92: 4}, 9: {12: 0, 17: 1, 37: 2, 68: 3, 76: 4}, 8: {3: 0, 42: 1, 43: 2, 88: 3, 97: 4}, 16: {36: 0, 50: 1, 65: 2, 74: 3, 80: 4}, 6: {5: 0, 20: 1, 25: 2, 84: 3, 94: 4}, 12: {34: 0, 63: 1, 64: 2, 66: 3, 75: 4}, 19: {41: 0, 69: 1, 81: 2, 85: 3, 89: 4}, 7: {6: 0, 7: 1, 14: 2, 18: 3, 24: 4}, 13: {26: 0, 45: 1, 77: 2, 79: 3, 99: 4}, 0: {4: 0, 30: 1, 55: 2, 72: 3, 95: 4}}\n"
     ]
    }
   ],
   "source": [
    "parent_to_child_class_from_0_to_5={}\n",
    "\n",
    "for parent, children in parent_to_child_class.items():\n",
    "    print(parent)\n",
    "    print(children)\n",
    "    test_keys = children.copy()\n",
    "    test_values = [0, 1, 2, 3 ,4]\n",
    "\n",
    "    # Printing original keys-value lists\n",
    "    print(\"Original values list is : \" + str(test_keys))\n",
    "    print(\"New value list is : \" + str(test_values))\n",
    "\n",
    "    # using naive method\n",
    "    # to convert lists to dictionary\n",
    "    res = {}\n",
    "    for key in test_keys:\n",
    "        for value in test_values:\n",
    "            res[int(key)] = value\n",
    "            test_values.remove(value)\n",
    "            break\n",
    "    parent_to_child_class_from_0_to_5[int(parent)]=res\n",
    "\n",
    "print(str(parent_to_child_class_from_0_to_5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{9: 0, 10: 1, 16: 2, 28: 3, 61: 4}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parent_to_child_class_from_0_to_5[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"parent_to_child_class_from_0_to_5.json\", \"w\") as outfile: \n",
    "    json.dump(parent_to_child_class_from_0_to_5, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': {'15': 0, '19': 1, '21': 2, '31': 3, '38': 4}, '15': {'27': 0, '29': 1, '44': 2, '78': 3, '93': 4}, '4': {'0': 0, '51': 1, '53': 2, '57': 3, '83': 4}, '14': {'2': 0, '11': 1, '35': 2, '46': 3, '98': 4}, '1': {'1': 0, '32': 1, '67': 2, '73': 3, '91': 4}, '5': {'22': 0, '39': 1, '40': 2, '86': 3, '87': 4}, '18': {'8': 0, '13': 1, '48': 2, '58': 3, '90': 4}, '3': {'9': 0, '10': 1, '16': 2, '28': 3, '61': 4}, '10': {'23': 0, '33': 1, '49': 2, '60': 3, '71': 4}, '17': {'47': 0, '52': 1, '56': 2, '59': 3, '96': 4}, '2': {'54': 0, '62': 1, '70': 2, '82': 3, '92': 4}, '9': {'12': 0, '17': 1, '37': 2, '68': 3, '76': 4}, '8': {'3': 0, '42': 1, '43': 2, '88': 3, '97': 4}, '16': {'36': 0, '50': 1, '65': 2, '74': 3, '80': 4}, '6': {'5': 0, '20': 1, '25': 2, '84': 3, '94': 4}, '12': {'34': 0, '63': 1, '64': 2, '66': 3, '75': 4}, '19': {'41': 0, '69': 1, '81': 2, '85': 3, '89': 4}, '7': {'6': 0, '7': 1, '14': 2, '18': 3, '24': 4}, '13': {'26': 0, '45': 1, '77': 2, '79': 3, '99': 4}, '0': {'4': 0, '30': 1, '55': 2, '72': 3, '95': 4}}\n"
     ]
    }
   ],
   "source": [
    "f = open('parent_to_child_class_from_0_to_5.json')\n",
    "data = json.load(f)\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
