{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "epochs = 120\n",
    "max_lr = 0.001\n",
    "grad_clip = 0.01\n",
    "weight_decay =0.001\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = torchvision.datasets.CIFAR100('./', train=True, download=True)\n",
    "\n",
    "# Stick all the images together to form a 1600000 X 32 X 3 array\n",
    "x = np.concatenate([np.asarray(train_data[i][0]) for i in range(len(train_data))])\n",
    "\n",
    "# calculate the mean and std along the (0, 1) axes\n",
    "mean = np.mean(x, axis=(0, 1))/255\n",
    "std = np.std(x, axis=(0, 1))/255\n",
    "# the the mean and std\n",
    "mean=mean.tolist()\n",
    "std=std.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [195 205 193]\n",
      "  [212 224 204]\n",
      "  [182 194 167]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [170 176 150]\n",
      "  [161 168 130]\n",
      "  [146 154 113]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [254 254 254]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [189 199 169]\n",
      "  [166 178 130]\n",
      "  [121 133  87]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[148 185  79]\n",
      "  [142 182  57]\n",
      "  [140 179  60]\n",
      "  ...\n",
      "  [ 30  17   1]\n",
      "  [ 65  62  15]\n",
      "  [ 76  77  20]]\n",
      "\n",
      " [[122 157  66]\n",
      "  [120 155  58]\n",
      "  [126 160  71]\n",
      "  ...\n",
      "  [ 22  16   3]\n",
      "  [ 97 112  56]\n",
      "  [141 161  87]]\n",
      "\n",
      " [[ 87 122  41]\n",
      "  [ 88 122  39]\n",
      "  [101 134  56]\n",
      "  ...\n",
      "  [ 34  36  10]\n",
      "  [105 133  59]\n",
      "  [138 173  79]]]\n"
     ]
    }
   ],
   "source": [
    "print(np.asarray(train_data[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5070751592371323, 0.48654887331495095, 0.4409178433670343]\n",
      "[0.26733428587941854, 0.25643846292120615, 0.2761504713263903]\n"
     ]
    }
   ],
   "source": [
    "print(mean)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tt.Compose([tt.RandomCrop(32, padding=4,padding_mode='reflect'), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(mean,std,inplace=True)])\n",
    "transform_test = tt.Compose([tt.ToTensor(), tt.Normalize(mean,std)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR100(\"./\",\n",
    "                                         train=True,\n",
    "                                         download=True,\n",
    "                                         transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size, shuffle=True, num_workers=2,pin_memory=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR100(\"./\",\n",
    "                                        train=False,\n",
    "                                        download=True,\n",
    "                                        transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size*2,pin_memory=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainloader = DeviceDataLoader(trainloader, device)\n",
    "testloader = DeviceDataLoader(testloader, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet9(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(512, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=1028, out_features=100, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        print(images.shape) \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True) \n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) \n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True) \n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) \n",
    "        self.conv5 = conv_block(512, 1028, pool=True) \n",
    "        self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))  \n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
    "                                        nn.Flatten(), # 1028 \n",
    "                                        nn.Linear(1028, num_classes)) # 1028 -> 100\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.conv5(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "model = to_device(ResNet9(3, 100), device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAvl0lEQVR4nO3dfXTU9Z3//ddMkpncT0hC7ky4tYKK0JYqzVpdKiw3+/t5tHLtpW2va7HrT49u8Kyy3bbsabW6uyeuPae17aH4x7qyPadoa6+iP91WqyjhpwUqVES0poJRArmDQCb3k8nM9/rDNd1UkPcHEj4kPB/nzDlk5s07n+98vzOvfDOT94SCIAgEAMBZFva9AADA+YkAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOBFpu8F/Kl0Oq2WlhYVFBQoFAr5Xg4AwFEQBOrp6VFVVZXC4ZOf55xzAdTS0qKamhrfywAAnKHm5mZVV1ef9PZxC6D169frO9/5jtra2rRgwQL98Ic/1BVXXHHK/1dQUCDpg4UXFhaO1/KAiSWVMJe2Nzc5td61+3Vzbe3nlzr1Li4ucaqfqFIOtf0pl2qpt/e4ufa9pkan3kXFuebaQ4cOmGv7+wb01//XmpHn85MZlwD66U9/qrVr1+rhhx/WokWL9NBDD2n58uVqbGxUWVnZx/7fD3/tVlhYSAABH3IIoP6CfKfWubk55trCUzyhfKT+PHkMu0RKpmMAhcLD5tq8/Dyn3vkF9vrcPHtYfehUL6OMy5sQvvvd7+rWW2/VV77yFV1yySV6+OGHlZubq3//938fj28HAJiAxjyAhoaGtHv3bi1d+sdT9XA4rKVLl2r79u0fqU8kEuru7h51AQBMfmMeQEePHlUqlVJ5efmo68vLy9XW1vaR+vr6esVisZELb0AAgPOD978DWrduneLx+MilubnZ95IAAGfBmL8JobS0VBkZGWpvbx91fXt7uyoqKj5SH41GFY1Gx3oZAIBz3JifAUUiES1cuFBbtmwZuS6dTmvLli2qra0d628HAJigxuVt2GvXrtXq1av1mc98RldccYUeeugh9fX16Stf+cp4fDsAwAQ0LgF044036siRI7rnnnvU1tamT37yk3r22Wc/8sYEAMD5a9wmIaxZs0Zr1qwZr/ajBEFwVr4PMFbSKfsfF0pSKGn/a/iejneder/0v39h790z6NT7//lf/8te7Pg4Tqcd6h1fbAjkNocy6bCWltaDTr2PdR0y17Y2v+nU+913jppr4932Y3BgwPaH097fBQcAOD8RQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAAL8ZtFM+56lSfUQ6cDtdhUOFQyu0/pHrsaxk44tQ6Lz1kru1s/eiHSn6c9rb2Uxf9l4yQ28/DsaKYuTYrkuXUO+04iicI0ubaTLelKJkaMNeWlJc49W4/Yh/F03qgxVybGEya6jgDAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXpx3s+DOF/bJVFKQTjj1Hj5unx8lSQPxXvtaInlOvQsvqLIXO84aCznM9wqnh516d7c2O9W/t2+Hubbp92879Q6HI+ba7taDTr23/vL/M9dOqapx6v1nV15lL84sdOrd2RV3qk/02mfkDQ52OPUOhu1zADuOvevU+3iX/bEcpO2PH2stZ0AAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAF4zimazSKXPp0f1uo1s6dr/sVN9/zD7WpG3I7Weii65abK79xILPOPUOZ9kfHm+8+YZT79deesmpvsdhdE93R7tT76zMqLl2sLPFqfdL//m+ufbiP1/u1Lv26iXm2sHEkFPv4x32dUvSu6/+0lzb3nLAqXfJ9Gnm2v50n1PvZL/9GI+Ey8y1Qdg23oszIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AWz4CapYNA2i0mSOhvdZlOpq9upvDhj2F4cdpvZ9e625821mUHIqXd2lX0G149//rRT7zd37XGqnzUlz1xbHHa4vyXlOcy8S2VkOfV+9w/22XEv/+HnTr0rqy811151xcVOvY+8/Run+td/vdlcm+g67tS77/Al5trcSxY69c7NKTXXFsycYq4d6O831XEGBADwYswD6Nvf/rZCodCoy9y5c8f62wAAJrhx+RXcpZdeqhdeeOGP3yST3/QBAEYbl2TIzMxURUXFeLQGAEwS4/Ia0DvvvKOqqirNmjVLX/7yl3Xw4MGT1iYSCXV3d4+6AAAmvzEPoEWLFmnjxo169tlntWHDBjU1Nemqq65ST0/PCevr6+sVi8VGLjU1NWO9JADAOWjMA2jlypX6q7/6K82fP1/Lly/XL3/5S3V1delnP/vZCevXrVuneDw+cmlutn/0MABg4hr3dwcUFRXpoosu0v79+094ezQaVTRq/0x6AMDkMO5/B9Tb26sDBw6osrJyvL8VAGACGfMA+upXv6qGhga99957+s1vfqMvfOELysjI0Be/+MWx/lYAgAlszH8Fd+jQIX3xi19UZ2enpk6dqs997nPasWOHpk6dOtbf6o/cJqycF8KRiLk2v6zKqfeRQ01O9YNHDplr8yJpp97dg/ad//aOl51690+Zbq799a9fcet9kjflnExB2P4bhIIp2U69+xL20T1vH2xz6t3WF5hrD3W6jaj5ycZH7b33lDn17m/e5VSfl+oz10Zz3F5ySPTZxtpI0vR8+2gdSQqXX2iuHQzZn1My+2z3x5gH0OOPPz7WLQEAkxCz4AAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvxv3jGM4K+7ip8Z0b57IOaVzXEmTad23FZQuceid7u5zqDxxsNNf2Hzvi1HsommOu/cMffu/Uuy9/wFybmXTb+d2dx5zq4yV55trs6W6T57uP22ew7X3fbRbckSH7/LCCWMyp98H9r5trdx4bdOr9idIsp/pIln3/dyXcjpWCMvsx3tri9nlqhbnF5tpIcYm5NpSZNNVxBgQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4MSlG8YQcJlsE4zj+JhS4zuJxae628FDavpasaLZT7wuuuNKpXg5TTVp/94pT6+qqGnNt59GUU++9O18z1+Zk2sf2SFJpgX1EjSQtvsp+ny9acIlT7x+uX2+u7RkYcurtcmwFwz1Ovfv7+s210Rr7GBlJSgduo3vaO7rNtZlTyp16h/Kmmmtff/OAU+/47rfNtZWzZplrE4mEqY4zIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4MU5OwsunQ6UNs4zc0nRtOO8tsEh20wjSYpkut2dGSH7ysNyHGLnMDtuWG73yYFjR53qjzvMA0tcNM+p96UL/8xcmzx4zKn3z/7zBXvvgT6n3l9Ysdip/ob/ucxc+87+d516d/TZZ+QNBRlOvbMCe+9Iplvvgmz7cZVXZJ+nJknxpNv+zCuvNNcGOYVOvQ8dsc/ISw24zSQc6rLPsHvpf++zryOVNtVxBgQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALw4Z2fBJZJDSiSHTLXZkYi5b3d/r9M6Xnl1p7m2MD/fqfenLp1vri3IyXXqnUoNm2sPH2lx6r31ZfuMNElqOnjQXJsYsO3zD0WrZphrh3sGnXp3vP++uba3x+24mj2jxqk+U/aZal1x+3wvSRpK22ewDRtnfH0o3W+fYxYOspx6Z2TbH/edx4479W7vcJt3mBPJM9fmxezzJSUpv8jeu8Bxnl5Opn0OZE1pkbk2OZzS64Y6zoAAAF44B9C2bdt07bXXqqqqSqFQSE8++eSo24Mg0D333KPKykrl5ORo6dKleuedd8ZqvQCAScI5gPr6+rRgwQKtX7/+hLc/+OCD+sEPfqCHH35YO3fuVF5enpYvX67BQbdffwAAJjfn14BWrlyplStXnvC2IAj00EMP6Zvf/Kauu+46SdKPf/xjlZeX68knn9RNN910ZqsFAEwaY/oaUFNTk9ra2rR06dKR62KxmBYtWqTt27ef8P8kEgl1d3ePugAAJr8xDaC2tjZJUnl5+ajry8vLR277U/X19YrFYiOXmhq3dwcBACYm7++CW7duneLx+MilubnZ95IAAGfBmAZQRUWFJKm9vX3U9e3t7SO3/aloNKrCwsJRFwDA5DemATRz5kxVVFRoy5YtI9d1d3dr586dqq2tHctvBQCY4JzfBdfb26v9+/ePfN3U1KQ9e/aouLhY06ZN01133aV//ud/1ic+8QnNnDlT3/rWt1RVVaXrr79+LNcNAJjgnANo165d+vznPz/y9dq1ayVJq1ev1saNG/W1r31NfX19uu2229TV1aXPfe5zevbZZ5Wdne30fUKZGQoZx0p099rHoLy653dO6zjYethcG41EnXpPLS41186ZMdupd7y701y7Z8/LTr1b33vLqb7toH2sScdxt5E2e974jbn2iuq5Tr1nVUw11x4vLnbqHSutdKpvbjnxm3hOpLXVbbRSX499TE1Rfo5b7177KJ7u48eces8qqzbX5me7PdX157jVp4bto69SfW5jgVJh+zuDh6aUOPVWpn3EUyxm3/dDSdv94RxAixcvVhCcfH5QKBTS/fffr/vvv9+1NQDgPOL9XXAAgPMTAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8MJ5FM/ZkkqklErY5hS9svO35r6739zrtI7Zc+3zplqa4069n3xmy6mL/sv//MukU+8D7/3eXtvc5NQ7nOE21+9Yh30W3OFD7zn1zk5dbq69bMYMp963/83/a67tirt9ku/sophTfUuLfSbhO2+4zerr6Txiro2VuM0aSw3bj5W8tFNrXTClwFwbhIeceofSbovJCJ98PNlHajNCTr2Hk/bHfn9vl1PvjMyIuTaVts+7S8v23M0ZEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAODFOTuKp7evWyHjeIsXt71g7ltSVeq0jsTgoLn2/XfbnHpbt0+Sfrv3Fafe+xxGDoUcD4MM18MmM2EuXbzkk06ty6YUm2uH+93GscybM8dcGz5+3Kn3oefsY5gkKedol7n2LwrKnHpXXDTfXLvrSKtT77dzssy1M6ornXpPzbYfh4ODPU69h1Nuo3jSafu4nIxM+30iSdHMHHPtUL/bdkZycs214ayouTYUtt1/nAEBALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvztlZcFm5EWXl2WYPxYrzzX0PHz7gtI69r+8z176/v9epd2W1fcZTSUW3U+90ethce/yY27qzHGbYSdKMWfbZZBVVBU69BxL2GVxDg26z4FID9vqB9w479e5/z22mWjxunzWXUxRz6n35tGpzbWXUbf8UdraYazOn5Dn1TmfZj/Eg5TZ/LeQw202SUkn7zMiQfaTaB9IZ9t7plFPr4YR93ZGwfR1K2dbBGRAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgxTk7imfX3j8oN882miMV2EdEZGS4bXLTu03m2sOH3Uba5E+Zaq5NpaY49e7p6TfXuo7imekwukWSyqbaR/EcOvQHp95TMrvMtVmX2kcfSVJmfMBc27znTafeb3b3OdX/51v2/vG0fbyKJBVl55prl835jFPvP4vUmGub299z6p0Rs4/XGc4NOfVOOoyokaQgbR/bFKTdnoNcxuWkUm4jhDKCtLk2nWlfdzDMKB4AwDmMAAIAeOEcQNu2bdO1116rqqoqhUIhPfnkk6Nuv/nmmxUKhUZdVqxYMVbrBQBMEs4B1NfXpwULFmj9+vUnrVmxYoVaW1tHLo899tgZLRIAMPk4vwlh5cqVWrly5cfWRKNRVVRUnPaiAACT37i8BrR161aVlZVpzpw5uuOOO9TZ2XnS2kQioe7u7lEXAMDkN+YBtGLFCv34xz/Wli1b9K//+q9qaGjQypUrlTrJJ+TV19crFouNXGpq7G/bBABMXGP+d0A33XTTyL8vu+wyzZ8/X7Nnz9bWrVu1ZMmSj9SvW7dOa9euHfm6u7ubEAKA88C4vw171qxZKi0t1f79+094ezQaVWFh4agLAGDyG/cAOnTokDo7O1VZWTne3woAMIE4/wqut7d31NlMU1OT9uzZo+LiYhUXF+u+++7TqlWrVFFRoQMHDuhrX/uaLrzwQi1fvnxMFw4AmNicA2jXrl36/Oc/P/L1h6/frF69Whs2bNDevXv1H//xH+rq6lJVVZWWLVumf/qnf1I0GnX6Pu8dfFM5ObbZXZmZgblvWUmp0zpCss9Kys6xz6STpKXX2EN57iWznHqnEr8z15YV2+8/SaqpnOZUP7W4wFw7q2aOU+9pU6vMtRmO5/vxlvfNtZ3dHU6935XbzK6C+fPNtcMDbu8k7ToWN9c+9f5bTr0vLbP/5mNmyO05Qm32WX0DMdtssg8Fwwmn+uFh+yy4dNI+w06SUrI/PvsH3eY6ZufZ75dIjsv+sfV1DqDFixcrCE5+hzz33HOuLQEA5yFmwQEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABejPnnAY2VymmDys2z1U4pzTX3TSbtM5skafn/uNxc29lpn00lSZnZ9jlMQ0Nu6/7Upy411w72uc29ajl41Kn+kxfb1zJ7xnSn3l1H7XPPWttanHofaz5krg1f6Lbuqz6/2Kl+MGyfH9bd63YcDjuMSXuz8Q2n3gcbT/wxLCdSluE2k7AwbJ/TGKTdeodD9t6SFEoP29ficodLGnZY+lDSbcZgZipkX8ew/bgaHrbdf5wBAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF6cs6N4Xvnd84pm25Y37DDaYtqMqU7r+OSfXWKuff9Am1PvcMg+6uVYb6dT73Qqw1zbE7ePEZGkzm77+BtJ+u3rcXPt2wcKnHofPmxfS3Zi0Kn33GiJuTacV+XUuy3uNi7nlVf/j7nWOAVlRFY0x1wb7z3i1Hsoy34cxrPt44YkKTPD3rtfbvs+lXYbl5ORaX8qzXSolaTksP3xGQ65nVNkZNrvw8GEfWRXklE8AIBzGQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeHHOzoKbOatYObm22VDJ4SFz37IKt3lT3b3vm2t7+o459c7MjJprk6lsp97xHvuMtORw4NS7uNptnl5W1D4LLiO7z6n39Ln2n6HSKbeftwoy7XPp/s/Lv3fq/eY7h93WUlBkrg2F3R7Wg0P2GV+dXW7HeDqwryWYUuzUu+f4cXPtwFC/U+9QKORUH4lExqVWkgYG7XPsMiNuz2/hsP0xMewwHy+dtj2ncAYEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeHHOjuL59GUXKq/ANn6mt3fA3Pett153WsexLvu4j7mXzHPqXZBf6FDtNhqk44h9vE5yyK13T1ePU3133xFzbUlxhVPvkuIp5treQbeft7Izisy1mbn2sT2SlEraj1lJioTyzbW5+XlOvcMOI4e6jjQ79S6qnGGunRJxezqKH/uDuTYdso/rkqRo1G1cTthhdM/wcNKpdzJpX3teTq5T79Rw2t47P2auTQ6nJZ36uZMzIACAF04BVF9fr8svv1wFBQUqKyvT9ddfr8bGxlE1g4ODqqurU0lJifLz87Vq1Sq1t7eP6aIBABOfUwA1NDSorq5OO3bs0PPPP69kMqlly5apr++PE4zvvvtuPf3003riiSfU0NCglpYW3XDDDWO+cADAxOb0S9dnn3121NcbN25UWVmZdu/erauvvlrxeFyPPPKINm3apGuuuUaS9Oijj+riiy/Wjh079NnPfnbsVg4AmNDO6DWgePyDz3kpLv7gczx2796tZDKppUuXjtTMnTtX06ZN0/bt20/YI5FIqLu7e9QFADD5nXYApdNp3XXXXbryyis1b94H7/5qa2tTJBJRUVHRqNry8nK1tbWdsE99fb1isdjIpaam5nSXBACYQE47gOrq6rRv3z49/vjjZ7SAdevWKR6Pj1yam93e5gkAmJhO6++A1qxZo2eeeUbbtm1TdXX1yPUVFRUaGhpSV1fXqLOg9vZ2VVSc+O87otGoolH7R1MDACYHpzOgIAi0Zs0abd68WS+++KJmzpw56vaFCxcqKytLW7ZsGbmusbFRBw8eVG1t7disGAAwKTidAdXV1WnTpk166qmnVFBQMPK6TiwWU05OjmKxmG655RatXbtWxcXFKiws1J133qna2lreAQcAGMUpgDZs2CBJWrx48ajrH330Ud18882SpO9973sKh8NatWqVEomEli9frh/96EdjslgAwOThFEBBcOr5YtnZ2Vq/fr3Wr19/2ouSpHhfp4ZDtteGwrK/htQdt88+kqS337bPMdv/boNT7+pppeba+Z+c7dR7mkPvnLDLTDopSLnNjksNp8y1kawcp96hLHtt7oB9Pp4kVeba7/NPfdJtBldprNip/pVtr5hr48e7nHoPO+yfI4c7nHoHeSXm2tRFbse4HI7DzGz7NkpSNNPhwJI00Ndvrk2nhp16R7Ltr5RkyO35bWjA4X6xjeb8gHETmQUHAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeHFaH8dwNuRmhZUbseVjkLaPn7jyswud1jF79sXm2nfff8+pd8eRQ+bars5ep97ZWfbxRO0D9nFDklRU5Da6p6CgwFwbZLmN+enpjptri/OqT13030wtm2pfR43bCKFXT/IJwSfT2XXUXJt2eDy4CrmMY5FUXGz/D8UXFDn17nP48Tkr5PazdiQnw6leIfuYp4GBAafWQdjeezjtNubH5VDpd1h3MmVrzBkQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADw4pydBRfOSCmcYZtrFM6yz0oqjGU5raO04gJz7cXzqpx6Dw7aZyul0ymn3q1HW821HXH7nDFJ6uhud6qvqLTPVIvF3IaNpcP2GXm9SbeftzoHf2uuPXys26n3vrdecapPDNr3UXa248A2B3kx+2NNkmqK7U8x8Z6DTr3DRfbtLMoqdeqd1pDbWsL2Y2s4cHss9/bYj/GMsOMMuwz7ulMOYxqtW8gZEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAODFOTuKZ3/bu8rpsS0vVlRg7hsdchuZUpidZ66dUmBfhyRlZ9vzP6yIU++yKSXm2qzMHKfe3T1HnOozAvsMj+6uLqfe7Uc6zbXx9vedeu8vfd1cWx37lFPvL//fVzvVv/GqfS1DQ25jZIqmTDHXJrLcjpWgK26u3ffWXqfeM6bmm2tL8oqdeg/3HXOq70zZxoZJUmFWkVPvIGR//PTGe5x6Z+fan99yC+33d3I4LenUj03OgAAAXhBAAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBfn7Cy4eG+3EoFteYPDg+a+0ah9NpUkJQti5tqe3l6n3lLaXJmbY5/ZJEn5uZXm2uyIfcaTJE2NFTrVJ5MD5tp4j9usvkP7W8y1mWG3w31ve7O5tjnbqbUuilzsVF/scBxWlVU59Q6n7XPMBnPtc8kkqTOrw1x7gdxmKeZk2u+TnDy33ql+tx2aTCXNtUODCbfeQ/b9099rf6xJUjRqv1+mTKkw1w4lhyWdevYiZ0AAAC+cAqi+vl6XX365CgoKVFZWpuuvv16NjY2jahYvXqxQKDTqcvvtt4/pogEAE59TADU0NKiurk47duzQ888/r2QyqWXLlqmvr29U3a233qrW1taRy4MPPjimiwYATHxOvxR/9tlnR329ceNGlZWVaffu3br66j9+vklubq4qKuy/LwQAnH/O6DWgePyDF/SLi0d/2NNPfvITlZaWat68eVq3bp36+/tP2iORSKi7u3vUBQAw+Z32u+DS6bTuuusuXXnllZo3b97I9V/60pc0ffp0VVVVae/evfr617+uxsZG/eIXvzhhn/r6et13332nuwwAwAR12gFUV1enffv26eWXXx51/W233Tby78suu0yVlZVasmSJDhw4oNmzZ3+kz7p167R27dqRr7u7u1VTU3O6ywIATBCnFUBr1qzRM888o23btqm6uvpjaxctWiRJ2r9//wkDKBqNKhqNns4yAAATmFMABUGgO++8U5s3b9bWrVs1c+bMU/6fPXv2SJIqK+1/GAkAmPycAqiurk6bNm3SU089pYKCArW1tUmSYrGYcnJydODAAW3atEl/+Zd/qZKSEu3du1d33323rr76as2fP39cNgAAMDE5BdCGDRskffDHpv/do48+qptvvlmRSEQvvPCCHnroIfX19ammpkarVq3SN7/5zTFbMABgcnD+FdzHqampUUNDwxkt6ENVZbOUmx8x1Q4P22eqhTPc3nk+MDBkru3o6jt10X/T3XPEXFsz3e3vqvqjtvtOkgZ73Nadn+82O66kpMRcm5WV69R71vRj5trcfLf5Xu8eyDDXRjPdZvWFK+3HrCQVldvn7/X29jj1zkjZZ5PNvvRCp97pt1Pm2uSw2/7JjtqPlVTY7f4uyXc7DjOz7MfK8aOdTr1Daftr5P0D9pl0kpTp8Pp7OMMeF9a7m1lwAAAvCCAAgBcEEADACwIIAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBen/XlA421ouF+Zw7axEtFojrlvXk6R0zpSw8Pm2v74yT/59YRrybWP70gl7aN1JOlY/3FzbXbE7TAIZTmVKx22j2PpH+p16l1WYR9Rk5vrNl6loqL41EX/ZThl30ZJSqQHnOpLikvNtQNxt97ZWfbRShm5jr2P2Mfr5LTZ96UkhdP2EUIpuY2bCmfYn1MkKSevyFzb32cf7yVJWdn2MUKpwD7eS5LSIfvonoFh+6dVDw3bHg+cAQEAvCCAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC/O2Vlw/QPHFYRtyxtOB+a+Pb3tTuvICNnnh4VC9tlhkhQrsNf397utOyvTPrAtlGmfSSdJfYNu89p6WuwzpHp7e5x6y2HfB+mQU+uMLHt9Ou04a0xua0n1x821mRn22WGS1Ndvn6nWM9Tp1DsUy7PX5rnNmes7ap+plgzcZvUNy36fSFJiwH6MJwP7/DVJOtR62Fzb1nHMqffUKvvMu6DfPhczmbQdg5wBAQC8IIAAAF4QQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF6cs6N4kgMFygzbxsn09XaY+6ZT9nESkjQ0ZB+BEgm7jfs43tRvru3us4/jkKR5l11kro23uY1XCYfcDpt02mE0jOO4nKYD9vslGrGPVZKkomL7mJLYFLef5WJFEad6DdlH/WTnum1nvHfQXNvfbx9/I0nBgP3xNphlHx8lSUkVmmvTyWy33hn2x6YkJTPto3j6k27jct492Gyu7Ym7PQcVVUfNtcNh+74fDjOKBwBwDiOAAABeEEAAAC8IIACAFwQQAMALAggA4AUBBADwggACAHhBAAEAvCCAAABeEEAAAC/O2VlwbS29iubYlpd2mB8WycpzWsfhVvuctKEhtxlPmZn2WWNFU+xzryTpcGu7uTYj7DZ/LSz7uiUpNyvfXJsdsddKUmY0aa59e//bTr2rBu33eebRhFPvrCyH+XiS8nMLzLV5eTGn3gMD9llwGRG3dacC+4y0/Oxqt97GWZGSpIEBp97Hh+2PH0kKlfWYa4/1us1e7Om13+eDgds5xYxPX2yunfep6fZ1DCT13LO/OGUdZ0AAAC+cAmjDhg2aP3++CgsLVVhYqNraWv3qV78auX1wcFB1dXUqKSlRfn6+Vq1apfZ2t58kAADnB6cAqq6u1gMPPKDdu3dr165duuaaa3TdddfpzTfflCTdfffdevrpp/XEE0+ooaFBLS0tuuGGG8Zl4QCAic3pNaBrr7121Nf/8i//og0bNmjHjh2qrq7WI488ok2bNumaa66RJD366KO6+OKLtWPHDn32s58du1UDACa8034NKJVK6fHHH1dfX59qa2u1e/duJZNJLV26dKRm7ty5mjZtmrZv337SPolEQt3d3aMuAIDJzzmA3njjDeXn5ysajer222/X5s2bdckll6itrU2RSERFRUWj6svLy9XW1nbSfvX19YrFYiOXmpoa540AAEw8zgE0Z84c7dmzRzt37tQdd9yh1atX66233jrtBaxbt07xeHzk0txs//hZAMDE5fx3QJFIRBdeeKEkaeHChXr11Vf1/e9/XzfeeKOGhobU1dU16iyovb1dFRUVJ+0XjUYVjdo/lxwAMDmc8d8BpdNpJRIJLVy4UFlZWdqyZcvIbY2NjTp48KBqa2vP9NsAACYZpzOgdevWaeXKlZo2bZp6enq0adMmbd26Vc8995xisZhuueUWrV27VsXFxSosLNSdd96p2tpa3gEHAPgIpwDq6OjQX//1X6u1tVWxWEzz58/Xc889p7/4i7+QJH3ve99TOBzWqlWrlEgktHz5cv3oRz86rYU1NbUpK5phqg3JPqqiIN9tlEj3cftJYk/PkFPvS+ZVmWtnTC9x6n2o5T1zbUHBFKfeQTJwqs/Ns4+0iTqM7ZGkGdPsY4SKi7Odeg8O9ptru7riTr3jx92Ow3Bxkbk2SNoeNyO9w/b7Jd531Kn3UKrPXNsVP+LUu7Av11wbdRxRMxi2r1uSohF7/3iP277v67P3jl0QceqdPdV+rKTy7SObUmHbiCynAHrkkUc+9vbs7GytX79e69evd2kLADgPMQsOAOAFAQQA8IIAAgB4QQABALwggAAAXhBAAAAvCCAAgBcEEADACwIIAOCF8zTs8RYEH4x5SSZS5v/jMopnKHPYaT0u6xgechuxkRiwr2Wg3zba4o+97evOynC7T4Jht1E8AxH72tOZbuOMBtL23oOu92HCfr8kBu33tyQNDY7fsRKW2/4Jh+3jjBJDjtuZsm9n2PE+TCTs+zNIuP2sPRS4rUX2KTVKJt32fTqw78902m3fDw3ajyuXx8/gwAe1wSnWHgpOVXGWHTp0iA+lA4BJoLm5WdXV1Se9/ZwLoHQ6rZaWFhUUFCgU+uNPZt3d3aqpqVFzc7MKC+3DLScatnPyOB+2UWI7J5ux2M4gCNTT06OqqiqFwyc/+zznfgUXDoc/NjELCwsn9c7/ENs5eZwP2yixnZPNmW5nLBY7ZQ1vQgAAeEEAAQC8mDABFI1Gde+99yoajfpeyrhiOyeP82EbJbZzsjmb23nOvQkBAHB+mDBnQACAyYUAAgB4QQABALwggAAAXkyYAFq/fr1mzJih7OxsLVq0SL/97W99L2lMffvb31YoFBp1mTt3ru9lnZFt27bp2muvVVVVlUKhkJ588slRtwdBoHvuuUeVlZXKycnR0qVL9c477/hZ7Bk41XbefPPNH9m3K1as8LPY01RfX6/LL79cBQUFKisr0/XXX6/GxsZRNYODg6qrq1NJSYny8/O1atUqtbe3e1rx6bFs5+LFiz+yP2+//XZPKz49GzZs0Pz580f+2LS2tla/+tWvRm4/W/tyQgTQT3/6U61du1b33nuvfve732nBggVavny5Ojo6fC9tTF166aVqbW0dubz88su+l3RG+vr6tGDBAq1fv/6Etz/44IP6wQ9+oIcfflg7d+5UXl6eli9frsFBh8mO54BTbackrVixYtS+feyxx87iCs9cQ0OD6urqtGPHDj3//PNKJpNatmyZ+vr6RmruvvtuPf3003riiSfU0NCglpYW3XDDDR5X7c6ynZJ06623jtqfDz74oKcVn57q6mo98MAD2r17t3bt2qVrrrlG1113nd58801JZ3FfBhPAFVdcEdTV1Y18nUqlgqqqqqC+vt7jqsbWvffeGyxYsMD3MsaNpGDz5s0jX6fT6aCioiL4zne+M3JdV1dXEI1Gg8cee8zDCsfGn25nEATB6tWrg+uuu87LesZLR0dHICloaGgIguCDfZeVlRU88cQTIzW///3vA0nB9u3bfS3zjP3pdgZBEPz5n/958Hd/93f+FjVOpkyZEvzbv/3bWd2X5/wZ0NDQkHbv3q2lS5eOXBcOh7V06VJt377d48rG3jvvvKOqqirNmjVLX/7yl3Xw4EHfSxo3TU1NamtrG7VfY7GYFi1aNOn2qyRt3bpVZWVlmjNnju644w51dnb6XtIZicfjkqTi4mJJ0u7du5VMJkftz7lz52ratGkTen/+6XZ+6Cc/+YlKS0s1b948rVu3Tv39/T6WNyZSqZQef/xx9fX1qba29qzuy3NuGOmfOnr0qFKplMrLy0ddX15errffftvTqsbeokWLtHHjRs2ZM0etra267777dNVVV2nfvn0qKCjwvbwx19bWJkkn3K8f3jZZrFixQjfccINmzpypAwcO6B//8R+1cuVKbd++XRkZGb6X5yydTuuuu+7SlVdeqXnz5kn6YH9GIhEVFRWNqp3I+/NE2ylJX/rSlzR9+nRVVVVp7969+vrXv67Gxkb94he/8Lhad2+88YZqa2s1ODio/Px8bd68WZdccon27Nlz1vblOR9A54uVK1eO/Hv+/PlatGiRpk+frp/97Ge65ZZbPK4MZ+qmm24a+fdll12m+fPna/bs2dq6dauWLFnicWWnp66uTvv27Zvwr1Geysm287bbbhv592WXXabKykotWbJEBw4c0OzZs8/2Mk/bnDlztGfPHsXjcf385z/X6tWr1dDQcFbXcM7/Cq60tFQZGRkfeQdGe3u7KioqPK1q/BUVFemiiy7S/v37fS9lXHy47863/SpJs2bNUmlp6YTct2vWrNEzzzyjl156adTHplRUVGhoaEhdXV2j6ifq/jzZdp7IokWLJGnC7c9IJKILL7xQCxcuVH19vRYsWKDvf//7Z3VfnvMBFIlEtHDhQm3ZsmXkunQ6rS1btqi2ttbjysZXb2+vDhw4oMrKSt9LGRczZ85URUXFqP3a3d2tnTt3Tur9Kn3wqb+dnZ0Tat8GQaA1a9Zo8+bNevHFFzVz5sxRty9cuFBZWVmj9mdjY6MOHjw4ofbnqbbzRPbs2SNJE2p/nkg6nVYikTi7+3JM39IwTh5//PEgGo0GGzduDN56663gtttuC4qKioK2tjbfSxszf//3fx9s3bo1aGpqCl555ZVg6dKlQWlpadDR0eF7aaetp6cneO2114LXXnstkBR897vfDV577bXg/fffD4IgCB544IGgqKgoeOqpp4K9e/cG1113XTBz5sxgYGDA88rdfNx29vT0BF/96leD7du3B01NTcELL7wQfPrTnw4+8YlPBIODg76XbnbHHXcEsVgs2Lp1a9Da2jpy6e/vH6m5/fbbg2nTpgUvvvhisGvXrqC2tjaora31uGp3p9rO/fv3B/fff3+wa9euoKmpKXjqqaeCWbNmBVdffbXnlbv5xje+ETQ0NARNTU3B3r17g2984xtBKBQKfv3rXwdBcPb25YQIoCAIgh/+8IfBtGnTgkgkElxxxRXBjh07fC9pTN14441BZWVlEIlEggsuuCC48cYbg/379/te1hl56aWXAkkfuaxevToIgg/eiv2tb30rKC8vD6LRaLBkyZKgsbHR76JPw8dtZ39/f7Bs2bJg6tSpQVZWVjB9+vTg1ltvnXA/PJ1o+yQFjz766EjNwMBA8Ld/+7fBlClTgtzc3OALX/hC0Nra6m/Rp+FU23nw4MHg6quvDoqLi4NoNBpceOGFwT/8wz8E8Xjc78Id/c3f/E0wffr0IBKJBFOnTg2WLFkyEj5BcPb2JR/HAADw4px/DQgAMDkRQAAALwggAIAXBBAAwAsCCADgBQEEAPCCAAIAeEEAAQC8IIAAAF4QQAAALwggAIAXBBAAwIv/H5PHIctnXGaBAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as plt\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(train_data[0][0], interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in test_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, test_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up cutom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, test_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 4.605477333068848, 'val_acc': 0.010192308574914932}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initial evaluation\n",
    "history = [evaluate(model, testloader)]\n",
    "history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the first 1/4 epochs\n",
    "current_time=time.time()\n",
    "history += fit_one_cycle(int(epochs/4), max_lr, model, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00001, train_loss: 0.0458, val_loss: 0.9933, val_acc: 0.7444\n",
      "Epoch [1], last_lr: 0.00002, train_loss: 0.0443, val_loss: 1.0016, val_acc: 0.7414\n",
      "Epoch [2], last_lr: 0.00003, train_loss: 0.0446, val_loss: 1.0191, val_acc: 0.7431\n",
      "Epoch [3], last_lr: 0.00004, train_loss: 0.0447, val_loss: 1.0509, val_acc: 0.7362\n",
      "Epoch [4], last_lr: 0.00006, train_loss: 0.0539, val_loss: 1.0993, val_acc: 0.7253\n",
      "Epoch [5], last_lr: 0.00008, train_loss: 0.0679, val_loss: 1.1241, val_acc: 0.7201\n",
      "Epoch [6], last_lr: 0.00009, train_loss: 0.0833, val_loss: 1.1472, val_acc: 0.7137\n",
      "Epoch [7], last_lr: 0.00010, train_loss: 0.0956, val_loss: 1.1689, val_acc: 0.7106\n",
      "Epoch [8], last_lr: 0.00010, train_loss: 0.0976, val_loss: 1.2208, val_acc: 0.7041\n",
      "Epoch [9], last_lr: 0.00010, train_loss: 0.0924, val_loss: 1.2538, val_acc: 0.7017\n",
      "Epoch [10], last_lr: 0.00010, train_loss: 0.0826, val_loss: 1.1906, val_acc: 0.7100\n",
      "Epoch [11], last_lr: 0.00010, train_loss: 0.0745, val_loss: 1.1619, val_acc: 0.7159\n",
      "Epoch [12], last_lr: 0.00009, train_loss: 0.0635, val_loss: 1.1421, val_acc: 0.7192\n",
      "Epoch [13], last_lr: 0.00009, train_loss: 0.0575, val_loss: 1.1451, val_acc: 0.7194\n",
      "Epoch [14], last_lr: 0.00008, train_loss: 0.0507, val_loss: 1.1379, val_acc: 0.7234\n",
      "Epoch [15], last_lr: 0.00008, train_loss: 0.0430, val_loss: 1.1313, val_acc: 0.7341\n",
      "Epoch [16], last_lr: 0.00007, train_loss: 0.0366, val_loss: 1.0914, val_acc: 0.7320\n",
      "Epoch [17], last_lr: 0.00006, train_loss: 0.0306, val_loss: 1.1145, val_acc: 0.7311\n",
      "Epoch [18], last_lr: 0.00005, train_loss: 0.0275, val_loss: 1.0883, val_acc: 0.7386\n",
      "Epoch [19], last_lr: 0.00005, train_loss: 0.0209, val_loss: 1.0716, val_acc: 0.7428\n",
      "Epoch [20], last_lr: 0.00004, train_loss: 0.0171, val_loss: 1.0562, val_acc: 0.7448\n",
      "Epoch [21], last_lr: 0.00003, train_loss: 0.0162, val_loss: 1.0621, val_acc: 0.7462\n",
      "Epoch [22], last_lr: 0.00003, train_loss: 0.0134, val_loss: 1.0460, val_acc: 0.7494\n",
      "Epoch [23], last_lr: 0.00002, train_loss: 0.0115, val_loss: 1.0545, val_acc: 0.7491\n",
      "Epoch [24], last_lr: 0.00001, train_loss: 0.0104, val_loss: 1.0432, val_acc: 0.7501\n",
      "Epoch [25], last_lr: 0.00001, train_loss: 0.0100, val_loss: 1.0420, val_acc: 0.7492\n",
      "Epoch [26], last_lr: 0.00000, train_loss: 0.0090, val_loss: 1.0362, val_acc: 0.7501\n",
      "Epoch [27], last_lr: 0.00000, train_loss: 0.0086, val_loss: 1.0355, val_acc: 0.7506\n",
      "Epoch [28], last_lr: 0.00000, train_loss: 0.0081, val_loss: 1.0345, val_acc: 0.7508\n",
      "Epoch [29], last_lr: 0.00000, train_loss: 0.0085, val_loss: 1.0343, val_acc: 0.7511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Fitting the second 1/4 epochs\n",
    "history += fit_one_cycle(int(epochs/4), max_lr/10, model, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00000, train_loss: 0.0083, val_loss: 1.0350, val_acc: 0.7526\n",
      "Epoch [1], last_lr: 0.00000, train_loss: 0.0083, val_loss: 1.0377, val_acc: 0.7515\n",
      "Epoch [2], last_lr: 0.00001, train_loss: 0.0085, val_loss: 1.0389, val_acc: 0.7504\n",
      "Epoch [3], last_lr: 0.00001, train_loss: 0.0093, val_loss: 1.0392, val_acc: 0.7506\n",
      "Epoch [4], last_lr: 0.00001, train_loss: 0.0092, val_loss: 1.0364, val_acc: 0.7502\n",
      "Epoch [5], last_lr: 0.00001, train_loss: 0.0087, val_loss: 1.0346, val_acc: 0.7526\n",
      "Epoch [6], last_lr: 0.00001, train_loss: 0.0098, val_loss: 1.0314, val_acc: 0.7496\n",
      "Epoch [7], last_lr: 0.00001, train_loss: 0.0091, val_loss: 1.0331, val_acc: 0.7488\n",
      "Epoch [8], last_lr: 0.00001, train_loss: 0.0085, val_loss: 1.0275, val_acc: 0.7513\n",
      "Epoch [9], last_lr: 0.00000, train_loss: 0.0079, val_loss: 1.0249, val_acc: 0.7529\n",
      "Epoch [10], last_lr: 0.00000, train_loss: 0.0075, val_loss: 1.0227, val_acc: 0.7529\n",
      "Epoch [11], last_lr: 0.00000, train_loss: 0.0073, val_loss: 1.0188, val_acc: 0.7537\n",
      "Epoch [12], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0199, val_acc: 0.7532\n",
      "Epoch [13], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0191, val_acc: 0.7543\n",
      "Epoch [14], last_lr: 0.00000, train_loss: 0.0072, val_loss: 1.0204, val_acc: 0.7537\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history += fit_one_cycle(int(epochs/8), max_lr/100, model, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00000, train_loss: 0.0071, val_loss: 1.0191, val_acc: 0.7541\n",
      "Epoch [1], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0189, val_acc: 0.7539\n",
      "Epoch [2], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0191, val_acc: 0.7534\n",
      "Epoch [3], last_lr: 0.00000, train_loss: 0.0071, val_loss: 1.0190, val_acc: 0.7535\n",
      "Epoch [4], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0194, val_acc: 0.7534\n",
      "Epoch [5], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0199, val_acc: 0.7537\n",
      "Epoch [6], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0197, val_acc: 0.7535\n",
      "Epoch [7], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0194, val_acc: 0.7540\n",
      "Epoch [8], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0212, val_acc: 0.7525\n",
      "Epoch [9], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0209, val_acc: 0.7534\n",
      "Epoch [10], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0177, val_acc: 0.7543\n",
      "Epoch [11], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0184, val_acc: 0.7530\n",
      "Epoch [12], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0200, val_acc: 0.7531\n",
      "Epoch [13], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0197, val_acc: 0.7537\n",
      "Epoch [14], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0201, val_acc: 0.7531\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history += fit_one_cycle(int(epochs/8), max_lr/1000, model, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0181, val_acc: 0.7526\n",
      "Epoch [1], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0207, val_acc: 0.7541\n",
      "Epoch [2], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0213, val_acc: 0.7536\n",
      "Epoch [3], last_lr: 0.00000, train_loss: 0.0069, val_loss: 1.0221, val_acc: 0.7517\n",
      "Epoch [4], last_lr: 0.00001, train_loss: 0.0072, val_loss: 1.0186, val_acc: 0.7519\n",
      "Epoch [5], last_lr: 0.00001, train_loss: 0.0079, val_loss: 1.0219, val_acc: 0.7494\n",
      "Epoch [6], last_lr: 0.00001, train_loss: 0.0079, val_loss: 1.0212, val_acc: 0.7490\n",
      "Epoch [7], last_lr: 0.00001, train_loss: 0.0088, val_loss: 1.0230, val_acc: 0.7475\n",
      "Epoch [8], last_lr: 0.00001, train_loss: 0.0090, val_loss: 1.0231, val_acc: 0.7478\n",
      "Epoch [9], last_lr: 0.00001, train_loss: 0.0092, val_loss: 1.0194, val_acc: 0.7504\n",
      "Epoch [10], last_lr: 0.00001, train_loss: 0.0087, val_loss: 1.0190, val_acc: 0.7500\n",
      "Epoch [11], last_lr: 0.00001, train_loss: 0.0089, val_loss: 1.0251, val_acc: 0.7474\n",
      "Epoch [12], last_lr: 0.00001, train_loss: 0.0086, val_loss: 1.0211, val_acc: 0.7473\n",
      "Epoch [13], last_lr: 0.00001, train_loss: 0.0091, val_loss: 1.0058, val_acc: 0.7499\n",
      "Epoch [14], last_lr: 0.00001, train_loss: 0.0087, val_loss: 1.0129, val_acc: 0.7500\n",
      "Epoch [15], last_lr: 0.00001, train_loss: 0.0083, val_loss: 1.0105, val_acc: 0.7491\n",
      "Epoch [16], last_lr: 0.00001, train_loss: 0.0081, val_loss: 1.0088, val_acc: 0.7505\n",
      "Epoch [17], last_lr: 0.00001, train_loss: 0.0078, val_loss: 1.0095, val_acc: 0.7496\n",
      "Epoch [18], last_lr: 0.00001, train_loss: 0.0079, val_loss: 1.0085, val_acc: 0.7492\n",
      "Epoch [19], last_lr: 0.00000, train_loss: 0.0077, val_loss: 1.0073, val_acc: 0.7499\n",
      "Epoch [20], last_lr: 0.00000, train_loss: 0.0072, val_loss: 1.0077, val_acc: 0.7505\n",
      "Epoch [21], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0057, val_acc: 0.7518\n",
      "Epoch [22], last_lr: 0.00000, train_loss: 0.0071, val_loss: 1.0036, val_acc: 0.7528\n",
      "Epoch [23], last_lr: 0.00000, train_loss: 0.0070, val_loss: 1.0030, val_acc: 0.7509\n",
      "Epoch [24], last_lr: 0.00000, train_loss: 0.0068, val_loss: 1.0026, val_acc: 0.7516\n",
      "Epoch [25], last_lr: 0.00000, train_loss: 0.0067, val_loss: 1.0020, val_acc: 0.7514\n",
      "Epoch [26], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0040, val_acc: 0.7516\n",
      "Epoch [27], last_lr: 0.00000, train_loss: 0.0065, val_loss: 1.0023, val_acc: 0.7520\n",
      "Epoch [28], last_lr: 0.00000, train_loss: 0.0066, val_loss: 1.0025, val_acc: 0.7528\n",
      "Epoch [29], last_lr: 0.00000, train_loss: 0.0064, val_loss: 1.0019, val_acc: 0.7522\n",
      "Training time: 2185.85 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history += fit_one_cycle(int(epochs/4), max_lr/100, model, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)\n",
    "# Print training time\n",
    "print('Training time: {:.2f} s'.format(time.time() - current_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1.42 s\n"
     ]
    }
   ],
   "source": [
    "# Collect training time and result\n",
    "current_time = time.time()\n",
    "result = evaluate(model, testloader)\n",
    "result\n",
    "print('Training time: {:.2f} s'.format(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model to h5 file\n",
    "torch.save(model.state_dict(), 'group22_pretrained_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[90  0  0 ...  0  1  0]\n",
      " [ 1 88  0 ...  0  0  0]\n",
      " [ 0  1 65 ...  0  4  0]\n",
      " ...\n",
      " [ 0  0  0 ... 87  1  0]\n",
      " [ 0  1  3 ...  0 58  0]\n",
      " [ 0  0  1 ...  0  0 74]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.90      0.91       100\n",
      "           1       0.83      0.88      0.85       100\n",
      "           2       0.65      0.65      0.65       100\n",
      "           3       0.64      0.58      0.61       100\n",
      "           4       0.54      0.64      0.59       100\n",
      "           5       0.71      0.76      0.73       100\n",
      "           6       0.77      0.85      0.81       100\n",
      "           7       0.85      0.82      0.83       100\n",
      "           8       0.91      0.86      0.89       100\n",
      "           9       0.89      0.85      0.87       100\n",
      "          10       0.60      0.56      0.58       100\n",
      "          11       0.60      0.55      0.57       100\n",
      "          12       0.76      0.80      0.78       100\n",
      "          13       0.67      0.62      0.65       100\n",
      "          14       0.73      0.66      0.69       100\n",
      "          15       0.74      0.82      0.78       100\n",
      "          16       0.75      0.74      0.74       100\n",
      "          17       0.88      0.86      0.87       100\n",
      "          18       0.70      0.69      0.70       100\n",
      "          19       0.76      0.68      0.72       100\n",
      "          20       0.90      0.89      0.89       100\n",
      "          21       0.80      0.90      0.85       100\n",
      "          22       0.79      0.78      0.78       100\n",
      "          23       0.80      0.86      0.83       100\n",
      "          24       0.89      0.86      0.87       100\n",
      "          25       0.63      0.63      0.63       100\n",
      "          26       0.71      0.69      0.70       100\n",
      "          27       0.66      0.66      0.66       100\n",
      "          28       0.80      0.77      0.79       100\n",
      "          29       0.74      0.70      0.72       100\n",
      "          30       0.75      0.70      0.73       100\n",
      "          31       0.86      0.73      0.79       100\n",
      "          32       0.73      0.66      0.69       100\n",
      "          33       0.68      0.69      0.69       100\n",
      "          34       0.76      0.79      0.77       100\n",
      "          35       0.57      0.63      0.60       100\n",
      "          36       0.76      0.84      0.80       100\n",
      "          37       0.75      0.75      0.75       100\n",
      "          38       0.73      0.70      0.71       100\n",
      "          39       0.91      0.88      0.89       100\n",
      "          40       0.69      0.68      0.68       100\n",
      "          41       0.85      0.88      0.87       100\n",
      "          42       0.75      0.76      0.76       100\n",
      "          43       0.83      0.74      0.78       100\n",
      "          44       0.52      0.51      0.52       100\n",
      "          45       0.63      0.66      0.65       100\n",
      "          46       0.60      0.53      0.56       100\n",
      "          47       0.67      0.69      0.68       100\n",
      "          48       0.89      0.93      0.91       100\n",
      "          49       0.83      0.90      0.86       100\n",
      "          50       0.59      0.57      0.58       100\n",
      "          51       0.77      0.79      0.78       100\n",
      "          52       0.62      0.65      0.63       100\n",
      "          53       0.87      0.92      0.89       100\n",
      "          54       0.80      0.91      0.85       100\n",
      "          55       0.51      0.59      0.55       100\n",
      "          56       0.91      0.93      0.92       100\n",
      "          57       0.75      0.81      0.78       100\n",
      "          58       0.84      0.84      0.84       100\n",
      "          59       0.72      0.68      0.70       100\n",
      "          60       0.88      0.84      0.86       100\n",
      "          61       0.75      0.74      0.74       100\n",
      "          62       0.77      0.81      0.79       100\n",
      "          63       0.78      0.73      0.75       100\n",
      "          64       0.74      0.60      0.66       100\n",
      "          65       0.63      0.69      0.66       100\n",
      "          66       0.77      0.82      0.80       100\n",
      "          67       0.64      0.65      0.65       100\n",
      "          68       0.90      0.92      0.91       100\n",
      "          69       0.81      0.83      0.82       100\n",
      "          70       0.85      0.78      0.81       100\n",
      "          71       0.80      0.86      0.83       100\n",
      "          72       0.57      0.47      0.51       100\n",
      "          73       0.59      0.63      0.61       100\n",
      "          74       0.60      0.59      0.60       100\n",
      "          75       0.88      0.88      0.88       100\n",
      "          76       0.88      0.92      0.90       100\n",
      "          77       0.81      0.68      0.74       100\n",
      "          78       0.62      0.66      0.64       100\n",
      "          79       0.76      0.84      0.80       100\n",
      "          80       0.69      0.60      0.64       100\n",
      "          81       0.70      0.78      0.74       100\n",
      "          82       0.96      0.96      0.96       100\n",
      "          83       0.77      0.69      0.73       100\n",
      "          84       0.76      0.68      0.72       100\n",
      "          85       0.86      0.85      0.85       100\n",
      "          86       0.79      0.72      0.75       100\n",
      "          87       0.83      0.85      0.84       100\n",
      "          88       0.85      0.82      0.83       100\n",
      "          89       0.83      0.91      0.87       100\n",
      "          90       0.75      0.83      0.79       100\n",
      "          91       0.83      0.82      0.82       100\n",
      "          92       0.71      0.65      0.68       100\n",
      "          93       0.61      0.61      0.61       100\n",
      "          94       0.87      0.95      0.91       100\n",
      "          95       0.73      0.74      0.73       100\n",
      "          96       0.71      0.70      0.70       100\n",
      "          97       0.79      0.87      0.83       100\n",
      "          98       0.58      0.58      0.58       100\n",
      "          99       0.76      0.74      0.75       100\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "F1 score: 0.750540\n",
      "Recall score: 0.751400\n",
      "Accuracy score: 0.751400\n"
     ]
    }
   ],
   "source": [
    "# Generate testing accuracy, predicted label, confusion matrix, and table for classification report\n",
    "def test_label_predictions(model, device, test_loader):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            prediction = output.argmax(dim=1, keepdim=True)\n",
    "            actuals.extend(target.view_as(prediction))\n",
    "            predictions.extend(prediction)\n",
    "    return [i.item() for i in actuals], [i.item() for i in predictions]\n",
    "\n",
    "y_test, y_pred = test_label_predictions(model, device, testloader)\n",
    "cm=confusion_matrix(y_test, y_pred)\n",
    "cr=classification_report(y_test, y_pred)\n",
    "fs=f1_score(y_test,y_pred,average='weighted')\n",
    "rs=recall_score(y_test, y_pred,average='weighted')\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n",
    "print(cr)\n",
    "print('F1 score: %f' % fs)\n",
    "print('Recall score: %f' % rs)\n",
    "print('Accuracy score: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]          73,856\n",
      "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
      "              ReLU-6          [-1, 128, 32, 32]               0\n",
      "         MaxPool2d-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "             ReLU-10          [-1, 128, 16, 16]               0\n",
      "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
      "             ReLU-13          [-1, 128, 16, 16]               0\n",
      "           Conv2d-14          [-1, 256, 16, 16]         295,168\n",
      "      BatchNorm2d-15          [-1, 256, 16, 16]             512\n",
      "             ReLU-16          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-17            [-1, 256, 8, 8]               0\n",
      "           Conv2d-18            [-1, 512, 8, 8]       1,180,160\n",
      "      BatchNorm2d-19            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-20            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-24            [-1, 512, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-27            [-1, 512, 4, 4]               0\n",
      "           Conv2d-28           [-1, 1028, 4, 4]       4,738,052\n",
      "      BatchNorm2d-29           [-1, 1028, 4, 4]           2,056\n",
      "             ReLU-30           [-1, 1028, 4, 4]               0\n",
      "        MaxPool2d-31           [-1, 1028, 2, 2]               0\n",
      "           Conv2d-32           [-1, 1028, 2, 2]       9,512,084\n",
      "      BatchNorm2d-33           [-1, 1028, 2, 2]           2,056\n",
      "             ReLU-34           [-1, 1028, 2, 2]               0\n",
      "           Conv2d-35           [-1, 1028, 2, 2]       9,512,084\n",
      "      BatchNorm2d-36           [-1, 1028, 2, 2]           2,056\n",
      "             ReLU-37           [-1, 1028, 2, 2]               0\n",
      "        MaxPool2d-38           [-1, 1028, 1, 1]               0\n",
      "          Flatten-39                 [-1, 1028]               0\n",
      "           Linear-40                  [-1, 100]         102,900\n",
      "================================================================\n",
      "Total params: 30,441,528\n",
      "Trainable params: 30,441,528\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 9.68\n",
      "Params size (MB): 116.13\n",
      "Estimated Total Size (MB): 125.81\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "print(summary(model,(3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
