{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'11': {'15': 0, '19': 1, '21': 2, '31': 3, '38': 4}, '15': {'27': 0, '29': 1, '44': 2, '78': 3, '93': 4}, '4': {'0': 0, '51': 1, '53': 2, '57': 3, '83': 4}, '14': {'2': 0, '11': 1, '35': 2, '46': 3, '98': 4}, '1': {'1': 0, '32': 1, '67': 2, '73': 3, '91': 4}, '5': {'22': 0, '39': 1, '40': 2, '86': 3, '87': 4}, '18': {'8': 0, '13': 1, '48': 2, '58': 3, '90': 4}, '3': {'9': 0, '10': 1, '16': 2, '28': 3, '61': 4}, '10': {'23': 0, '33': 1, '49': 2, '60': 3, '71': 4}, '17': {'47': 0, '52': 1, '56': 2, '59': 3, '96': 4}, '2': {'54': 0, '62': 1, '70': 2, '82': 3, '92': 4}, '9': {'12': 0, '17': 1, '37': 2, '68': 3, '76': 4}, '8': {'3': 0, '42': 1, '43': 2, '88': 3, '97': 4}, '16': {'36': 0, '50': 1, '65': 2, '74': 3, '80': 4}, '6': {'5': 0, '20': 1, '25': 2, '84': 3, '94': 4}, '12': {'34': 0, '63': 1, '64': 2, '66': 3, '75': 4}, '19': {'41': 0, '69': 1, '81': 2, '85': 3, '89': 4}, '7': {'6': 0, '7': 1, '14': 2, '18': 3, '24': 4}, '13': {'26': 0, '45': 1, '77': 2, '79': 3, '99': 4}, '0': {'4': 0, '30': 1, '55': 2, '72': 3, '95': 4}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open('parent_to_child_class_from_0_to_5.json')\n",
    "parent_to_child_class_from_0_to_5 = json.load(f)\n",
    "print(parent_to_child_class_from_0_to_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets.utils import download_url, check_integrity\n",
    "import torchvision.transforms as tt\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "from sklearn.metrics import *\n",
    "from torchsummary import summary\n",
    "\n",
    "class CIFAR10(data.Dataset):\n",
    "    \"\"\"`CIFAR10 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory\n",
    "            ``cifar-10-batches-py`` exists.\n",
    "        train (bool, optional): If True, creates dataset from training set, otherwise\n",
    "            creates from test set.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-10-batches-py'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\"\n",
    "    filename = \"cifar-10-python.tar.gz\"\n",
    "    tgz_md5 = 'c58f30108f718f92721af3b95e74349a'\n",
    "    train_list = [\n",
    "        ['data_batch_1', 'c99cafc152244af753f735de768cd75f'],\n",
    "        ['data_batch_2', 'd4bba439e000b95fd0a9bffe97cbabec'],\n",
    "        ['data_batch_3', '54ebc095f3ab1f0389bbae665268c751'],\n",
    "        ['data_batch_4', '634d18415352ddfa80567beed471001a'],\n",
    "        ['data_batch_5', '482c414d41f54cd18b22e5b47cb7c3cb'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test_batch', '40351d587109b95175f43aff81a1287e'],\n",
    "    ]\n",
    "\n",
    "    meta = {\n",
    "        \"filename\": \"batches.meta\",\n",
    "        \"key\": \"label_names\",\n",
    "        \"md5\": \"5ff9c542aee3614f3951f8cda6e48888\",\n",
    "    }\n",
    "\n",
    "\n",
    "    def __init__(self, root, train=True,\n",
    "                 transform=None, target_transform=None,\n",
    "                 download=False, coarse=False, coarseNumber=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "        self.coarse = coarse\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        # now load the picked numpy arrays\n",
    "        if self.train:\n",
    "            self.train_data = []\n",
    "            self.train_labels = []\n",
    "            self.train_coarse_labels = []\n",
    "            for fentry in self.train_list:\n",
    "                f = fentry[0]\n",
    "                file = os.path.join(self.root, self.base_folder, f)\n",
    "                fo = open(file, 'rb')\n",
    "                if sys.version_info[0] == 2:\n",
    "                    entry = pickle.load(fo)\n",
    "                else:\n",
    "                    entry = pickle.load(fo, encoding='latin1')\n",
    "                self.train_data.append(entry['data'])\n",
    "                if 'labels' in entry:\n",
    "                    self.train_labels += entry['labels']\n",
    "                else:\n",
    "                    self.train_labels += entry['fine_labels']\n",
    "                    if self.coarse:\n",
    "                        self.train_coarse_labels += entry['coarse_labels']\n",
    "                fo.close()\n",
    "            ###\n",
    "            new_train_coarse_labels=[]\n",
    "            new_train_fine_labels=[]\n",
    "            new_train_data=[]\n",
    "            #print(\"before data Train\",np.concatenate(self.train_data))\n",
    "            #print(len(self.train_data))\n",
    "            #print(\"before fine Train\",self.train_labels)\n",
    "            #print(len(self.train_labels))\n",
    "            #print(\"before coarse Train\",self.train_coarse_labels)\n",
    "            #print(len(self.train_coarse_labels))\n",
    "            for data, fine,coarse in zip(self.train_data[0],self.train_labels, self.train_coarse_labels):\n",
    "                if coarse==coarseNumber:\n",
    "                    new_train_data.append(data)\n",
    "                    new_train_fine_labels.append(parent_to_child_class_from_0_to_5[str(coarseNumber)][str(fine)])\n",
    "                    new_train_coarse_labels.append(coarseNumber)\n",
    "            # print(\"now coarse \",new_train_coarse_labels)\n",
    "            # print(\"now fine \",new_train_fine_labels)\n",
    "            # print(len(new_train_coarse_labels))\n",
    "            # print(len(new_train_fine_labels))\n",
    "            # print(\"============\")\n",
    "            # self.train_data=new_train_data\n",
    "            #print(parent_to_child_class_from_0_to_5[str(coarseNumber)])\n",
    "            self.train_data=np.array([np.array(xi) for xi in new_train_data])\n",
    "            self.train_labels=new_train_fine_labels\n",
    "            self.train_coarse_labels=new_train_coarse_labels\n",
    "            #print(self.train_labels)\n",
    "            # print(\" \",self.train_data)\n",
    "            # print(\" \",self.train_labels)\n",
    "            # print(\" \",self.train_coarse_labels)\n",
    "            ###\n",
    "            # print(\"after data Train\",self.train_data)\n",
    "            # print(len(self.train_data))\n",
    "            #print(\"after fine Train\",self.train_labels)\n",
    "            #print(len(self.train_labels))\n",
    "            #print(\"after coarse Train\",self.train_coarse_labels)\n",
    "            #self.train_data = np.concatenate(self.train_data)\n",
    "            self.train_data = self.train_data.reshape((len(new_train_data), 3, 32, 32))\n",
    "            self.train_data = self.train_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "        else:\n",
    "            f = self.test_list[0][0]\n",
    "            file = os.path.join(self.root, self.base_folder, f)\n",
    "            fo = open(file, 'rb')\n",
    "            if sys.version_info[0] == 2:\n",
    "                entry = pickle.load(fo)\n",
    "            else:\n",
    "                entry = pickle.load(fo, encoding='latin1')\n",
    "            self.test_data = entry['data']\n",
    "\n",
    "            if 'labels' in entry:\n",
    "                self.test_labels = entry['labels']\n",
    "            else:\n",
    "                self.test_labels = entry['fine_labels']\n",
    "                if self.coarse:\n",
    "                    self.test_coarse_labels = entry['coarse_labels']\n",
    "                new_test_coarse_labels=[]\n",
    "                new_test_fine_labels=[]\n",
    "                new_test_data=[]\n",
    "                # print(\"before fine \",entry['fine_labels'])\n",
    "                # print(len(entry['fine_labels']))\n",
    "                # print(\"before coarse \",entry['coarse_labels'])\n",
    "                # print(len(entry['coarse_labels']))\n",
    "                for data, fine,coarse in zip(self.test_data,self.test_labels, self.test_coarse_labels):\n",
    "                    if coarse==coarseNumber:\n",
    "                        new_test_data.append(data)\n",
    "                        new_test_fine_labels.append(parent_to_child_class_from_0_to_5[str(coarseNumber)][str(fine)])\n",
    "                        new_test_coarse_labels.append(coarseNumber)\n",
    "                # print(\"now coarse \",new_test_coarse_labels)\n",
    "                # print(\"now fine \",new_test_fine_labels)\n",
    "                # print(len(new_test_coarse_labels))\n",
    "                # print(len(new_test_fine_labels))\n",
    "                # print(\"============\")\n",
    "                # print(\"now data \",new_test_data)\n",
    "                # print(len(new_test_data))\n",
    "                # print(\"self test data before\",self.test_data)\n",
    "                # print(\"test data type before\", type(self.test_data))\n",
    "                # print(\"shape of test data before \", self.test_data.shape)\n",
    "                self.test_data=np.array([np.array(xi) for xi in new_test_data])\n",
    "                # print(\"test data type after\", type(self.test_data))\n",
    "                # print(\"shape of test data after \", self.test_data.shape)\n",
    "                self.test_labels=new_test_fine_labels\n",
    "                self.test_coarse_labels=new_test_coarse_labels\n",
    "            fo.close()\n",
    "            self.test_data = self.test_data.reshape((len(new_test_data), 3, 32, 32))\n",
    "            self.test_data = self.test_data.transpose((0, 2, 3, 1))  # convert to HWC\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "            if self.coarse:\n",
    "                coarse_target = self.train_coarse_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "            if self.coarse:\n",
    "                coarse_target = self.test_coarse_labels[index]\n",
    "\n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        if not self.coarse:\n",
    "            return img, target\n",
    "        else:\n",
    "            return img, target, coarse_target\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        root = self.root\n",
    "        for fentry in (self.train_list + self.test_list):\n",
    "            filename, md5 = fentry[0], fentry[1]\n",
    "            fpath = os.path.join(root, self.base_folder, filename)\n",
    "            if not check_integrity(fpath, md5):\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        root = self.root\n",
    "        download_url(self.url, root, self.filename, self.tgz_md5)\n",
    "\n",
    "        # extract file\n",
    "        cwd = os.getcwd()\n",
    "        tar = tarfile.open(os.path.join(root, self.filename), \"r:gz\")\n",
    "        os.chdir(root)\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "        os.chdir(cwd)\n",
    "\n",
    "\n",
    "class CIFAR100(CIFAR10):\n",
    "    \"\"\"`CIFAR100 <https://www.cs.toronto.edu/~kriz/cifar.html>`_ Dataset.\n",
    "    This is a subclass of the `CIFAR10` Dataset.\n",
    "    \"\"\"\n",
    "    base_folder = 'cifar-100-python'\n",
    "    url = \"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\"\n",
    "    filename = \"cifar-100-python.tar.gz\"\n",
    "    tgz_md5 = 'eb9058c3a382ffc7106e4002c42a8d85'\n",
    "    train_list = [\n",
    "        ['train', '16019d7e3df5f24257cddd939b257f8d'],\n",
    "    ]\n",
    "\n",
    "    test_list = [\n",
    "        ['test', 'f0ef6b0ae62326f3e7ffdfab6717acfc'],\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = tt.Compose([tt.RandomCrop(32, padding=4,padding_mode='reflect'), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         #tt.Normalize(mean,std,inplace=True) \n",
    "                         ]\n",
    "                         )\n",
    "transform_test = tt.Compose([tt.ToTensor(), \n",
    "                             #tt.Normalize(mean,std)\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 400\n",
    "epochs = 120\n",
    "max_lr = 0.001\n",
    "grad_clip = 0.01\n",
    "weight_decay =0.001\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = CIFAR100('./data', train=True,\n",
    "                 transform=transform_train,\n",
    "                 download=True, coarse=True, coarseNumber=2)\n",
    "test_data = CIFAR100('./data', train=False,\n",
    "                 transform=transform_test,\n",
    "                 download=True, coarse=True, coarseNumber=2)\n",
    "\n",
    "train_length = train_data.__len__() # Length training dataset\n",
    "train_indices = np.arange(train_length)\n",
    "    \n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                        train_data, \n",
    "                        batch_size=batch_size, \n",
    "                        num_workers=2,\n",
    "                        pin_memory=True,\n",
    "                    )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                        test_data, \n",
    "                        batch_size=batch_size*2,\n",
    "                        num_workers=2,\n",
    "                        pin_memory=True\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "trainloader = DeviceDataLoader(train_loader, device)\n",
    "testloader = DeviceDataLoader(test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def __init__(self, fine):\n",
    "        super(ImageClassificationBase, self).__init__()\n",
    "        self.fine = fine\n",
    "        #self.parent_to_child_class={}\n",
    "        self.unique_outs=[]\n",
    "    \n",
    "    def training_step(self, batch):\n",
    "        images, fine, coarse = batch\n",
    "        if self.fine:\n",
    "            labels=fine\n",
    "        else:\n",
    "            labels=coarse\n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, fine, coarse = batch \n",
    "\n",
    "        #print(\"coarse \", coarse)\n",
    "        #print(\"fine \", fine)    \n",
    "        if self.fine:\n",
    "            labels=fine\n",
    "        else:\n",
    "            labels=coarse\n",
    "        out = self(images)\n",
    "        #print(\"out shape \",out.shape)\n",
    "        #print(\"out first three\", out[:3,:])  \n",
    "        #print(\"labels shape \",labels.shape)\n",
    "        #print(\"labels first three\", labels[:3])\n",
    "        #labels=labels.cpu().apply_(lambda val: dict.get(val)).to(labels.device)\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        \n",
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes, fine):\n",
    "        super().__init__(fine)\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True) \n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128)) \n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True) \n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512)) \n",
    "        self.conv5 = conv_block(512, 1028, pool=True) \n",
    "        self.res3 = nn.Sequential(conv_block(1028, 1028), conv_block(1028, 1028))  \n",
    "        \n",
    "        self.classifier_parent = nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
    "                                        nn.Flatten(), # 1028 \n",
    "                                        nn.Linear(1028, 20))\n",
    "        \n",
    "        self.classifier_child = nn.Sequential(self.classifier_parent,\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(20, num_classes)) # 1028 -> 100\n",
    "        \n",
    "        # -------------------\n",
    "\n",
    "        # self.thousand_tofifty=nn.Sequential(nn.MaxPool2d(2), # 1028 x 1 x 1\n",
    "        #                                 nn.Flatten(), # 1028 \n",
    "        #                                 nn.Linear(1028, 50))  \n",
    "        \n",
    "        # self.classifier = nn.Sequential(self.thousand_tofifty,\n",
    "        #                                 nn.ReLU(),\n",
    "        #                                 nn.Linear(50, num_classes)\n",
    "        #                                 ) # 1028 -> 100\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out) + out\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out) + out\n",
    "        out = self.conv5(out)\n",
    "        out = self.res3(out) + out\n",
    "        out = self.classifier_child(out)\n",
    "        #parent=out.argmax(dim=1, keepdim=True)\n",
    "        #print(\"parent shape \", parent.shape)\n",
    "        #delete=delete.cpu().numpy().argmax()\n",
    "        #if delete not in self.unique_outs:\n",
    "        #    self.unique_outs.append(delete)\n",
    "        #print(self.unique_outs)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in test_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, test_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up cutom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, test_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate testing accuracy, predicted label, confusion matrix, and table for classification report\n",
    "def test_label_predictions(model, device, test_loader, fine):\n",
    "    model.eval()\n",
    "    actuals = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        if fine:\n",
    "            for data, target, _ in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                prediction = output.argmax(dim=1, keepdim=True)\n",
    "                print(\"predicted \", prediction, \" actual: \", target)\n",
    "                actuals.extend(target.view_as(prediction))\n",
    "                predictions.extend(prediction)\n",
    "        else:\n",
    "            for data, _, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                prediction = output.argmax(dim=1, keepdim=True)\n",
    "                print(\"predicted \", prediction, \" actual: \", target)\n",
    "                actuals.extend(target.view_as(prediction))\n",
    "                predictions.extend(prediction)\n",
    "\n",
    "    return [i.item() for i in actuals], [i.item() for i in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model20To100 = ResNet9(3, 100, True)\n",
    "model20To100.load_state_dict(torch.load('group_1028_to_parent_Loss_parent_and_child_pretrained_model.h5'))\n",
    "for param in model20To100.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet9(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(512, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier_parent): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=1028, out_features=20, bias=True)\n",
       "  )\n",
       "  (classifier_child): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=1028, out_features=20, bias=True)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model20To100.classifier_child = nn.Sequential(model20To100.classifier_parent,\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(20, 5)\n",
    "                                        )\n",
    "                                        \n",
    "model20To100=to_device(model20To100, device)\n",
    "model20To100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param in model20To100.named_parameters():\n",
    "#     print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 13.6788911819458, 'val_acc': 0.20000000298023224}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial evaluation\n",
    "history = [evaluate(model20To100, testloader)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00004, train_loss: 13.8512, val_loss: 13.2686, val_acc: 0.1980\n",
      "Epoch [1], last_lr: 0.00005, train_loss: 13.7009, val_loss: 13.1755, val_acc: 0.1960\n",
      "Epoch [2], last_lr: 0.00005, train_loss: 13.6189, val_loss: 13.1191, val_acc: 0.2000\n",
      "Epoch [3], last_lr: 0.00007, train_loss: 13.4872, val_loss: 13.0010, val_acc: 0.1980\n",
      "Epoch [4], last_lr: 0.00008, train_loss: 13.2468, val_loss: 12.8292, val_acc: 0.2000\n",
      "Epoch [5], last_lr: 0.00010, train_loss: 13.0427, val_loss: 12.5910, val_acc: 0.1980\n",
      "Epoch [6], last_lr: 0.00012, train_loss: 12.7924, val_loss: 12.2832, val_acc: 0.1980\n",
      "Epoch [7], last_lr: 0.00015, train_loss: 12.4032, val_loss: 11.9387, val_acc: 0.1900\n",
      "Epoch [8], last_lr: 0.00018, train_loss: 12.0350, val_loss: 11.5159, val_acc: 0.1840\n",
      "Epoch [9], last_lr: 0.00021, train_loss: 11.5175, val_loss: 11.0218, val_acc: 0.1820\n",
      "Epoch [10], last_lr: 0.00024, train_loss: 11.0735, val_loss: 10.4758, val_acc: 0.1660\n",
      "Epoch [11], last_lr: 0.00028, train_loss: 10.5729, val_loss: 9.8877, val_acc: 0.1660\n",
      "Epoch [12], last_lr: 0.00031, train_loss: 9.9625, val_loss: 9.2678, val_acc: 0.1640\n",
      "Epoch [13], last_lr: 0.00035, train_loss: 9.3193, val_loss: 8.5672, val_acc: 0.1520\n",
      "Epoch [14], last_lr: 0.00039, train_loss: 8.5942, val_loss: 7.8705, val_acc: 0.1460\n",
      "Epoch [15], last_lr: 0.00043, train_loss: 7.8626, val_loss: 7.2471, val_acc: 0.1400\n",
      "Epoch [16], last_lr: 0.00048, train_loss: 7.2760, val_loss: 6.7825, val_acc: 0.1380\n",
      "Epoch [17], last_lr: 0.00052, train_loss: 6.8476, val_loss: 6.4211, val_acc: 0.1400\n",
      "Epoch [18], last_lr: 0.00056, train_loss: 6.4120, val_loss: 6.0485, val_acc: 0.1400\n",
      "Epoch [19], last_lr: 0.00060, train_loss: 6.0505, val_loss: 5.6070, val_acc: 0.1380\n",
      "Epoch [20], last_lr: 0.00064, train_loss: 5.5305, val_loss: 5.1168, val_acc: 0.1440\n",
      "Epoch [21], last_lr: 0.00068, train_loss: 5.0573, val_loss: 4.5740, val_acc: 0.1360\n",
      "Epoch [22], last_lr: 0.00072, train_loss: 4.4783, val_loss: 4.0506, val_acc: 0.1400\n",
      "Epoch [23], last_lr: 0.00076, train_loss: 3.9559, val_loss: 3.6495, val_acc: 0.1540\n",
      "Epoch [24], last_lr: 0.00079, train_loss: 3.6250, val_loss: 3.4904, val_acc: 0.1620\n",
      "Epoch [25], last_lr: 0.00083, train_loss: 3.4541, val_loss: 3.3211, val_acc: 0.1840\n",
      "Epoch [26], last_lr: 0.00086, train_loss: 3.2646, val_loss: 3.1337, val_acc: 0.1920\n",
      "Epoch [27], last_lr: 0.00089, train_loss: 3.0760, val_loss: 2.9491, val_acc: 0.2140\n",
      "Epoch [28], last_lr: 0.00091, train_loss: 2.8481, val_loss: 2.7813, val_acc: 0.2160\n",
      "Epoch [29], last_lr: 0.00094, train_loss: 2.6883, val_loss: 2.6166, val_acc: 0.2320\n",
      "Epoch [30], last_lr: 0.00095, train_loss: 2.4901, val_loss: 2.4580, val_acc: 0.2560\n",
      "Epoch [31], last_lr: 0.00097, train_loss: 2.3607, val_loss: 2.3102, val_acc: 0.2880\n",
      "Epoch [32], last_lr: 0.00098, train_loss: 2.2130, val_loss: 2.1763, val_acc: 0.3080\n",
      "Epoch [33], last_lr: 0.00099, train_loss: 2.0622, val_loss: 2.0606, val_acc: 0.3320\n",
      "Epoch [34], last_lr: 0.00100, train_loss: 1.9140, val_loss: 1.9589, val_acc: 0.3700\n",
      "Epoch [35], last_lr: 0.00100, train_loss: 1.8208, val_loss: 1.8604, val_acc: 0.4000\n",
      "Epoch [36], last_lr: 0.00100, train_loss: 1.7279, val_loss: 1.7801, val_acc: 0.4200\n",
      "Epoch [37], last_lr: 0.00100, train_loss: 1.6279, val_loss: 1.7056, val_acc: 0.4340\n",
      "Epoch [38], last_lr: 0.00100, train_loss: 1.5317, val_loss: 1.6293, val_acc: 0.4460\n",
      "Epoch [39], last_lr: 0.00099, train_loss: 1.4657, val_loss: 1.5637, val_acc: 0.4640\n",
      "Epoch [40], last_lr: 0.00099, train_loss: 1.3935, val_loss: 1.5043, val_acc: 0.4760\n",
      "Epoch [41], last_lr: 0.00099, train_loss: 1.3117, val_loss: 1.4570, val_acc: 0.5080\n",
      "Epoch [42], last_lr: 0.00098, train_loss: 1.2621, val_loss: 1.4122, val_acc: 0.5180\n",
      "Epoch [43], last_lr: 0.00098, train_loss: 1.2228, val_loss: 1.3698, val_acc: 0.5280\n",
      "Epoch [44], last_lr: 0.00097, train_loss: 1.2045, val_loss: 1.3296, val_acc: 0.5320\n",
      "Epoch [45], last_lr: 0.00097, train_loss: 1.1077, val_loss: 1.2993, val_acc: 0.5340\n",
      "Epoch [46], last_lr: 0.00096, train_loss: 1.1135, val_loss: 1.2683, val_acc: 0.5460\n",
      "Epoch [47], last_lr: 0.00095, train_loss: 1.0640, val_loss: 1.2443, val_acc: 0.5580\n",
      "Epoch [48], last_lr: 0.00094, train_loss: 1.0350, val_loss: 1.2092, val_acc: 0.5620\n",
      "Epoch [49], last_lr: 0.00093, train_loss: 1.0097, val_loss: 1.1867, val_acc: 0.5760\n",
      "Epoch [50], last_lr: 0.00092, train_loss: 0.9723, val_loss: 1.1625, val_acc: 0.5800\n",
      "Epoch [51], last_lr: 0.00091, train_loss: 0.9549, val_loss: 1.1472, val_acc: 0.5800\n",
      "Epoch [52], last_lr: 0.00090, train_loss: 0.9301, val_loss: 1.1269, val_acc: 0.5900\n",
      "Epoch [53], last_lr: 0.00089, train_loss: 0.9019, val_loss: 1.1131, val_acc: 0.5960\n",
      "Epoch [54], last_lr: 0.00088, train_loss: 0.8778, val_loss: 1.0999, val_acc: 0.5920\n",
      "Epoch [55], last_lr: 0.00087, train_loss: 0.8943, val_loss: 1.0862, val_acc: 0.6040\n",
      "Epoch [56], last_lr: 0.00085, train_loss: 0.8681, val_loss: 1.0767, val_acc: 0.6020\n",
      "Epoch [57], last_lr: 0.00084, train_loss: 0.8553, val_loss: 1.0654, val_acc: 0.6000\n",
      "Epoch [58], last_lr: 0.00083, train_loss: 0.8340, val_loss: 1.0527, val_acc: 0.6040\n",
      "Epoch [59], last_lr: 0.00081, train_loss: 0.8223, val_loss: 1.0451, val_acc: 0.6120\n",
      "Epoch [60], last_lr: 0.00080, train_loss: 0.7961, val_loss: 1.0338, val_acc: 0.6200\n",
      "Epoch [61], last_lr: 0.00078, train_loss: 0.7933, val_loss: 1.0242, val_acc: 0.6180\n",
      "Epoch [62], last_lr: 0.00077, train_loss: 0.7813, val_loss: 1.0150, val_acc: 0.6240\n",
      "Epoch [63], last_lr: 0.00075, train_loss: 0.7755, val_loss: 1.0109, val_acc: 0.6260\n",
      "Epoch [64], last_lr: 0.00073, train_loss: 0.7708, val_loss: 1.0027, val_acc: 0.6240\n",
      "Epoch [65], last_lr: 0.00072, train_loss: 0.7826, val_loss: 0.9960, val_acc: 0.6280\n",
      "Epoch [66], last_lr: 0.00070, train_loss: 0.7869, val_loss: 0.9928, val_acc: 0.6320\n",
      "Epoch [67], last_lr: 0.00068, train_loss: 0.7457, val_loss: 0.9846, val_acc: 0.6320\n",
      "Epoch [68], last_lr: 0.00067, train_loss: 0.7286, val_loss: 0.9793, val_acc: 0.6340\n",
      "Epoch [69], last_lr: 0.00065, train_loss: 0.7505, val_loss: 0.9754, val_acc: 0.6240\n",
      "Epoch [70], last_lr: 0.00063, train_loss: 0.7475, val_loss: 0.9698, val_acc: 0.6400\n",
      "Epoch [71], last_lr: 0.00061, train_loss: 0.7462, val_loss: 0.9675, val_acc: 0.6320\n",
      "Epoch [72], last_lr: 0.00059, train_loss: 0.7249, val_loss: 0.9656, val_acc: 0.6260\n",
      "Epoch [73], last_lr: 0.00057, train_loss: 0.7253, val_loss: 0.9629, val_acc: 0.6380\n",
      "Epoch [74], last_lr: 0.00056, train_loss: 0.7071, val_loss: 0.9585, val_acc: 0.6360\n",
      "Epoch [75], last_lr: 0.00054, train_loss: 0.7449, val_loss: 0.9565, val_acc: 0.6320\n",
      "Epoch [76], last_lr: 0.00052, train_loss: 0.7053, val_loss: 0.9518, val_acc: 0.6400\n",
      "Epoch [77], last_lr: 0.00050, train_loss: 0.7142, val_loss: 0.9515, val_acc: 0.6380\n",
      "Epoch [78], last_lr: 0.00048, train_loss: 0.7022, val_loss: 0.9501, val_acc: 0.6420\n",
      "Epoch [79], last_lr: 0.00046, train_loss: 0.7004, val_loss: 0.9470, val_acc: 0.6360\n",
      "Epoch [80], last_lr: 0.00044, train_loss: 0.6933, val_loss: 0.9459, val_acc: 0.6420\n",
      "Epoch [81], last_lr: 0.00043, train_loss: 0.6914, val_loss: 0.9460, val_acc: 0.6520\n",
      "Epoch [82], last_lr: 0.00041, train_loss: 0.6901, val_loss: 0.9462, val_acc: 0.6480\n",
      "Epoch [83], last_lr: 0.00039, train_loss: 0.7129, val_loss: 0.9418, val_acc: 0.6460\n",
      "Epoch [84], last_lr: 0.00037, train_loss: 0.7028, val_loss: 0.9410, val_acc: 0.6460\n",
      "Epoch [85], last_lr: 0.00035, train_loss: 0.6888, val_loss: 0.9423, val_acc: 0.6420\n",
      "Epoch [86], last_lr: 0.00033, train_loss: 0.6992, val_loss: 0.9402, val_acc: 0.6420\n",
      "Epoch [87], last_lr: 0.00032, train_loss: 0.6942, val_loss: 0.9377, val_acc: 0.6520\n",
      "Epoch [88], last_lr: 0.00030, train_loss: 0.7004, val_loss: 0.9373, val_acc: 0.6540\n",
      "Epoch [89], last_lr: 0.00028, train_loss: 0.6882, val_loss: 0.9375, val_acc: 0.6460\n",
      "Epoch [90], last_lr: 0.00027, train_loss: 0.6961, val_loss: 0.9382, val_acc: 0.6460\n",
      "Epoch [91], last_lr: 0.00025, train_loss: 0.6896, val_loss: 0.9366, val_acc: 0.6460\n",
      "Epoch [92], last_lr: 0.00023, train_loss: 0.6835, val_loss: 0.9360, val_acc: 0.6500\n",
      "Epoch [93], last_lr: 0.00022, train_loss: 0.6940, val_loss: 0.9330, val_acc: 0.6500\n",
      "Epoch [94], last_lr: 0.00020, train_loss: 0.6952, val_loss: 0.9344, val_acc: 0.6460\n",
      "Epoch [95], last_lr: 0.00019, train_loss: 0.6784, val_loss: 0.9341, val_acc: 0.6480\n",
      "Epoch [96], last_lr: 0.00017, train_loss: 0.6666, val_loss: 0.9334, val_acc: 0.6500\n",
      "Epoch [97], last_lr: 0.00016, train_loss: 0.6886, val_loss: 0.9327, val_acc: 0.6500\n",
      "Epoch [98], last_lr: 0.00015, train_loss: 0.6767, val_loss: 0.9329, val_acc: 0.6480\n",
      "Epoch [99], last_lr: 0.00013, train_loss: 0.6794, val_loss: 0.9326, val_acc: 0.6500\n",
      "Epoch [100], last_lr: 0.00012, train_loss: 0.6708, val_loss: 0.9316, val_acc: 0.6500\n",
      "Epoch [101], last_lr: 0.00011, train_loss: 0.6966, val_loss: 0.9298, val_acc: 0.6520\n",
      "Epoch [102], last_lr: 0.00010, train_loss: 0.6682, val_loss: 0.9301, val_acc: 0.6500\n",
      "Epoch [103], last_lr: 0.00009, train_loss: 0.6872, val_loss: 0.9297, val_acc: 0.6520\n",
      "Epoch [104], last_lr: 0.00008, train_loss: 0.6761, val_loss: 0.9308, val_acc: 0.6520\n",
      "Epoch [105], last_lr: 0.00007, train_loss: 0.6774, val_loss: 0.9290, val_acc: 0.6500\n",
      "Epoch [106], last_lr: 0.00006, train_loss: 0.6830, val_loss: 0.9289, val_acc: 0.6520\n",
      "Epoch [107], last_lr: 0.00005, train_loss: 0.6674, val_loss: 0.9291, val_acc: 0.6500\n",
      "Epoch [108], last_lr: 0.00004, train_loss: 0.6990, val_loss: 0.9298, val_acc: 0.6480\n",
      "Epoch [109], last_lr: 0.00003, train_loss: 0.6738, val_loss: 0.9300, val_acc: 0.6480\n",
      "Epoch [110], last_lr: 0.00003, train_loss: 0.6796, val_loss: 0.9291, val_acc: 0.6500\n",
      "Epoch [111], last_lr: 0.00002, train_loss: 0.6841, val_loss: 0.9282, val_acc: 0.6480\n",
      "Epoch [112], last_lr: 0.00002, train_loss: 0.6610, val_loss: 0.9295, val_acc: 0.6520\n",
      "Epoch [113], last_lr: 0.00001, train_loss: 0.6628, val_loss: 0.9293, val_acc: 0.6540\n",
      "Epoch [114], last_lr: 0.00001, train_loss: 0.6679, val_loss: 0.9290, val_acc: 0.6540\n",
      "Epoch [115], last_lr: 0.00001, train_loss: 0.6701, val_loss: 0.9290, val_acc: 0.6540\n",
      "Epoch [116], last_lr: 0.00000, train_loss: 0.6684, val_loss: 0.9299, val_acc: 0.6540\n",
      "Epoch [117], last_lr: 0.00000, train_loss: 0.6852, val_loss: 0.9281, val_acc: 0.6540\n",
      "Epoch [118], last_lr: 0.00000, train_loss: 0.6723, val_loss: 0.9278, val_acc: 0.6520\n",
      "Epoch [119], last_lr: 0.00000, train_loss: 0.6734, val_loss: 0.9279, val_acc: 0.6520\n"
     ]
    }
   ],
   "source": [
    "# Fitting the first 1/4 epochs\n",
    "#current_time=time.time()\n",
    "history += fit_one_cycle(int(epochs), max_lr, model20To100, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00000, train_loss: 0.6604, val_loss: 0.9288, val_acc: 0.6520\n",
      "Epoch [1], last_lr: 0.00000, train_loss: 0.6897, val_loss: 0.9297, val_acc: 0.6520\n",
      "Epoch [2], last_lr: 0.00001, train_loss: 0.6855, val_loss: 0.9297, val_acc: 0.6520\n",
      "Epoch [3], last_lr: 0.00001, train_loss: 0.6751, val_loss: 0.9284, val_acc: 0.6520\n",
      "Epoch [4], last_lr: 0.00001, train_loss: 0.6805, val_loss: 0.9294, val_acc: 0.6520\n",
      "Epoch [5], last_lr: 0.00001, train_loss: 0.6617, val_loss: 0.9287, val_acc: 0.6540\n",
      "Epoch [6], last_lr: 0.00001, train_loss: 0.6667, val_loss: 0.9299, val_acc: 0.6540\n",
      "Epoch [7], last_lr: 0.00001, train_loss: 0.6660, val_loss: 0.9295, val_acc: 0.6540\n",
      "Epoch [8], last_lr: 0.00002, train_loss: 0.6655, val_loss: 0.9301, val_acc: 0.6520\n",
      "Epoch [9], last_lr: 0.00002, train_loss: 0.6705, val_loss: 0.9307, val_acc: 0.6540\n",
      "Epoch [10], last_lr: 0.00002, train_loss: 0.6857, val_loss: 0.9308, val_acc: 0.6540\n",
      "Epoch [11], last_lr: 0.00003, train_loss: 0.6775, val_loss: 0.9298, val_acc: 0.6520\n",
      "Epoch [12], last_lr: 0.00003, train_loss: 0.6963, val_loss: 0.9294, val_acc: 0.6520\n",
      "Epoch [13], last_lr: 0.00004, train_loss: 0.6714, val_loss: 0.9299, val_acc: 0.6520\n",
      "Epoch [14], last_lr: 0.00004, train_loss: 0.6821, val_loss: 0.9298, val_acc: 0.6520\n",
      "Epoch [15], last_lr: 0.00004, train_loss: 0.6747, val_loss: 0.9283, val_acc: 0.6500\n",
      "Epoch [16], last_lr: 0.00005, train_loss: 0.6752, val_loss: 0.9292, val_acc: 0.6500\n",
      "Epoch [17], last_lr: 0.00005, train_loss: 0.6683, val_loss: 0.9287, val_acc: 0.6500\n",
      "Epoch [18], last_lr: 0.00006, train_loss: 0.6874, val_loss: 0.9270, val_acc: 0.6540\n",
      "Epoch [19], last_lr: 0.00006, train_loss: 0.6976, val_loss: 0.9267, val_acc: 0.6540\n",
      "Epoch [20], last_lr: 0.00006, train_loss: 0.6503, val_loss: 0.9276, val_acc: 0.6540\n",
      "Epoch [21], last_lr: 0.00007, train_loss: 0.6636, val_loss: 0.9283, val_acc: 0.6540\n",
      "Epoch [22], last_lr: 0.00007, train_loss: 0.6809, val_loss: 0.9269, val_acc: 0.6540\n",
      "Epoch [23], last_lr: 0.00008, train_loss: 0.6719, val_loss: 0.9268, val_acc: 0.6540\n",
      "Epoch [24], last_lr: 0.00008, train_loss: 0.6729, val_loss: 0.9272, val_acc: 0.6520\n",
      "Epoch [25], last_lr: 0.00008, train_loss: 0.6744, val_loss: 0.9263, val_acc: 0.6520\n",
      "Epoch [26], last_lr: 0.00009, train_loss: 0.6802, val_loss: 0.9271, val_acc: 0.6520\n",
      "Epoch [27], last_lr: 0.00009, train_loss: 0.6503, val_loss: 0.9260, val_acc: 0.6520\n",
      "Epoch [28], last_lr: 0.00009, train_loss: 0.6867, val_loss: 0.9265, val_acc: 0.6480\n",
      "Epoch [29], last_lr: 0.00009, train_loss: 0.6814, val_loss: 0.9256, val_acc: 0.6460\n",
      "Epoch [30], last_lr: 0.00010, train_loss: 0.6799, val_loss: 0.9263, val_acc: 0.6520\n",
      "Epoch [31], last_lr: 0.00010, train_loss: 0.6569, val_loss: 0.9262, val_acc: 0.6520\n",
      "Epoch [32], last_lr: 0.00010, train_loss: 0.6702, val_loss: 0.9257, val_acc: 0.6540\n",
      "Epoch [33], last_lr: 0.00010, train_loss: 0.6665, val_loss: 0.9264, val_acc: 0.6520\n",
      "Epoch [34], last_lr: 0.00010, train_loss: 0.6725, val_loss: 0.9264, val_acc: 0.6520\n",
      "Epoch [35], last_lr: 0.00010, train_loss: 0.6764, val_loss: 0.9272, val_acc: 0.6520\n",
      "Epoch [36], last_lr: 0.00010, train_loss: 0.6734, val_loss: 0.9271, val_acc: 0.6520\n",
      "Epoch [37], last_lr: 0.00010, train_loss: 0.6644, val_loss: 0.9256, val_acc: 0.6520\n",
      "Epoch [38], last_lr: 0.00010, train_loss: 0.6668, val_loss: 0.9246, val_acc: 0.6520\n",
      "Epoch [39], last_lr: 0.00010, train_loss: 0.6817, val_loss: 0.9253, val_acc: 0.6520\n",
      "Epoch [40], last_lr: 0.00010, train_loss: 0.6715, val_loss: 0.9255, val_acc: 0.6500\n",
      "Epoch [41], last_lr: 0.00010, train_loss: 0.6727, val_loss: 0.9237, val_acc: 0.6500\n",
      "Epoch [42], last_lr: 0.00010, train_loss: 0.6832, val_loss: 0.9230, val_acc: 0.6520\n",
      "Epoch [43], last_lr: 0.00010, train_loss: 0.6646, val_loss: 0.9218, val_acc: 0.6520\n",
      "Epoch [44], last_lr: 0.00010, train_loss: 0.6682, val_loss: 0.9221, val_acc: 0.6520\n",
      "Epoch [45], last_lr: 0.00010, train_loss: 0.6606, val_loss: 0.9214, val_acc: 0.6520\n",
      "Epoch [46], last_lr: 0.00010, train_loss: 0.6752, val_loss: 0.9226, val_acc: 0.6520\n",
      "Epoch [47], last_lr: 0.00010, train_loss: 0.6357, val_loss: 0.9220, val_acc: 0.6520\n",
      "Epoch [48], last_lr: 0.00009, train_loss: 0.6595, val_loss: 0.9208, val_acc: 0.6520\n",
      "Epoch [49], last_lr: 0.00009, train_loss: 0.6538, val_loss: 0.9236, val_acc: 0.6520\n",
      "Epoch [50], last_lr: 0.00009, train_loss: 0.6595, val_loss: 0.9236, val_acc: 0.6520\n",
      "Epoch [51], last_lr: 0.00009, train_loss: 0.6596, val_loss: 0.9231, val_acc: 0.6520\n",
      "Epoch [52], last_lr: 0.00009, train_loss: 0.6742, val_loss: 0.9193, val_acc: 0.6520\n",
      "Epoch [53], last_lr: 0.00009, train_loss: 0.6749, val_loss: 0.9219, val_acc: 0.6480\n",
      "Epoch [54], last_lr: 0.00009, train_loss: 0.6688, val_loss: 0.9220, val_acc: 0.6500\n",
      "Epoch [55], last_lr: 0.00009, train_loss: 0.6604, val_loss: 0.9221, val_acc: 0.6500\n",
      "Epoch [56], last_lr: 0.00009, train_loss: 0.6715, val_loss: 0.9219, val_acc: 0.6520\n",
      "Epoch [57], last_lr: 0.00008, train_loss: 0.6582, val_loss: 0.9222, val_acc: 0.6500\n",
      "Epoch [58], last_lr: 0.00008, train_loss: 0.6685, val_loss: 0.9218, val_acc: 0.6520\n",
      "Epoch [59], last_lr: 0.00008, train_loss: 0.6553, val_loss: 0.9221, val_acc: 0.6500\n",
      "Epoch [60], last_lr: 0.00008, train_loss: 0.6397, val_loss: 0.9217, val_acc: 0.6500\n",
      "Epoch [61], last_lr: 0.00008, train_loss: 0.6526, val_loss: 0.9208, val_acc: 0.6500\n",
      "Epoch [62], last_lr: 0.00008, train_loss: 0.6688, val_loss: 0.9206, val_acc: 0.6480\n",
      "Epoch [63], last_lr: 0.00008, train_loss: 0.6657, val_loss: 0.9207, val_acc: 0.6500\n",
      "Epoch [64], last_lr: 0.00007, train_loss: 0.6644, val_loss: 0.9202, val_acc: 0.6480\n",
      "Epoch [65], last_lr: 0.00007, train_loss: 0.6684, val_loss: 0.9200, val_acc: 0.6500\n",
      "Epoch [66], last_lr: 0.00007, train_loss: 0.6753, val_loss: 0.9192, val_acc: 0.6500\n",
      "Epoch [67], last_lr: 0.00007, train_loss: 0.6490, val_loss: 0.9179, val_acc: 0.6520\n",
      "Epoch [68], last_lr: 0.00007, train_loss: 0.6607, val_loss: 0.9191, val_acc: 0.6520\n",
      "Epoch [69], last_lr: 0.00006, train_loss: 0.6801, val_loss: 0.9203, val_acc: 0.6500\n",
      "Epoch [70], last_lr: 0.00006, train_loss: 0.6582, val_loss: 0.9193, val_acc: 0.6500\n",
      "Epoch [71], last_lr: 0.00006, train_loss: 0.6617, val_loss: 0.9196, val_acc: 0.6480\n",
      "Epoch [72], last_lr: 0.00006, train_loss: 0.6667, val_loss: 0.9213, val_acc: 0.6480\n",
      "Epoch [73], last_lr: 0.00006, train_loss: 0.6592, val_loss: 0.9203, val_acc: 0.6500\n",
      "Epoch [74], last_lr: 0.00006, train_loss: 0.6452, val_loss: 0.9205, val_acc: 0.6500\n",
      "Epoch [75], last_lr: 0.00005, train_loss: 0.6568, val_loss: 0.9211, val_acc: 0.6480\n",
      "Epoch [76], last_lr: 0.00005, train_loss: 0.6469, val_loss: 0.9205, val_acc: 0.6480\n",
      "Epoch [77], last_lr: 0.00005, train_loss: 0.6653, val_loss: 0.9205, val_acc: 0.6480\n",
      "Epoch [78], last_lr: 0.00005, train_loss: 0.6584, val_loss: 0.9188, val_acc: 0.6480\n",
      "Epoch [79], last_lr: 0.00005, train_loss: 0.6531, val_loss: 0.9179, val_acc: 0.6480\n",
      "Epoch [80], last_lr: 0.00004, train_loss: 0.6488, val_loss: 0.9183, val_acc: 0.6500\n",
      "Epoch [81], last_lr: 0.00004, train_loss: 0.6594, val_loss: 0.9187, val_acc: 0.6480\n",
      "Epoch [82], last_lr: 0.00004, train_loss: 0.6481, val_loss: 0.9192, val_acc: 0.6500\n",
      "Epoch [83], last_lr: 0.00004, train_loss: 0.6671, val_loss: 0.9188, val_acc: 0.6480\n",
      "Epoch [84], last_lr: 0.00004, train_loss: 0.6511, val_loss: 0.9206, val_acc: 0.6480\n",
      "Epoch [85], last_lr: 0.00004, train_loss: 0.6753, val_loss: 0.9186, val_acc: 0.6480\n",
      "Epoch [86], last_lr: 0.00003, train_loss: 0.6638, val_loss: 0.9190, val_acc: 0.6480\n",
      "Epoch [87], last_lr: 0.00003, train_loss: 0.6719, val_loss: 0.9200, val_acc: 0.6460\n",
      "Epoch [88], last_lr: 0.00003, train_loss: 0.6796, val_loss: 0.9180, val_acc: 0.6500\n",
      "Epoch [89], last_lr: 0.00003, train_loss: 0.6546, val_loss: 0.9172, val_acc: 0.6480\n",
      "Epoch [90], last_lr: 0.00003, train_loss: 0.6595, val_loss: 0.9184, val_acc: 0.6500\n",
      "Epoch [91], last_lr: 0.00003, train_loss: 0.6497, val_loss: 0.9169, val_acc: 0.6500\n",
      "Epoch [92], last_lr: 0.00002, train_loss: 0.6774, val_loss: 0.9185, val_acc: 0.6500\n",
      "Epoch [93], last_lr: 0.00002, train_loss: 0.6711, val_loss: 0.9171, val_acc: 0.6460\n",
      "Epoch [94], last_lr: 0.00002, train_loss: 0.6728, val_loss: 0.9187, val_acc: 0.6500\n",
      "Epoch [95], last_lr: 0.00002, train_loss: 0.6522, val_loss: 0.9192, val_acc: 0.6480\n",
      "Epoch [96], last_lr: 0.00002, train_loss: 0.6564, val_loss: 0.9189, val_acc: 0.6480\n",
      "Epoch [97], last_lr: 0.00002, train_loss: 0.6684, val_loss: 0.9184, val_acc: 0.6480\n",
      "Epoch [98], last_lr: 0.00001, train_loss: 0.6571, val_loss: 0.9195, val_acc: 0.6480\n",
      "Epoch [99], last_lr: 0.00001, train_loss: 0.6580, val_loss: 0.9186, val_acc: 0.6480\n",
      "Epoch [100], last_lr: 0.00001, train_loss: 0.6650, val_loss: 0.9169, val_acc: 0.6460\n",
      "Epoch [101], last_lr: 0.00001, train_loss: 0.6384, val_loss: 0.9178, val_acc: 0.6440\n",
      "Epoch [102], last_lr: 0.00001, train_loss: 0.6535, val_loss: 0.9193, val_acc: 0.6460\n",
      "Epoch [103], last_lr: 0.00001, train_loss: 0.6496, val_loss: 0.9190, val_acc: 0.6480\n",
      "Epoch [104], last_lr: 0.00001, train_loss: 0.6596, val_loss: 0.9189, val_acc: 0.6500\n",
      "Epoch [105], last_lr: 0.00001, train_loss: 0.6644, val_loss: 0.9191, val_acc: 0.6480\n",
      "Epoch [106], last_lr: 0.00001, train_loss: 0.6571, val_loss: 0.9171, val_acc: 0.6500\n",
      "Epoch [107], last_lr: 0.00000, train_loss: 0.6710, val_loss: 0.9180, val_acc: 0.6500\n",
      "Epoch [108], last_lr: 0.00000, train_loss: 0.6676, val_loss: 0.9175, val_acc: 0.6500\n",
      "Epoch [109], last_lr: 0.00000, train_loss: 0.6611, val_loss: 0.9180, val_acc: 0.6500\n",
      "Epoch [110], last_lr: 0.00000, train_loss: 0.6685, val_loss: 0.9185, val_acc: 0.6480\n",
      "Epoch [111], last_lr: 0.00000, train_loss: 0.6401, val_loss: 0.9186, val_acc: 0.6480\n",
      "Epoch [112], last_lr: 0.00000, train_loss: 0.6505, val_loss: 0.9186, val_acc: 0.6460\n",
      "Epoch [113], last_lr: 0.00000, train_loss: 0.6566, val_loss: 0.9183, val_acc: 0.6480\n",
      "Epoch [114], last_lr: 0.00000, train_loss: 0.6653, val_loss: 0.9195, val_acc: 0.6480\n",
      "Epoch [115], last_lr: 0.00000, train_loss: 0.6694, val_loss: 0.9206, val_acc: 0.6480\n",
      "Epoch [116], last_lr: 0.00000, train_loss: 0.6832, val_loss: 0.9193, val_acc: 0.6460\n",
      "Epoch [117], last_lr: 0.00000, train_loss: 0.6714, val_loss: 0.9185, val_acc: 0.6500\n",
      "Epoch [118], last_lr: 0.00000, train_loss: 0.6736, val_loss: 0.9187, val_acc: 0.6500\n",
      "Epoch [119], last_lr: 0.00000, train_loss: 0.6369, val_loss: 0.9192, val_acc: 0.6460\n"
     ]
    }
   ],
   "source": [
    "# Fitting the second 1/4 epochs\n",
    "history += fit_one_cycle(int(epochs), max_lr/10, model20To100, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[369  22   0   0   1   5   1   2   7   4   2  10  19  11   4  19  23   0\n",
      "    1   0]\n",
      " [ 28 384   5   4   6   7   1   4   1   1   3   4   2   3   6  23  13   0\n",
      "    3   2]\n",
      " [  0   3 463   1   8   3   0  13   0   1   1   0   0   3   1   2   0   0\n",
      "    1   0]\n",
      " [  0   5   1 416   7  28   3   1   1   4   0   2   4   9   7   5   0   1\n",
      "    1   5]\n",
      " [  2   2  17   7 430   6   1   4   1   2   2   5   2   8   0   4   5   0\n",
      "    0   2]\n",
      " [  2   1   1  27   5 403  17   5   0   4   2   0   1   2  10   4   4   1\n",
      "    6   5]\n",
      " [  2   0   1  14   1  18 431   3   1   3   2   0   0   2   4   5   2   0\n",
      "    6   5]\n",
      " [  3   4  10   2   2   2   0 420   3   1   0   1   6  23   4  14   3   0\n",
      "    0   2]\n",
      " [ 27   0   0   1   3   1   2   1 384   0   1  20  23   4   2  13  16   0\n",
      "    2   0]\n",
      " [  1   0   0   1   1   5   3   0   0 449   9   2   0   2   0   1   0   3\n",
      "    8  15]\n",
      " [  2   4   3   0   2   1   2   6   4   9 436   2   3   1   0   2   1  16\n",
      "    1   5]\n",
      " [ 15   1   0   1   4   3   0   1  14   2   2 401   9   4   6  14  19   2\n",
      "    1   1]\n",
      " [ 17   3   2   1   1   0   2   8  20   1   0   9 381   9   1  13  27   0\n",
      "    1   4]\n",
      " [  2   8   5   7   5   7   2  27   3   0   1   6   4 380   3  29   6   1\n",
      "    1   3]\n",
      " [  2   4   3   2   6   4   3   3   0   0   0   8   0   4 453   3   3   0\n",
      "    1   1]\n",
      " [ 27  23   2   3   1   1   1  15   6   2   2  11   7  34   2 347  11   1\n",
      "    1   3]\n",
      " [ 29   4   3   3   4   0   2  11  14   0   1  14  22   8   2  13 367   0\n",
      "    1   2]\n",
      " [  1   2   0   0   2   2   0   1   1   6  12   0   2   3   0   2   1 464\n",
      "    1   0]\n",
      " [  0   1   0   4   0   6   1   1   0   7   0   0   1   2   1   1   0   0\n",
      "  448  27]\n",
      " [  3   0   0   4   2   7   6   4   0  18   2   1   1   1   0   3   0   2\n",
      "   41 405]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.74      0.72       500\n",
      "           1       0.82      0.77      0.79       500\n",
      "           2       0.90      0.93      0.91       500\n",
      "           3       0.84      0.83      0.83       500\n",
      "           4       0.88      0.86      0.87       500\n",
      "           5       0.79      0.81      0.80       500\n",
      "           6       0.90      0.86      0.88       500\n",
      "           7       0.79      0.84      0.82       500\n",
      "           8       0.83      0.77      0.80       500\n",
      "           9       0.87      0.90      0.89       500\n",
      "          10       0.91      0.87      0.89       500\n",
      "          11       0.81      0.80      0.81       500\n",
      "          12       0.78      0.76      0.77       500\n",
      "          13       0.74      0.76      0.75       500\n",
      "          14       0.90      0.91      0.90       500\n",
      "          15       0.67      0.69      0.68       500\n",
      "          16       0.73      0.73      0.73       500\n",
      "          17       0.95      0.93      0.94       500\n",
      "          18       0.85      0.90      0.87       500\n",
      "          19       0.83      0.81      0.82       500\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "F1 score: 0.823346\n",
      "Recall score: 0.823100\n",
      "Accuracy score: 0.823100\n"
     ]
    }
   ],
   "source": [
    "y_test, y_pred = test_label_predictions(model20To100, device, testloader)\n",
    "cm=confusion_matrix(y_test, y_pred)\n",
    "cr=classification_report(y_test, y_pred)\n",
    "fs=f1_score(y_test,y_pred,average='weighted')\n",
    "rs=recall_score(y_test, y_pred,average='weighted')\n",
    "accuracy=accuracy_score(y_test, y_pred)\n",
    "print('Confusion matrix:')\n",
    "print(cm)\n",
    "print(cr)\n",
    "print('F1 score: %f' % fs)\n",
    "print('Recall score: %f' % rs)\n",
    "print('Accuracy score: %f' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('conv1.0.weight', Parameter containing:\n",
      "tensor([[[[-0.0537, -0.0406, -0.0054],\n",
      "          [-0.0047,  0.0979, -0.0671],\n",
      "          [ 0.0052,  0.0285,  0.0212]],\n",
      "\n",
      "         [[-0.1100,  0.0250,  0.1058],\n",
      "          [ 0.1139,  0.0382, -0.1640],\n",
      "          [ 0.0080,  0.0672, -0.0545]],\n",
      "\n",
      "         [[ 0.0856,  0.0807,  0.0574],\n",
      "          [ 0.0714, -0.0394, -0.1632],\n",
      "          [ 0.0013, -0.1898,  0.0771]]],\n",
      "\n",
      "\n",
      "        [[[-0.0945,  0.0627,  0.0989],\n",
      "          [ 0.0811, -0.0594, -0.1612],\n",
      "          [-0.0206, -0.0538,  0.1215]],\n",
      "\n",
      "         [[ 0.0871, -0.0288, -0.1742],\n",
      "          [ 0.1471, -0.0779, -0.0412],\n",
      "          [-0.1064,  0.1198,  0.1197]],\n",
      "\n",
      "         [[-0.0016, -0.1337,  0.0934],\n",
      "          [-0.0356, -0.0773,  0.0223],\n",
      "          [-0.1250,  0.1000,  0.1316]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0904,  0.1007,  0.0675],\n",
      "          [ 0.0560, -0.0529,  0.1005],\n",
      "          [-0.0373,  0.0333, -0.1333]],\n",
      "\n",
      "         [[-0.1216,  0.0743, -0.0036],\n",
      "          [-0.1071, -0.0565, -0.1269],\n",
      "          [-0.0621,  0.0335,  0.0935]],\n",
      "\n",
      "         [[ 0.0573,  0.1544, -0.1301],\n",
      "          [-0.0400,  0.0969,  0.0204],\n",
      "          [-0.1239, -0.1591,  0.1445]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0801, -0.1068,  0.0013],\n",
      "          [-0.1389,  0.1112,  0.1464],\n",
      "          [-0.1490, -0.0140,  0.0858]],\n",
      "\n",
      "         [[-0.0010,  0.0943, -0.0384],\n",
      "          [ 0.1113, -0.1622, -0.1065],\n",
      "          [ 0.0721,  0.1231, -0.1149]],\n",
      "\n",
      "         [[-0.1060,  0.0651,  0.1025],\n",
      "          [-0.0369, -0.0176, -0.0740],\n",
      "          [ 0.1440, -0.0558, -0.0176]]],\n",
      "\n",
      "\n",
      "        [[[-0.0774,  0.0404, -0.0910],\n",
      "          [ 0.0298, -0.1686,  0.1249],\n",
      "          [ 0.1157,  0.0215,  0.0536]],\n",
      "\n",
      "         [[ 0.0316,  0.1315,  0.1557],\n",
      "          [-0.0980, -0.1206, -0.0092],\n",
      "          [-0.0628, -0.0643, -0.0453]],\n",
      "\n",
      "         [[-0.0824,  0.0172,  0.0247],\n",
      "          [ 0.1162,  0.0402, -0.0310],\n",
      "          [ 0.0188,  0.0963, -0.1552]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0061, -0.0511,  0.0073],\n",
      "          [ 0.1442, -0.0288,  0.0470],\n",
      "          [-0.0183, -0.0826,  0.0206]],\n",
      "\n",
      "         [[ 0.0908,  0.0371, -0.0834],\n",
      "          [-0.1562,  0.0800, -0.0753],\n",
      "          [-0.0125,  0.0276,  0.0732]],\n",
      "\n",
      "         [[ 0.1546,  0.0767, -0.1307],\n",
      "          [ 0.0129,  0.0365, -0.1412],\n",
      "          [-0.0626, -0.1257,  0.1530]]]], device='cuda:0'))\n",
      "('conv1.0.bias', Parameter containing:\n",
      "tensor([-1.3028e-06,  9.1279e-06, -2.6405e-06, -9.8505e-06, -1.4473e-06,\n",
      "         7.0560e-08,  2.5192e-07, -1.0355e-07, -4.8000e-06,  2.1080e-07,\n",
      "         1.1784e-06, -1.4341e-05, -2.7098e-07,  3.0228e-05,  4.7410e-06,\n",
      "        -2.4472e-06,  3.8970e-07, -1.5548e-07, -5.7554e-06, -3.4304e-06,\n",
      "         7.2773e-06,  3.3821e-06,  6.7075e-06, -1.1492e-05,  1.9502e-05,\n",
      "        -5.9774e-07,  3.4124e-07, -4.0133e-06, -5.9061e-06, -3.3585e-06,\n",
      "         3.0863e-06, -9.2364e-07, -1.7064e-05,  3.1182e-07, -5.0024e-06,\n",
      "         2.4393e-06, -3.2291e-06,  2.4829e-06,  6.0668e-05, -8.7948e-07,\n",
      "         2.6551e-07, -5.0805e-06,  1.9030e-05, -1.8039e-06,  4.4158e-06,\n",
      "        -9.0361e-07,  1.7886e-05, -2.0608e-06, -1.6663e-06, -1.1778e-08,\n",
      "         1.9506e-06,  1.1899e-06,  1.5636e-06, -2.3185e-06, -4.5285e-07,\n",
      "        -1.4899e-06, -5.6352e-06,  3.4866e-06, -4.8523e-06, -1.0116e-06,\n",
      "        -7.7852e-06,  3.2208e-05,  1.7513e-06,  7.7285e-06], device='cuda:0'))\n",
      "('conv1.1.weight', Parameter containing:\n",
      "tensor([0.7558, 0.9982, 0.8017, 0.7177, 0.6516, 0.6588, 0.7015, 0.6193, 0.6926,\n",
      "        0.7871, 0.7764, 1.0810, 0.7448, 0.9729, 0.7005, 0.6413, 0.6530, 0.8037,\n",
      "        0.8326, 0.7023, 0.9749, 0.7292, 0.9671, 0.9707, 0.9179, 0.6616, 0.6988,\n",
      "        0.8393, 0.8775, 0.7824, 0.7459, 0.6491, 0.9884, 0.6933, 0.7541, 0.6579,\n",
      "        0.7405, 0.8748, 0.8737, 0.6199, 0.6475, 0.9848, 0.8839, 0.7357, 0.9128,\n",
      "        0.6898, 0.9061, 0.7698, 0.7137, 0.6916, 0.6817, 0.7235, 0.6462, 0.7824,\n",
      "        0.7848, 0.5938, 0.8302, 0.6299, 1.0631, 0.7289, 0.8725, 0.9430, 0.8507,\n",
      "        0.7582], device='cuda:0'))\n",
      "('conv1.1.bias', Parameter containing:\n",
      "tensor([ 0.1453,  0.2419, -0.0097,  0.2428, -0.0808, -0.1365,  0.0175, -0.1320,\n",
      "         0.0098,  0.0917,  0.1449,  0.3278,  0.1261,  0.2936,  0.0284, -0.1115,\n",
      "        -0.1350, -0.0255,  0.1919, -0.1111,  0.2066,  0.0307,  0.1391,  0.2146,\n",
      "         0.2395, -0.1809, -0.0060,  0.1352,  0.1624,  0.0256,  0.0666, -0.1243,\n",
      "         0.2744, -0.1331,  0.1045, -0.1239, -0.0092,  0.2515,  0.2800, -0.0840,\n",
      "        -0.1148,  0.2706,  0.2948,  0.0105,  0.0769, -0.0920,  0.3666,  0.0278,\n",
      "        -0.1379, -0.0454, -0.0238, -0.0415, -0.1649, -0.1095,  0.0378, -0.1426,\n",
      "         0.1812, -0.0378,  0.2963,  0.0152,  0.2769,  0.2912,  0.2321,  0.1262],\n",
      "       device='cuda:0'))\n",
      "('conv2.0.weight', Parameter containing:\n",
      "tensor([[[[ 0.0474,  0.0113, -0.0166],\n",
      "          [-0.0406, -0.0025,  0.0327],\n",
      "          [-0.0100, -0.0195, -0.0315]],\n",
      "\n",
      "         [[ 0.0216,  0.0768,  0.0064],\n",
      "          [-0.0592, -0.0724,  0.0357],\n",
      "          [-0.0453, -0.0613, -0.0155]],\n",
      "\n",
      "         [[ 0.0408,  0.0243, -0.0479],\n",
      "          [ 0.0287, -0.0113, -0.0578],\n",
      "          [-0.0004, -0.0027,  0.0107]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0829, -0.0771, -0.0120],\n",
      "          [-0.0598, -0.0506, -0.0671],\n",
      "          [ 0.0042, -0.0337, -0.0319]],\n",
      "\n",
      "         [[-0.0186, -0.0068, -0.0053],\n",
      "          [-0.0106,  0.0113, -0.0197],\n",
      "          [-0.0134,  0.0536, -0.0275]],\n",
      "\n",
      "         [[ 0.0148,  0.0370, -0.0084],\n",
      "          [ 0.0044,  0.0003, -0.0116],\n",
      "          [-0.0436, -0.0416,  0.0006]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0664, -0.0299,  0.0046],\n",
      "          [-0.0506,  0.0091,  0.0134],\n",
      "          [ 0.0009, -0.0157, -0.0421]],\n",
      "\n",
      "         [[ 0.0609,  0.0482, -0.0703],\n",
      "          [-0.0287,  0.0038,  0.0034],\n",
      "          [-0.0032, -0.0023, -0.0517]],\n",
      "\n",
      "         [[ 0.0520,  0.0263,  0.0099],\n",
      "          [-0.0329, -0.0055,  0.0034],\n",
      "          [-0.0089,  0.0046,  0.0157]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0547,  0.1100,  0.0170],\n",
      "          [-0.0535, -0.0890, -0.0878],\n",
      "          [ 0.0134, -0.0086,  0.0236]],\n",
      "\n",
      "         [[ 0.0377,  0.0048, -0.0396],\n",
      "          [-0.0637, -0.0857,  0.0104],\n",
      "          [ 0.0073,  0.0197, -0.0330]],\n",
      "\n",
      "         [[ 0.0223,  0.0136,  0.0454],\n",
      "          [ 0.0180, -0.0484, -0.0237],\n",
      "          [-0.0037, -0.0292, -0.0259]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0051, -0.0174, -0.0662],\n",
      "          [ 0.0166,  0.0292, -0.0666],\n",
      "          [-0.0084,  0.0540, -0.0355]],\n",
      "\n",
      "         [[ 0.0274,  0.0002, -0.0073],\n",
      "          [ 0.0283, -0.0069, -0.0350],\n",
      "          [ 0.0173,  0.0156,  0.0103]],\n",
      "\n",
      "         [[-0.0405, -0.0203, -0.0246],\n",
      "          [-0.0355,  0.0346,  0.0063],\n",
      "          [-0.0503, -0.0487,  0.0027]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0654, -0.0076, -0.0627],\n",
      "          [-0.0299,  0.0004, -0.0366],\n",
      "          [-0.0428, -0.0115, -0.0540]],\n",
      "\n",
      "         [[ 0.0325, -0.0266,  0.0474],\n",
      "          [ 0.0234, -0.0129,  0.0019],\n",
      "          [ 0.0398, -0.0025,  0.0341]],\n",
      "\n",
      "         [[ 0.0221, -0.0172, -0.0159],\n",
      "          [ 0.0133,  0.0214, -0.0434],\n",
      "          [-0.0195, -0.0220, -0.0230]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0004, -0.0442,  0.0093],\n",
      "          [-0.0140, -0.0288, -0.0188],\n",
      "          [ 0.0029, -0.0333,  0.0024]],\n",
      "\n",
      "         [[ 0.0805, -0.0101, -0.0501],\n",
      "          [ 0.0349, -0.0456, -0.0459],\n",
      "          [-0.0160,  0.0161, -0.0583]],\n",
      "\n",
      "         [[ 0.0001,  0.0229, -0.0020],\n",
      "          [ 0.0187,  0.0329, -0.0062],\n",
      "          [ 0.0010,  0.0265,  0.0331]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0363,  0.0269, -0.0184],\n",
      "          [ 0.0297,  0.0264, -0.0480],\n",
      "          [ 0.0395,  0.0162, -0.0404]],\n",
      "\n",
      "         [[ 0.0102,  0.0502,  0.0391],\n",
      "          [-0.0082,  0.0022, -0.0294],\n",
      "          [ 0.0032, -0.0327, -0.0187]],\n",
      "\n",
      "         [[ 0.0162, -0.0258,  0.0164],\n",
      "          [-0.0190,  0.0116, -0.0138],\n",
      "          [-0.0042, -0.0355, -0.0101]]],\n",
      "\n",
      "\n",
      "        [[[-0.0198,  0.0140, -0.0542],\n",
      "          [ 0.0252, -0.0085,  0.0196],\n",
      "          [-0.0274, -0.0065,  0.0325]],\n",
      "\n",
      "         [[-0.0601,  0.0478, -0.0076],\n",
      "          [-0.0239,  0.0232, -0.0196],\n",
      "          [-0.0466, -0.0076,  0.0202]],\n",
      "\n",
      "         [[-0.0177,  0.0619, -0.0146],\n",
      "          [ 0.0384,  0.0264, -0.0326],\n",
      "          [ 0.0046, -0.0241, -0.0198]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0051,  0.0112, -0.0160],\n",
      "          [ 0.0255,  0.0157,  0.0259],\n",
      "          [ 0.0034,  0.0450, -0.0270]],\n",
      "\n",
      "         [[-0.0087,  0.0046, -0.0058],\n",
      "          [ 0.0066, -0.0052, -0.0295],\n",
      "          [ 0.0688, -0.0453, -0.0164]],\n",
      "\n",
      "         [[-0.0070,  0.0188, -0.0274],\n",
      "          [-0.0354,  0.0368,  0.0118],\n",
      "          [-0.0105,  0.0317,  0.0152]]],\n",
      "\n",
      "\n",
      "        [[[-0.0397, -0.0002,  0.0013],\n",
      "          [ 0.0033,  0.0353,  0.0319],\n",
      "          [-0.0688,  0.0275,  0.0124]],\n",
      "\n",
      "         [[ 0.0122, -0.0992,  0.0717],\n",
      "          [ 0.0353,  0.0258, -0.1213],\n",
      "          [-0.1101,  0.0398,  0.0955]],\n",
      "\n",
      "         [[-0.0364,  0.0248, -0.0044],\n",
      "          [-0.0120, -0.0239,  0.0148],\n",
      "          [ 0.0071, -0.0560, -0.0221]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0072, -0.0038, -0.0094],\n",
      "          [-0.0103,  0.0627,  0.0406],\n",
      "          [-0.0126, -0.0062, -0.0091]],\n",
      "\n",
      "         [[-0.0292, -0.0361, -0.0503],\n",
      "          [-0.0263,  0.0471,  0.0243],\n",
      "          [ 0.0245, -0.0943,  0.0346]],\n",
      "\n",
      "         [[ 0.0083,  0.0085,  0.0213],\n",
      "          [ 0.0247, -0.0562, -0.0025],\n",
      "          [-0.0228,  0.0734, -0.0773]]]], device='cuda:0'))\n",
      "('conv2.0.bias', Parameter containing:\n",
      "tensor([ 5.2037e-08,  1.6816e-07, -1.8231e-07,  1.3309e-07, -2.7950e-08,\n",
      "         1.7233e-07,  1.8756e-07, -2.9921e-07,  1.4627e-07, -2.4475e-07,\n",
      "        -2.7785e-07,  9.9346e-08, -4.7917e-10, -6.8207e-08,  2.5167e-07,\n",
      "        -5.9980e-08,  1.0956e-07, -1.3248e-07,  2.5679e-07, -4.4735e-08,\n",
      "        -8.4941e-08, -2.7306e-08, -2.5431e-07, -1.0812e-07,  6.0522e-08,\n",
      "        -2.2795e-08, -4.5871e-07, -2.8904e-07,  3.0298e-07, -4.6681e-07,\n",
      "         1.3443e-07, -7.6318e-08,  1.5719e-07, -3.9536e-07,  7.3721e-08,\n",
      "        -2.2970e-07,  2.1271e-07,  2.7816e-07, -2.9130e-07,  1.3469e-07,\n",
      "         5.8262e-08, -2.8987e-08,  4.2988e-07, -4.9896e-08, -2.6563e-08,\n",
      "         9.9850e-08, -2.3396e-07, -3.1836e-08, -1.5603e-07,  1.4999e-07,\n",
      "        -7.4851e-08,  1.9116e-07, -2.3357e-07, -1.1634e-07, -6.0892e-08,\n",
      "        -1.5797e-07,  2.4697e-07,  1.4678e-07,  1.5116e-07,  1.7938e-07,\n",
      "         1.4526e-08, -1.6486e-07, -9.5360e-08,  1.3946e-07, -1.3070e-07,\n",
      "         3.2099e-08,  2.5890e-07,  1.1790e-07, -3.1081e-08, -1.4896e-07,\n",
      "        -2.8961e-07,  9.4378e-08, -2.5388e-07, -1.4899e-07,  1.7642e-08,\n",
      "        -2.2234e-08,  5.0404e-09,  4.4365e-08, -2.5692e-07,  1.7018e-07,\n",
      "        -7.3354e-08,  1.0447e-08, -2.8903e-07,  2.5358e-07,  3.6251e-08,\n",
      "        -4.8722e-07,  3.0441e-08, -1.3389e-07, -7.4858e-08,  7.4285e-08,\n",
      "        -3.2580e-08, -3.1576e-07, -2.7958e-07,  4.6758e-07, -9.9340e-08,\n",
      "         3.5101e-07,  4.6664e-08,  3.6270e-07, -8.3728e-08,  7.9654e-08,\n",
      "        -1.1166e-07,  1.6900e-07,  6.5315e-08,  3.2916e-08, -2.3040e-08,\n",
      "        -6.6483e-08,  1.3506e-07,  5.1538e-07, -3.6439e-07,  6.7566e-08,\n",
      "         1.1134e-07, -1.6083e-07,  4.1988e-08, -2.7582e-07,  1.6557e-07,\n",
      "         1.3652e-07, -1.5458e-07,  3.1151e-07, -3.7305e-08, -1.0384e-07,\n",
      "         3.8107e-08,  2.5279e-09, -2.9077e-07, -1.4673e-08,  9.3421e-08,\n",
      "        -5.9109e-08, -1.0075e-07,  1.1419e-07], device='cuda:0'))\n",
      "('conv2.1.weight', Parameter containing:\n",
      "tensor([0.6827, 0.7838, 0.7104, 0.6933, 0.6863, 0.7404, 0.6401, 0.7319, 0.7419,\n",
      "        0.7255, 0.6924, 0.6526, 0.7039, 0.7525, 0.6952, 0.6747, 0.6925, 0.7507,\n",
      "        0.7890, 0.7215, 0.7679, 0.8337, 0.7996, 0.7349, 0.6414, 0.6983, 0.6903,\n",
      "        0.6421, 0.7027, 0.7693, 0.7132, 0.7041, 0.7520, 0.7445, 0.7392, 0.7459,\n",
      "        0.8298, 0.7255, 0.7575, 0.7829, 0.6444, 0.7242, 0.8349, 0.6785, 0.7140,\n",
      "        0.6550, 0.7684, 0.6785, 0.6659, 0.7148, 0.7755, 0.6785, 0.7520, 0.7491,\n",
      "        0.7003, 0.7375, 0.7282, 0.6660, 0.7739, 0.7633, 0.7602, 0.6987, 0.6646,\n",
      "        0.7541, 0.6859, 0.7159, 0.7519, 0.7043, 0.6944, 0.6302, 0.8175, 0.7192,\n",
      "        0.7138, 0.6808, 0.7195, 0.7233, 0.7063, 0.7358, 0.7321, 0.7361, 0.6899,\n",
      "        0.7615, 0.7393, 0.7395, 0.7080, 0.7395, 0.6863, 0.6989, 0.6976, 0.7199,\n",
      "        0.6483, 0.7141, 0.7950, 0.7594, 0.7284, 0.7544, 0.6957, 0.7549, 0.7702,\n",
      "        0.7491, 0.6872, 0.7942, 0.6766, 0.7656, 0.6987, 0.7056, 0.6980, 0.7912,\n",
      "        0.7552, 0.8211, 0.7067, 0.7266, 0.7960, 0.7365, 0.7114, 0.7344, 0.7280,\n",
      "        0.7958, 0.7755, 0.7068, 0.7454, 0.6362, 0.8528, 0.6934, 0.7221, 0.7354,\n",
      "        0.7470, 0.7439], device='cuda:0'))\n",
      "('conv2.1.bias', Parameter containing:\n",
      "tensor([-0.1532, -0.0607, -0.0776, -0.1025, -0.1354, -0.1571, -0.0994, -0.1311,\n",
      "        -0.1070, -0.0298, -0.1122, -0.1061, -0.1564, -0.1053, -0.1504, -0.1915,\n",
      "        -0.1437, -0.1176, -0.1187, -0.1273, -0.1418, -0.1010, -0.0174, -0.1970,\n",
      "        -0.0648, -0.1456, -0.1061, -0.0986, -0.0976, -0.1164, -0.2003, -0.1318,\n",
      "        -0.1045, -0.1096, -0.1864, -0.1317, -0.1297, -0.1286, -0.0645, -0.1063,\n",
      "        -0.1273, -0.1638, -0.0412, -0.1087, -0.1334, -0.1343, -0.0964, -0.1342,\n",
      "        -0.1532, -0.1272, -0.0934, -0.1586, -0.1301, -0.1026, -0.1650, -0.0693,\n",
      "        -0.1902, -0.1460, -0.1075, -0.1126, -0.1022, -0.1142, -0.0790, -0.1503,\n",
      "        -0.1663, -0.1637, -0.0856, -0.0850, -0.1116, -0.1688, -0.1483, -0.1310,\n",
      "        -0.1924, -0.1052, -0.1355, -0.0578, -0.0775, -0.1258, -0.1595, -0.1237,\n",
      "        -0.1435, -0.1195, -0.0911, -0.1001, -0.0848, -0.1591, -0.1532, -0.1412,\n",
      "        -0.0815, -0.1238, -0.1777, -0.0582, -0.1248, -0.0834, -0.0900, -0.0801,\n",
      "        -0.1084, -0.2076, -0.1307, -0.1213, -0.0669, -0.1107, -0.1334, -0.1453,\n",
      "        -0.1557, -0.2036, -0.1406, -0.0945, -0.1112, -0.1058, -0.0815, -0.0726,\n",
      "        -0.0732, -0.1186, -0.1312, -0.0905, -0.1873, -0.0721, -0.1344, -0.1396,\n",
      "        -0.1027, -0.1102, -0.0713, -0.1218, -0.1542, -0.0953, -0.1268, -0.0883],\n",
      "       device='cuda:0'))\n",
      "('res1.0.0.weight', Parameter containing:\n",
      "tensor([[[[-0.0041,  0.0171,  0.0215],\n",
      "          [-0.0337,  0.0132,  0.0229],\n",
      "          [-0.0130, -0.0047,  0.0161]],\n",
      "\n",
      "         [[-0.0042,  0.0078,  0.0023],\n",
      "          [-0.0314,  0.0024,  0.0059],\n",
      "          [ 0.0158,  0.0274, -0.0022]],\n",
      "\n",
      "         [[ 0.0144,  0.0164,  0.0188],\n",
      "          [-0.0111, -0.0028,  0.0306],\n",
      "          [-0.0059,  0.0347,  0.0325]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0299, -0.0300, -0.0307],\n",
      "          [-0.0232, -0.0183,  0.0055],\n",
      "          [-0.0327, -0.0153, -0.0100]],\n",
      "\n",
      "         [[-0.0324, -0.0398,  0.0228],\n",
      "          [-0.0154,  0.0139,  0.0187],\n",
      "          [ 0.0208, -0.0132, -0.0264]],\n",
      "\n",
      "         [[ 0.0230,  0.0312,  0.0247],\n",
      "          [ 0.0089, -0.0007,  0.0346],\n",
      "          [-0.0123, -0.0063,  0.0096]]],\n",
      "\n",
      "\n",
      "        [[[-0.0305, -0.0221, -0.0184],\n",
      "          [-0.0142,  0.0168, -0.0039],\n",
      "          [ 0.0064, -0.0025, -0.0123]],\n",
      "\n",
      "         [[ 0.0236,  0.0307,  0.0083],\n",
      "          [ 0.0043,  0.0135, -0.0157],\n",
      "          [-0.0248, -0.0077, -0.0427]],\n",
      "\n",
      "         [[-0.0032,  0.0251, -0.0002],\n",
      "          [-0.0317,  0.0028,  0.0009],\n",
      "          [-0.0221,  0.0152, -0.0279]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0196, -0.0316, -0.0187],\n",
      "          [ 0.0304,  0.0225,  0.0173],\n",
      "          [-0.0007,  0.0159,  0.0383]],\n",
      "\n",
      "         [[-0.0091, -0.0678, -0.0662],\n",
      "          [ 0.0379, -0.0368, -0.0757],\n",
      "          [ 0.0371,  0.0295, -0.0346]],\n",
      "\n",
      "         [[-0.0658, -0.0206,  0.0149],\n",
      "          [-0.0263,  0.0273,  0.0159],\n",
      "          [-0.0066,  0.0184, -0.0105]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0552, -0.0003, -0.0238],\n",
      "          [ 0.0191, -0.0119, -0.0110],\n",
      "          [-0.0340, -0.0061,  0.0203]],\n",
      "\n",
      "         [[ 0.0276,  0.0172,  0.0073],\n",
      "          [ 0.0219, -0.0196,  0.0319],\n",
      "          [ 0.0269, -0.0286, -0.0035]],\n",
      "\n",
      "         [[-0.0092, -0.0337,  0.0099],\n",
      "          [-0.0112, -0.0145,  0.0063],\n",
      "          [-0.0018,  0.0230,  0.0400]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0164,  0.0068, -0.0023],\n",
      "          [ 0.0194, -0.0056, -0.0334],\n",
      "          [-0.0290, -0.0223, -0.0640]],\n",
      "\n",
      "         [[-0.0200,  0.0549,  0.0075],\n",
      "          [-0.0031,  0.0090, -0.0089],\n",
      "          [-0.0096, -0.0105, -0.0002]],\n",
      "\n",
      "         [[ 0.0358,  0.0093,  0.0322],\n",
      "          [-0.0109, -0.0303, -0.0012],\n",
      "          [-0.0120, -0.0089,  0.0361]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0159,  0.0548,  0.0773],\n",
      "          [-0.0134,  0.0362,  0.0476],\n",
      "          [-0.0237, -0.0015,  0.0127]],\n",
      "\n",
      "         [[ 0.0249,  0.0011,  0.0305],\n",
      "          [-0.0203, -0.0439, -0.0551],\n",
      "          [ 0.0188,  0.0202,  0.0254]],\n",
      "\n",
      "         [[ 0.0105,  0.0190,  0.0112],\n",
      "          [ 0.0437,  0.0323,  0.0087],\n",
      "          [-0.0485, -0.0083, -0.0190]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0080,  0.0022, -0.0274],\n",
      "          [ 0.0221,  0.0535, -0.0252],\n",
      "          [ 0.0217,  0.0342, -0.0324]],\n",
      "\n",
      "         [[-0.0143, -0.0201,  0.0223],\n",
      "          [-0.0163, -0.0117,  0.0159],\n",
      "          [-0.0144, -0.0025,  0.0012]],\n",
      "\n",
      "         [[-0.0200, -0.0321,  0.0160],\n",
      "          [ 0.0022, -0.0282,  0.0230],\n",
      "          [ 0.0116,  0.0119,  0.0179]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0061,  0.0059, -0.0180],\n",
      "          [-0.0078, -0.0100, -0.0323],\n",
      "          [-0.0119, -0.0096,  0.0037]],\n",
      "\n",
      "         [[-0.0277,  0.0051,  0.0130],\n",
      "          [-0.0466, -0.0317, -0.0302],\n",
      "          [ 0.0258,  0.0143,  0.0435]],\n",
      "\n",
      "         [[ 0.0023,  0.0144,  0.0045],\n",
      "          [-0.0033,  0.0015,  0.0136],\n",
      "          [ 0.0253,  0.0432,  0.0295]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0248,  0.0034,  0.0117],\n",
      "          [-0.0204,  0.0106, -0.0055],\n",
      "          [ 0.0165, -0.0057,  0.0021]],\n",
      "\n",
      "         [[-0.0013,  0.0211,  0.0226],\n",
      "          [ 0.0076, -0.0286, -0.0127],\n",
      "          [-0.0041, -0.0359, -0.0330]],\n",
      "\n",
      "         [[-0.0236, -0.0418, -0.0404],\n",
      "          [ 0.0041, -0.0405, -0.0521],\n",
      "          [ 0.0364,  0.0277,  0.0304]]],\n",
      "\n",
      "\n",
      "        [[[-0.0174,  0.0073,  0.0108],\n",
      "          [-0.0035,  0.0189, -0.0013],\n",
      "          [ 0.0058,  0.0227, -0.0073]],\n",
      "\n",
      "         [[-0.0024, -0.0247, -0.0098],\n",
      "          [-0.0244, -0.0114,  0.0120],\n",
      "          [-0.0305,  0.0252,  0.0023]],\n",
      "\n",
      "         [[ 0.0155,  0.0143,  0.0082],\n",
      "          [-0.0028, -0.0113,  0.0046],\n",
      "          [ 0.0006,  0.0021,  0.0063]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0236,  0.0216,  0.0196],\n",
      "          [-0.0094,  0.0236,  0.0271],\n",
      "          [ 0.0049, -0.0101, -0.0231]],\n",
      "\n",
      "         [[-0.0456, -0.0239, -0.0149],\n",
      "          [-0.0166,  0.0053, -0.0281],\n",
      "          [-0.0192, -0.0235, -0.0326]],\n",
      "\n",
      "         [[ 0.0416,  0.0375,  0.0026],\n",
      "          [ 0.0131,  0.0425,  0.0339],\n",
      "          [-0.0077, -0.0158, -0.0054]]]], device='cuda:0'))\n",
      "('res1.0.0.bias', Parameter containing:\n",
      "tensor([-6.3566e-08, -5.1314e-08, -1.2731e-07, -1.0864e-07, -3.8809e-08,\n",
      "         9.2884e-09, -6.6265e-08,  2.8465e-08, -1.1434e-07,  7.5226e-08,\n",
      "         8.2864e-08, -1.6772e-08,  1.0267e-07,  3.5036e-08,  2.2167e-08,\n",
      "         1.0780e-07,  4.2386e-08, -4.4093e-08, -9.7491e-08,  4.4094e-08,\n",
      "         2.5078e-07, -1.4758e-07,  1.5152e-07,  9.9644e-08,  1.3003e-08,\n",
      "        -1.1408e-07, -6.1520e-08,  1.6630e-07,  4.6553e-08,  1.1938e-08,\n",
      "         2.0616e-08,  4.1117e-08, -6.3797e-08, -8.6200e-08,  1.1412e-08,\n",
      "        -1.1263e-07,  4.1841e-08,  1.9218e-08, -5.8366e-08,  7.2026e-08,\n",
      "        -2.9664e-08, -7.7720e-08,  5.8460e-09, -1.3125e-07,  6.0598e-08,\n",
      "         1.6195e-08,  7.9970e-09,  1.1869e-08,  3.3454e-08, -2.0061e-07,\n",
      "         3.8898e-08,  1.0302e-07, -3.4572e-08,  1.6825e-08,  9.6337e-08,\n",
      "         3.8824e-08,  4.5221e-08,  7.0709e-08,  2.4993e-08,  2.3592e-08,\n",
      "        -6.7787e-08,  6.8836e-08,  7.5123e-09, -1.3109e-08, -7.2401e-08,\n",
      "        -7.7239e-10,  7.2290e-08, -7.0639e-09,  5.8202e-09,  3.8684e-08,\n",
      "         1.2231e-07, -4.9238e-10,  1.1652e-07, -1.8696e-09, -1.2084e-07,\n",
      "         5.4332e-09, -1.6506e-09,  9.2914e-09,  3.5037e-08,  2.8428e-08,\n",
      "        -7.8936e-09, -6.1875e-08,  7.1404e-10,  3.8134e-08,  4.4614e-08,\n",
      "         5.7175e-08, -1.1755e-07, -3.1455e-08, -2.8587e-08, -1.9348e-08,\n",
      "         2.0517e-07,  4.9882e-08,  3.9247e-08,  1.4920e-07, -1.9183e-08,\n",
      "        -2.0272e-08,  9.9125e-08,  1.9979e-08, -9.9305e-08, -2.9196e-08,\n",
      "         5.2230e-09,  8.2130e-08,  3.9078e-08, -1.3691e-07, -6.2913e-08,\n",
      "         4.1692e-08,  8.5534e-08, -2.5859e-08,  6.4090e-09,  1.0464e-07,\n",
      "         3.5428e-10, -1.0832e-08, -3.9630e-08,  7.9844e-08,  2.8272e-08,\n",
      "        -2.1483e-08, -4.9941e-09, -1.4315e-07, -1.6017e-07,  8.4428e-08,\n",
      "         1.0641e-07,  5.9676e-08, -7.1620e-08,  5.1759e-08,  9.4206e-08,\n",
      "         9.3287e-09, -1.5232e-07, -8.3299e-08], device='cuda:0'))\n",
      "('res1.0.1.weight', Parameter containing:\n",
      "tensor([0.6239, 0.7413, 0.6715, 0.6356, 0.6275, 0.6896, 0.6695, 0.6840, 0.7538,\n",
      "        0.6347, 0.6193, 0.7150, 0.7158, 0.5400, 0.6495, 0.6195, 0.7190, 0.7150,\n",
      "        0.7034, 0.6629, 0.6912, 0.6642, 0.7527, 0.6139, 0.6477, 0.6731, 0.6881,\n",
      "        0.6142, 0.7031, 0.6683, 0.6850, 0.6206, 0.6822, 0.6402, 0.6712, 0.6827,\n",
      "        0.6803, 0.6469, 0.6163, 0.6112, 0.6220, 0.6800, 0.6071, 0.6782, 0.5727,\n",
      "        0.7153, 0.6707, 0.7091, 0.6691, 0.6982, 0.7166, 0.6879, 0.6328, 0.5889,\n",
      "        0.6615, 0.6667, 0.7065, 0.6446, 0.6319, 0.6736, 0.7410, 0.6627, 0.6714,\n",
      "        0.6207, 0.6367, 0.5348, 0.6553, 0.6344, 0.6925, 0.7242, 0.7311, 0.5770,\n",
      "        0.6456, 0.6483, 0.6127, 0.7157, 0.6240, 0.5972, 0.6727, 0.6429, 0.6604,\n",
      "        0.6123, 0.6094, 0.6765, 0.6836, 0.6257, 0.7001, 0.6228, 0.6420, 0.6643,\n",
      "        0.6572, 0.6466, 0.6360, 0.6411, 0.7639, 0.6838, 0.7662, 0.7334, 0.6080,\n",
      "        0.6826, 0.6356, 0.7032, 0.6892, 0.7256, 0.6634, 0.7117, 0.5922, 0.6551,\n",
      "        0.6098, 0.6517, 0.7038, 0.6494, 0.6929, 0.7615, 0.7020, 0.6140, 0.6849,\n",
      "        0.6145, 0.6584, 0.6728, 0.7439, 0.5901, 0.6471, 0.7048, 0.6906, 0.6551,\n",
      "        0.7277, 0.6902], device='cuda:0'))\n",
      "('res1.0.1.bias', Parameter containing:\n",
      "tensor([-0.1682, -0.1178, -0.1576, -0.1412, -0.1477, -0.1036, -0.0968, -0.1066,\n",
      "        -0.0842, -0.1440, -0.1366, -0.1554, -0.0922, -0.0575, -0.1252, -0.1214,\n",
      "        -0.0375, -0.1278, -0.1260, -0.1320, -0.1194, -0.1549, -0.1347, -0.1476,\n",
      "        -0.1625, -0.1490, -0.1390, -0.1291, -0.1846, -0.0816, -0.1137, -0.1326,\n",
      "        -0.0988, -0.1197, -0.0715, -0.0939, -0.1058, -0.1057, -0.1213, -0.1320,\n",
      "        -0.1416, -0.1657, -0.1642, -0.1883, -0.1312, -0.1322, -0.1411, -0.1110,\n",
      "        -0.1185, -0.0667, -0.1206, -0.0955, -0.1154, -0.1235, -0.0995, -0.1509,\n",
      "        -0.1252, -0.0980, -0.1264, -0.0979, -0.0987, -0.0947, -0.1412, -0.0729,\n",
      "        -0.1243, -0.1045, -0.1239, -0.1454, -0.0935, -0.1347, -0.0980, -0.1151,\n",
      "        -0.1456, -0.1020, -0.1499, -0.0993, -0.1046, -0.1232, -0.0921, -0.0477,\n",
      "        -0.1085, -0.0876, -0.0740, -0.1236, -0.0734, -0.1247, -0.1282, -0.1256,\n",
      "        -0.0897, -0.1274, -0.1230, -0.1559, -0.1531, -0.1554, -0.1361, -0.0886,\n",
      "        -0.1102, -0.0505, -0.0719, -0.1323, -0.1525, -0.0944, -0.0679, -0.0174,\n",
      "        -0.1519, -0.0542, -0.1289, -0.0582, -0.1596, -0.0899, -0.1168, -0.1517,\n",
      "        -0.1459, -0.1106, -0.1403, -0.0990, -0.1133, -0.0713, -0.1253, -0.1010,\n",
      "        -0.1284, -0.1857, -0.1400, -0.0881, -0.1306, -0.1519, -0.1475, -0.1276],\n",
      "       device='cuda:0'))\n",
      "('res1.1.0.weight', Parameter containing:\n",
      "tensor([[[[ 2.4920e-02,  2.1961e-02, -1.1064e-02],\n",
      "          [-1.0701e-02, -1.4222e-02,  1.4049e-03],\n",
      "          [-1.6782e-02, -1.2809e-02,  7.8727e-03]],\n",
      "\n",
      "         [[ 6.2381e-03,  6.6641e-03, -1.4203e-02],\n",
      "          [ 2.2543e-02,  2.9483e-02,  2.4261e-02],\n",
      "          [ 6.3521e-03,  2.5758e-02,  7.3858e-03]],\n",
      "\n",
      "         [[-9.2824e-03, -9.8893e-05, -7.2310e-03],\n",
      "          [ 4.5558e-03, -1.9136e-02, -1.1638e-02],\n",
      "          [ 9.6667e-03,  6.3329e-03, -1.5491e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8997e-02, -6.8879e-03, -1.7111e-02],\n",
      "          [ 1.8871e-02, -4.1367e-03, -6.2243e-03],\n",
      "          [-1.5568e-02, -2.0977e-02, -4.1969e-02]],\n",
      "\n",
      "         [[-1.2627e-02, -1.8207e-02, -9.9590e-03],\n",
      "          [-2.2051e-02, -5.2798e-02, -2.8783e-02],\n",
      "          [-1.2865e-02, -3.9042e-02, -3.8850e-02]],\n",
      "\n",
      "         [[-4.6922e-02, -2.1239e-02,  9.7270e-03],\n",
      "          [-2.7597e-02, -1.0954e-02, -1.6505e-02],\n",
      "          [-9.4724e-03, -4.0805e-02, -1.7238e-02]]],\n",
      "\n",
      "\n",
      "        [[[-7.3577e-04, -2.7013e-03, -7.6793e-03],\n",
      "          [ 3.3140e-03,  1.7446e-02,  2.3125e-02],\n",
      "          [-1.4102e-02,  1.5637e-02, -4.7049e-03]],\n",
      "\n",
      "         [[-7.9613e-02, -2.7640e-02, -2.4312e-02],\n",
      "          [-2.0523e-02,  3.0269e-02,  3.0821e-02],\n",
      "          [ 3.6876e-02,  6.8383e-02,  5.7927e-02]],\n",
      "\n",
      "         [[-1.7733e-02, -9.7249e-03, -4.2999e-02],\n",
      "          [-1.6474e-02, -7.5778e-03, -2.0136e-02],\n",
      "          [-2.0791e-02,  1.0294e-02,  2.5382e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.9060e-02, -2.9044e-02, -1.4231e-03],\n",
      "          [-1.3416e-02, -3.9289e-02, -1.7263e-02],\n",
      "          [-2.0618e-02, -3.2666e-02,  6.2554e-03]],\n",
      "\n",
      "         [[ 5.7010e-03, -3.9643e-03, -2.7005e-02],\n",
      "          [ 2.3488e-02, -5.8418e-04, -4.4848e-02],\n",
      "          [-1.4752e-03, -3.3351e-02, -6.2443e-02]],\n",
      "\n",
      "         [[ 3.1065e-02,  4.9295e-04, -5.1931e-03],\n",
      "          [ 3.6579e-03, -1.6797e-04, -9.9501e-03],\n",
      "          [ 2.4660e-02,  6.8823e-03, -2.8080e-03]]],\n",
      "\n",
      "\n",
      "        [[[-8.6649e-03, -8.1982e-03, -1.9847e-02],\n",
      "          [-1.1471e-02,  4.1116e-03, -4.8781e-02],\n",
      "          [-2.6006e-02, -1.1970e-02,  5.4160e-03]],\n",
      "\n",
      "         [[ 3.4399e-02,  4.3681e-02, -2.9426e-03],\n",
      "          [-5.7114e-03,  4.2776e-02, -8.1916e-03],\n",
      "          [-4.0553e-02,  1.2151e-02,  1.6991e-02]],\n",
      "\n",
      "         [[-1.0491e-02,  3.0045e-02,  1.7654e-03],\n",
      "          [-2.6082e-02, -3.6157e-03, -5.5728e-03],\n",
      "          [-5.4797e-03, -2.5096e-02, -1.4590e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.5706e-03, -3.4902e-02, -3.7552e-02],\n",
      "          [ 3.5527e-02, -1.4011e-02,  1.4291e-02],\n",
      "          [ 3.5178e-03, -2.4667e-02,  3.9830e-03]],\n",
      "\n",
      "         [[ 8.0224e-04, -6.5799e-03, -4.2505e-02],\n",
      "          [ 6.9935e-03,  1.3341e-02, -1.2624e-02],\n",
      "          [ 2.4173e-02,  3.8679e-02,  2.3435e-02]],\n",
      "\n",
      "         [[ 2.0701e-02, -1.4830e-02, -2.3781e-02],\n",
      "          [ 2.4045e-03, -8.4693e-03, -7.1982e-03],\n",
      "          [ 1.6273e-02,  3.6844e-03, -2.8060e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.2710e-02,  5.0528e-02,  3.5721e-02],\n",
      "          [ 3.6157e-02,  4.5778e-02,  3.3752e-02],\n",
      "          [ 4.4087e-03,  8.8802e-03, -9.2025e-03]],\n",
      "\n",
      "         [[-1.1034e-02, -1.0187e-02, -2.6257e-02],\n",
      "          [-9.7102e-03,  4.4871e-02,  5.1670e-02],\n",
      "          [-3.7199e-03, -1.4095e-03, -1.7444e-03]],\n",
      "\n",
      "         [[-4.6634e-02, -3.3511e-02, -1.6374e-03],\n",
      "          [ 5.4333e-03, -1.0236e-02, -2.9369e-02],\n",
      "          [ 1.0755e-02,  5.2127e-03, -1.5359e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.1804e-02,  1.1167e-02, -1.1691e-02],\n",
      "          [ 1.8583e-03,  1.9742e-02, -2.4242e-02],\n",
      "          [ 1.6853e-02,  2.0912e-02,  1.1625e-02]],\n",
      "\n",
      "         [[-2.0443e-02, -1.8090e-02,  8.8067e-03],\n",
      "          [-1.1719e-03,  1.0497e-02,  1.0398e-02],\n",
      "          [-4.4548e-02, -1.7486e-02, -2.2834e-02]],\n",
      "\n",
      "         [[-3.0271e-02, -4.5742e-02, -4.0773e-02],\n",
      "          [-8.6602e-03, -1.3185e-02, -2.1043e-02],\n",
      "          [ 2.1236e-02,  5.4757e-03, -2.6735e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.5301e-02, -3.6519e-02, -1.9140e-02],\n",
      "          [-1.3955e-02, -1.1229e-02, -1.1448e-02],\n",
      "          [ 3.5274e-03,  3.7561e-02,  1.6954e-02]],\n",
      "\n",
      "         [[ 1.6644e-02,  2.3085e-02,  2.3072e-02],\n",
      "          [ 2.6352e-02,  2.3413e-02, -7.1132e-03],\n",
      "          [-2.3040e-02, -3.7995e-02, -1.6476e-02]],\n",
      "\n",
      "         [[-1.6043e-02,  1.1291e-02,  2.6896e-02],\n",
      "          [ 8.6081e-03,  1.4397e-02,  4.9076e-02],\n",
      "          [ 1.4648e-02,  5.3423e-02,  4.1692e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.9120e-03,  1.3141e-02,  3.6328e-03],\n",
      "          [-2.2365e-02, -3.0414e-03, -1.5375e-02],\n",
      "          [ 2.6491e-02, -1.1907e-02,  4.7007e-02]],\n",
      "\n",
      "         [[ 5.2855e-03, -4.3133e-02, -4.5989e-02],\n",
      "          [-2.7140e-02, -2.4928e-02, -3.9192e-02],\n",
      "          [ 2.5343e-02, -3.5353e-03,  2.1767e-02]],\n",
      "\n",
      "         [[ 7.3384e-03,  4.5556e-03, -7.5871e-03],\n",
      "          [ 3.7196e-02, -7.7073e-03, -2.4293e-02],\n",
      "          [ 3.0710e-03, -3.2039e-02, -4.1134e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 5.5744e-05, -3.9077e-03, -4.0345e-03],\n",
      "          [-1.0987e-03,  1.4215e-02, -1.7217e-02],\n",
      "          [ 3.1409e-02,  1.2226e-03, -2.3159e-04]],\n",
      "\n",
      "         [[-2.8618e-02,  1.1942e-03,  2.6662e-02],\n",
      "          [ 8.3523e-03,  3.9088e-03, -1.9189e-02],\n",
      "          [ 9.3808e-03, -3.4404e-02, -6.8313e-02]],\n",
      "\n",
      "         [[-2.9703e-03, -4.1858e-03,  3.4622e-03],\n",
      "          [ 2.1216e-02, -8.9299e-03,  2.9091e-02],\n",
      "          [-2.3519e-03, -1.1395e-04,  3.3517e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.2366e-02,  3.1155e-02,  3.7496e-02],\n",
      "          [-2.1123e-02, -1.6689e-02,  1.1152e-02],\n",
      "          [ 6.6371e-03, -2.2407e-02,  6.8803e-03]],\n",
      "\n",
      "         [[ 1.6367e-02,  9.4255e-03, -1.0217e-02],\n",
      "          [-3.4556e-02, -3.4318e-02,  2.7364e-03],\n",
      "          [ 3.1517e-03, -3.2815e-02,  2.7057e-02]],\n",
      "\n",
      "         [[-1.5239e-02, -1.8311e-02,  3.8161e-02],\n",
      "          [-3.3686e-02,  2.2286e-02,  3.3210e-02],\n",
      "          [ 2.4032e-02,  2.1354e-02,  4.3959e-03]]]], device='cuda:0'))\n",
      "('res1.1.0.bias', Parameter containing:\n",
      "tensor([ 4.1142e-08,  2.2848e-07, -2.9890e-08, -1.1391e-07, -7.9160e-08,\n",
      "         3.0355e-07, -1.6956e-07, -9.1738e-10, -1.5448e-07,  9.5317e-08,\n",
      "         1.5676e-08,  4.9595e-08, -1.3605e-07,  1.1138e-08,  3.9735e-08,\n",
      "        -7.0054e-08, -8.0935e-08, -1.4459e-08,  1.4863e-08, -1.3071e-07,\n",
      "        -3.5906e-08,  1.8037e-07, -2.6570e-07, -2.5923e-08, -1.9500e-07,\n",
      "         1.3914e-07, -1.0476e-07, -4.2934e-08,  8.5779e-09,  3.7298e-08,\n",
      "        -7.6791e-08, -3.5559e-07,  2.9743e-08, -5.2470e-08,  2.7259e-07,\n",
      "         6.2599e-08,  1.1925e-07,  1.1304e-07,  1.2962e-07, -1.6808e-08,\n",
      "        -2.7278e-08, -1.2358e-08, -9.6504e-08, -3.4240e-08,  7.8690e-08,\n",
      "         1.0936e-07,  5.0253e-07,  5.6025e-08,  1.9990e-10, -3.3506e-08,\n",
      "        -1.2823e-07, -9.9903e-08, -3.9878e-08, -9.4310e-08,  2.0141e-08,\n",
      "         5.1699e-08, -1.2355e-07, -1.1223e-07, -5.4948e-08,  6.0438e-08,\n",
      "        -1.0630e-07,  1.3686e-07, -9.5602e-08,  6.1015e-08, -4.7284e-08,\n",
      "         5.6651e-08, -2.2730e-11,  1.0435e-07,  1.8410e-07, -3.1273e-08,\n",
      "         4.0216e-08,  6.1280e-08, -2.2589e-07,  2.9371e-08, -7.1973e-09,\n",
      "        -1.0972e-07,  2.0428e-07, -2.1449e-07,  2.3179e-07, -1.8022e-07,\n",
      "         3.1557e-09,  1.0707e-07,  6.3979e-08, -1.1717e-07,  3.7997e-08,\n",
      "         8.8794e-08, -2.6427e-07, -2.3438e-07, -3.4097e-07,  3.0309e-07,\n",
      "         1.7568e-07, -1.1827e-07,  9.1982e-08, -9.2951e-08,  6.0457e-08,\n",
      "         4.3293e-08,  2.6302e-07,  1.1097e-08,  2.7488e-08, -3.1366e-07,\n",
      "        -2.8618e-07, -1.0999e-07, -8.6399e-08,  5.9437e-08, -1.6261e-07,\n",
      "        -7.2129e-09, -4.6217e-08, -9.2488e-08, -2.3356e-07,  5.2085e-08,\n",
      "         3.3253e-08,  8.8743e-08, -2.9567e-08, -9.9739e-08, -1.0758e-07,\n",
      "        -6.9775e-08,  1.4933e-09,  1.8178e-07, -1.6364e-08,  7.2982e-09,\n",
      "         1.7657e-07,  7.1755e-08, -9.7718e-09,  5.8628e-08, -2.9700e-07,\n",
      "        -1.6523e-08,  8.3619e-08, -1.7589e-07], device='cuda:0'))\n",
      "('res1.1.1.weight', Parameter containing:\n",
      "tensor([0.6689, 0.7746, 0.6851, 0.7138, 0.7351, 0.7391, 0.7717, 0.7595, 0.6969,\n",
      "        0.7664, 0.7636, 0.6815, 0.6928, 0.6672, 0.6834, 0.6801, 0.7222, 0.6929,\n",
      "        0.6548, 0.7248, 0.7539, 0.7283, 0.7638, 0.7918, 0.7418, 0.7827, 0.6688,\n",
      "        0.6932, 0.7749, 0.8254, 0.7861, 0.7653, 0.7016, 0.6766, 0.7652, 0.7343,\n",
      "        0.7661, 0.6864, 0.7645, 0.7174, 0.7424, 0.7231, 0.6665, 0.8247, 0.7636,\n",
      "        0.7095, 0.7449, 0.7306, 0.6980, 0.6960, 0.7709, 0.7818, 0.7401, 0.7179,\n",
      "        0.7314, 0.6631, 0.7202, 0.6505, 0.7665, 0.7408, 0.7091, 0.7671, 0.6343,\n",
      "        0.7347, 0.7567, 0.6918, 0.7095, 0.8058, 0.7459, 0.6573, 0.6854, 0.7704,\n",
      "        0.7129, 0.7715, 0.7092, 0.7643, 0.7891, 0.6559, 0.6893, 0.6880, 0.7289,\n",
      "        0.7322, 0.7192, 0.7314, 0.7130, 0.7555, 0.6905, 0.7254, 0.7138, 0.6994,\n",
      "        0.8126, 0.7117, 0.6826, 0.7575, 0.7251, 0.7128, 0.7817, 0.6668, 0.7044,\n",
      "        0.7666, 0.7740, 0.7671, 0.7185, 0.7458, 0.7455, 0.5923, 0.7213, 0.6297,\n",
      "        0.7139, 0.6383, 0.7351, 0.7038, 0.7884, 0.6990, 0.7523, 0.7111, 0.7741,\n",
      "        0.7193, 0.6986, 0.6698, 0.6545, 0.7659, 0.7291, 0.7415, 0.6539, 0.8005,\n",
      "        0.6981, 0.6688], device='cuda:0'))\n",
      "('res1.1.1.bias', Parameter containing:\n",
      "tensor([-0.0541, -0.0749, -0.1161, -0.0627, -0.1085, -0.0463, -0.1266, -0.1132,\n",
      "        -0.0520, -0.0288, -0.0756, -0.0660, -0.0329, -0.0800, -0.0266, -0.0138,\n",
      "        -0.0725, -0.0794, -0.1053, -0.0451, -0.1396, -0.0905, -0.1021, -0.0054,\n",
      "        -0.0851,  0.0012, -0.1176, -0.0027, -0.0378, -0.0643, -0.0554, -0.0345,\n",
      "        -0.0572, -0.0434, -0.0558, -0.1164, -0.0366, -0.0916, -0.0795, -0.0838,\n",
      "        -0.0904, -0.0422, -0.1001, -0.0804, -0.0747, -0.0435, -0.1392, -0.0974,\n",
      "        -0.1238, -0.1217, -0.0737, -0.0899, -0.1002, -0.0004, -0.0489, -0.0769,\n",
      "        -0.0393, -0.0171,  0.0251, -0.0813, -0.1106,  0.0077, -0.1196, -0.0173,\n",
      "        -0.0262,  0.0051,  0.0036, -0.0560, -0.0042, -0.0802, -0.0539, -0.0238,\n",
      "        -0.0577,  0.0080, -0.0050, -0.1199, -0.0931, -0.0151, -0.0842, -0.0624,\n",
      "        -0.1298, -0.0197, -0.0084, -0.0532, -0.1079, -0.0336, -0.0284, -0.0384,\n",
      "         0.0076, -0.1273, -0.0123, -0.0609, -0.1295, -0.0537, -0.0479, -0.0741,\n",
      "        -0.1007, -0.0252, -0.0185, -0.0002, -0.0163, -0.0419, -0.1256, -0.1155,\n",
      "        -0.0902, -0.1134, -0.0146, -0.0883, -0.0281, -0.1346, -0.1009, -0.0470,\n",
      "        -0.0256, -0.1111, -0.0588, -0.0435, -0.1570, -0.0524,  0.0523, -0.1336,\n",
      "        -0.0915, -0.1145, -0.0737, -0.1163, -0.0610, -0.0170, -0.0422, -0.0670],\n",
      "       device='cuda:0'))\n",
      "('conv3.0.weight', Parameter containing:\n",
      "tensor([[[[-7.0863e-03, -4.1419e-02, -3.0457e-02],\n",
      "          [-4.1195e-03, -3.8115e-02, -4.0685e-02],\n",
      "          [-1.8985e-02, -4.0381e-02, -3.8198e-02]],\n",
      "\n",
      "         [[-1.9365e-02, -3.2983e-02, -1.1838e-02],\n",
      "          [ 3.8678e-03, -3.6424e-02, -1.3664e-02],\n",
      "          [-3.2381e-02, -1.7270e-02,  1.8047e-02]],\n",
      "\n",
      "         [[ 8.4806e-05, -2.0841e-02, -2.0176e-02],\n",
      "          [ 2.5700e-02, -5.7122e-03, -3.6950e-02],\n",
      "          [ 2.9996e-02,  4.8560e-03, -1.4028e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.0989e-02, -2.5976e-02,  8.8586e-03],\n",
      "          [-8.9394e-03, -4.4726e-02,  1.4655e-03],\n",
      "          [-1.2095e-02, -4.0622e-02, -1.1607e-02]],\n",
      "\n",
      "         [[ 1.1154e-02, -2.0448e-02, -6.6055e-03],\n",
      "          [-3.5603e-02, -2.4602e-02, -6.2891e-02],\n",
      "          [-3.0399e-02, -1.9601e-02, -6.3066e-02]],\n",
      "\n",
      "         [[ 1.8720e-02, -6.8328e-03,  2.3328e-04],\n",
      "          [-2.8187e-03, -7.5271e-03, -2.5337e-02],\n",
      "          [-1.3920e-02, -1.9673e-02, -1.7131e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 8.9036e-03, -1.1135e-02, -1.8431e-02],\n",
      "          [ 1.0294e-02, -2.5731e-02,  8.2845e-03],\n",
      "          [-9.6032e-03, -4.0488e-02,  1.2320e-02]],\n",
      "\n",
      "         [[ 2.7196e-02, -4.3937e-02, -5.2249e-03],\n",
      "          [ 1.5378e-02, -2.8217e-02,  6.3546e-03],\n",
      "          [ 4.0277e-03, -9.7733e-03,  1.5562e-02]],\n",
      "\n",
      "         [[-2.8300e-03,  1.3044e-02,  1.3099e-02],\n",
      "          [-3.2235e-03,  4.7489e-03,  8.4289e-03],\n",
      "          [ 1.8406e-02,  1.5453e-02,  1.7686e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.3134e-02, -3.2082e-03, -2.5692e-02],\n",
      "          [-3.2911e-03, -2.6504e-02, -5.1921e-02],\n",
      "          [-1.2958e-02, -2.9467e-02, -2.0282e-02]],\n",
      "\n",
      "         [[ 1.3667e-02,  6.6232e-03,  1.4656e-02],\n",
      "          [ 1.2613e-02, -2.3384e-02, -3.0747e-02],\n",
      "          [-3.7623e-03, -1.1106e-02, -4.0350e-02]],\n",
      "\n",
      "         [[ 1.2784e-02, -7.4684e-03, -1.9200e-02],\n",
      "          [-6.9317e-03, -2.7851e-02,  1.9214e-02],\n",
      "          [ 9.2187e-03,  1.1366e-02,  3.2755e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.6251e-02,  2.3088e-03,  1.4568e-02],\n",
      "          [-5.6583e-04,  4.5610e-02,  2.3408e-02],\n",
      "          [-7.6787e-03, -2.3337e-02,  2.5989e-02]],\n",
      "\n",
      "         [[-3.1003e-02, -4.9050e-03,  1.2655e-02],\n",
      "          [-4.3463e-02, -3.2541e-03,  5.4833e-02],\n",
      "          [-1.6207e-02,  6.2724e-03,  2.7726e-02]],\n",
      "\n",
      "         [[ 2.0703e-02, -1.0630e-02,  2.2571e-03],\n",
      "          [ 1.5688e-02, -1.5411e-02,  4.3363e-03],\n",
      "          [ 5.4436e-02,  4.6006e-03,  4.8543e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5712e-02,  2.0218e-02,  3.5309e-03],\n",
      "          [ 6.7880e-03,  5.8105e-02,  9.9551e-03],\n",
      "          [-3.2947e-02, -1.0072e-02, -8.3283e-03]],\n",
      "\n",
      "         [[ 2.3624e-02, -8.1833e-03, -5.5639e-03],\n",
      "          [-4.9125e-02, -1.3022e-02,  3.4954e-02],\n",
      "          [-1.9259e-02, -1.9871e-02,  1.1758e-02]],\n",
      "\n",
      "         [[ 3.5981e-02,  3.6655e-02,  1.5337e-02],\n",
      "          [ 3.4730e-02,  4.0744e-02,  2.9430e-02],\n",
      "          [ 3.2508e-02,  1.4317e-02,  1.2064e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-7.6689e-02, -2.0806e-02, -1.0091e-02],\n",
      "          [-7.2092e-02, -2.2386e-02,  1.6149e-03],\n",
      "          [ 3.2845e-03, -1.2550e-02, -9.5245e-03]],\n",
      "\n",
      "         [[ 9.4428e-04, -1.8823e-02,  2.8533e-03],\n",
      "          [-7.1188e-02, -3.3229e-02, -3.9054e-02],\n",
      "          [-4.3769e-02, -1.7433e-02, -4.6807e-02]],\n",
      "\n",
      "         [[ 2.4509e-02,  1.5497e-02,  3.3231e-02],\n",
      "          [ 5.5186e-02,  4.1249e-02,  4.2920e-03],\n",
      "          [ 4.4418e-02,  3.6271e-02, -7.3691e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9061e-02, -1.7331e-02, -2.4782e-02],\n",
      "          [-1.7213e-02, -5.2352e-02, -3.7812e-02],\n",
      "          [ 3.2423e-03,  8.1354e-03, -1.7615e-02]],\n",
      "\n",
      "         [[ 1.3293e-02,  8.4824e-03,  1.0162e-03],\n",
      "          [ 1.9229e-03, -1.5971e-02,  2.2321e-02],\n",
      "          [-2.1582e-02, -1.0988e-02, -2.3424e-02]],\n",
      "\n",
      "         [[-1.9271e-02, -3.1467e-02, -1.0552e-02],\n",
      "          [-5.1683e-03,  1.9949e-03, -1.4087e-02],\n",
      "          [-1.5934e-02, -1.4734e-02, -2.2559e-02]]],\n",
      "\n",
      "\n",
      "        [[[-5.2005e-03, -2.2183e-02,  1.1908e-02],\n",
      "          [-1.0966e-02,  1.1818e-02, -7.6406e-04],\n",
      "          [ 2.5195e-03, -1.0628e-02,  3.9335e-03]],\n",
      "\n",
      "         [[-4.6711e-02, -4.8605e-02, -1.1717e-02],\n",
      "          [ 2.2588e-02, -2.5952e-02, -2.9146e-02],\n",
      "          [ 6.4695e-03,  8.3176e-03, -2.8849e-02]],\n",
      "\n",
      "         [[-4.5998e-02, -5.2730e-02, -2.6997e-02],\n",
      "          [ 9.1086e-03, -2.2808e-02, -2.2376e-02],\n",
      "          [ 2.4832e-02,  1.9254e-02, -1.3346e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.8042e-02, -1.5516e-02,  2.1054e-02],\n",
      "          [-4.9956e-02, -4.2268e-03,  2.1401e-03],\n",
      "          [-3.6581e-02, -2.4358e-02, -2.3512e-02]],\n",
      "\n",
      "         [[-1.0318e-02,  2.0057e-03,  4.5864e-03],\n",
      "          [ 2.6393e-02,  5.9457e-04,  2.5352e-02],\n",
      "          [ 1.9971e-02,  3.1325e-02,  3.4757e-02]],\n",
      "\n",
      "         [[ 6.5325e-02,  6.1399e-02,  3.9453e-02],\n",
      "          [-3.5309e-03,  9.9860e-03,  4.1285e-02],\n",
      "          [-4.0655e-02, -4.2517e-02, -7.0008e-03]]],\n",
      "\n",
      "\n",
      "        [[[-4.3350e-03, -2.0599e-02, -4.0318e-02],\n",
      "          [ 3.5998e-03, -3.0793e-02, -2.7760e-02],\n",
      "          [-4.9662e-03, -9.5880e-03, -4.0714e-02]],\n",
      "\n",
      "         [[ 3.5919e-02,  4.4915e-02,  1.8771e-02],\n",
      "          [-1.9651e-02, -7.5069e-03, -1.2027e-02],\n",
      "          [-1.5590e-02, -5.7618e-02, -4.6663e-02]],\n",
      "\n",
      "         [[ 1.2115e-02, -4.0581e-03, -5.0300e-03],\n",
      "          [ 2.5166e-02,  1.4569e-02,  2.9773e-02],\n",
      "          [ 1.7929e-02,  2.1358e-02, -1.6422e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.5589e-02,  3.5840e-03, -2.5180e-02],\n",
      "          [ 1.9872e-02,  1.4046e-02,  1.1726e-02],\n",
      "          [-1.0756e-02,  2.8111e-03, -9.1095e-03]],\n",
      "\n",
      "         [[-2.7949e-02, -2.4676e-03, -1.7961e-02],\n",
      "          [ 4.0525e-02,  4.1696e-02,  1.8982e-02],\n",
      "          [ 3.6418e-02, -3.0222e-02, -1.6673e-02]],\n",
      "\n",
      "         [[ 1.2285e-02,  1.6912e-02,  3.0007e-02],\n",
      "          [-1.2732e-02, -3.1947e-02, -1.8213e-02],\n",
      "          [-2.2825e-02, -2.7121e-02, -2.7280e-02]]]], device='cuda:0'))\n",
      "('conv3.0.bias', Parameter containing:\n",
      "tensor([ 3.4846e-08, -6.3048e-08,  1.5323e-08,  6.8421e-09,  4.7507e-08,\n",
      "         1.0983e-07,  1.4882e-08,  4.8230e-08,  7.0195e-08, -2.9104e-10,\n",
      "        -8.8180e-08, -8.4032e-08, -1.0716e-08, -8.2087e-08,  9.9630e-09,\n",
      "         2.1278e-09,  9.9363e-08, -9.6144e-08, -1.0633e-07, -4.7006e-08,\n",
      "         1.1766e-08, -5.5596e-09,  2.1944e-08,  6.0281e-08, -7.8261e-08,\n",
      "         3.0018e-08,  8.0650e-08,  1.5216e-07,  3.3329e-08, -6.8949e-08,\n",
      "         9.8214e-09,  5.4927e-08,  8.5607e-08,  5.5721e-08,  4.2242e-08,\n",
      "        -2.2612e-08,  4.6168e-08, -4.1456e-08, -1.0612e-08, -6.4410e-08,\n",
      "        -4.4699e-08,  1.4034e-07,  5.5063e-08, -6.3094e-08, -5.3484e-08,\n",
      "         2.1930e-07,  2.3592e-08,  1.5408e-08, -2.6490e-08, -5.0192e-08,\n",
      "         9.1640e-08, -4.1150e-08, -6.0505e-08, -2.5885e-08, -2.4022e-08,\n",
      "        -1.2518e-08, -4.3879e-08,  3.0029e-08,  9.0262e-08,  1.6323e-08,\n",
      "         4.9384e-08,  4.4549e-08,  4.0438e-08, -5.2995e-09,  1.6626e-08,\n",
      "         8.7507e-08,  5.5383e-08,  4.3178e-08, -4.6001e-08,  5.0683e-08,\n",
      "        -9.0960e-08, -8.2918e-08, -1.6141e-08,  5.8365e-08,  2.6210e-08,\n",
      "        -7.9013e-08, -2.5426e-08, -9.7425e-08, -4.6061e-08,  1.0166e-08,\n",
      "         1.2891e-07,  4.6389e-08, -7.1299e-09, -3.7853e-08,  2.0774e-08,\n",
      "         1.8960e-09, -4.7673e-08, -7.1486e-08,  6.9865e-08, -1.4224e-07,\n",
      "        -5.1904e-08,  3.1243e-08,  2.4364e-07, -7.8002e-08, -1.6293e-09,\n",
      "         2.3375e-08,  7.2643e-09, -8.0646e-08, -8.0064e-08,  2.2469e-08,\n",
      "        -4.8347e-09, -1.8004e-07, -7.4000e-09,  3.1202e-08,  4.4872e-09,\n",
      "         1.3202e-07, -2.3685e-08, -1.2969e-08, -1.9367e-08,  8.5590e-08,\n",
      "        -1.2725e-07, -3.1701e-09, -7.6860e-08,  4.0830e-08,  4.8675e-09,\n",
      "        -6.3568e-08, -8.4824e-08,  1.0952e-07,  1.2443e-08,  1.2026e-08,\n",
      "         1.7201e-08, -4.3590e-08, -2.3811e-08, -6.2739e-09, -4.3533e-08,\n",
      "        -4.4239e-08, -5.6155e-09,  6.2839e-09, -7.6242e-08,  7.9493e-08,\n",
      "         2.0356e-09,  3.3327e-08,  1.2768e-07,  3.7531e-08,  1.6815e-08,\n",
      "        -8.8137e-08,  2.2679e-08,  1.4872e-08,  5.2759e-08,  1.1558e-08,\n",
      "        -7.3168e-08,  4.4626e-09,  5.8443e-08, -1.9654e-08, -1.3523e-08,\n",
      "        -2.1448e-08, -3.4638e-08,  2.0629e-08, -2.5955e-08,  2.9454e-08,\n",
      "         8.0901e-08, -1.0990e-07,  1.0111e-08,  2.9568e-08,  5.3829e-09,\n",
      "         4.2247e-08, -1.3733e-08, -2.9032e-08,  1.2445e-08, -7.1625e-08,\n",
      "        -2.4129e-08, -7.8358e-08,  8.4631e-09, -5.5979e-08,  3.5620e-08,\n",
      "        -4.8737e-08,  9.8923e-09,  7.4793e-08,  2.3337e-08, -5.4003e-08,\n",
      "         1.6958e-08,  4.2057e-08, -4.2082e-08, -3.2802e-09,  3.4053e-08,\n",
      "        -6.9680e-08,  6.8425e-08,  5.2941e-08, -5.2443e-09, -4.8478e-08,\n",
      "        -2.0034e-08,  6.5743e-08,  1.2508e-08,  2.5151e-08, -4.5568e-09,\n",
      "         3.9962e-08,  4.3358e-08, -1.0662e-08, -1.9756e-09, -1.8523e-08,\n",
      "        -1.1769e-08,  1.1665e-08, -9.1974e-08,  5.0555e-08,  2.5497e-08,\n",
      "        -6.4659e-08,  9.0925e-08,  1.4319e-07, -9.9199e-10, -6.0171e-08,\n",
      "         2.4786e-08,  2.2372e-08, -4.4525e-08,  2.4026e-09,  5.6049e-08,\n",
      "        -4.8898e-08, -6.1918e-08, -1.4637e-08,  2.4510e-08,  5.7842e-08,\n",
      "        -6.6174e-08, -2.5079e-08, -3.4475e-08,  8.4979e-08,  5.0109e-09,\n",
      "        -7.8825e-09,  8.1516e-08, -4.9620e-08,  4.5402e-09, -2.6093e-08,\n",
      "         8.3554e-08,  5.6554e-08, -4.1296e-08, -1.5530e-07, -6.6316e-09,\n",
      "        -5.1553e-08, -9.5489e-08, -9.1084e-08, -6.2883e-08,  1.2208e-07,\n",
      "        -1.9765e-08,  6.0842e-08,  1.4932e-08, -1.3171e-08, -5.7635e-08,\n",
      "        -8.5176e-08, -8.9030e-09,  2.4882e-08, -5.3389e-08, -3.1564e-08,\n",
      "         7.0925e-09,  9.9798e-08, -1.1361e-08, -4.0711e-08, -6.6495e-08,\n",
      "        -8.3883e-08, -7.4679e-08,  2.4640e-08,  1.0563e-08,  2.3641e-08,\n",
      "        -3.0682e-09, -1.9283e-08,  8.5851e-08, -7.5689e-08, -8.8772e-08,\n",
      "        -1.7134e-08], device='cuda:0'))\n",
      "('conv3.1.weight', Parameter containing:\n",
      "tensor([0.5981, 0.6729, 0.6541, 0.5642, 0.6447, 0.6656, 0.7261, 0.6312, 0.5696,\n",
      "        0.6620, 0.6134, 0.6068, 0.6061, 0.7540, 0.5802, 0.6687, 0.6648, 0.6177,\n",
      "        0.6144, 0.6118, 0.5627, 0.6793, 0.6032, 0.6906, 0.6083, 0.5103, 0.6131,\n",
      "        0.7433, 0.6653, 0.6182, 0.6315, 0.6456, 0.6623, 0.5475, 0.6070, 0.5657,\n",
      "        0.5893, 0.7450, 0.6586, 0.6270, 0.5841, 0.6382, 0.6345, 0.6711, 0.6384,\n",
      "        0.7321, 0.6048, 0.7114, 0.6103, 0.5454, 0.5924, 0.6279, 0.6744, 0.6214,\n",
      "        0.6701, 0.6302, 0.6641, 0.6636, 0.6113, 0.6765, 0.5508, 0.5063, 0.6586,\n",
      "        0.7320, 0.6657, 0.7265, 0.7245, 0.5957, 0.6994, 0.6531, 0.6366, 0.5945,\n",
      "        0.6245, 0.5521, 0.7181, 0.5677, 0.6816, 0.6208, 0.6673, 0.5395, 0.6481,\n",
      "        0.6352, 0.6447, 0.6252, 0.5658, 0.6368, 0.6560, 0.6246, 0.6099, 0.6936,\n",
      "        0.5656, 0.6963, 0.6524, 0.6506, 0.6553, 0.6627, 0.5822, 0.6466, 0.7719,\n",
      "        0.6297, 0.6383, 0.6351, 0.6259, 0.5508, 0.6735, 0.6509, 0.7096, 0.6499,\n",
      "        0.8369, 0.5397, 0.6791, 0.5618, 0.5451, 0.5938, 0.5660, 0.6865, 0.6986,\n",
      "        0.6104, 0.6317, 0.5666, 0.6583, 0.6030, 0.5992, 0.7049, 0.7439, 0.6986,\n",
      "        0.6486, 0.6396, 0.6297, 0.6069, 0.6831, 0.6068, 0.7857, 0.6163, 0.6670,\n",
      "        0.6362, 0.5190, 0.6522, 0.5999, 0.6118, 0.6311, 0.6785, 0.5996, 0.5881,\n",
      "        0.6282, 0.8280, 0.5724, 0.5935, 0.6322, 0.6342, 0.6377, 0.6146, 0.7224,\n",
      "        0.4712, 0.7549, 0.7235, 0.6380, 0.5642, 0.5508, 0.6882, 0.6511, 0.6132,\n",
      "        0.6102, 0.5992, 0.6765, 0.6273, 0.6012, 0.6110, 0.5510, 0.6740, 0.5850,\n",
      "        0.6697, 0.5726, 0.5682, 0.6783, 0.6194, 0.6555, 0.6000, 0.7505, 0.5740,\n",
      "        0.5225, 0.6796, 0.5997, 0.6139, 0.6211, 0.5963, 0.6062, 0.7203, 0.5670,\n",
      "        0.6819, 0.7766, 0.6232, 0.7041, 0.6043, 0.6004, 0.5835, 0.6544, 0.5953,\n",
      "        0.5987, 0.5782, 0.6534, 0.6572, 0.5829, 0.6103, 0.6453, 0.4553, 0.6582,\n",
      "        0.6282, 0.7003, 0.6481, 0.6682, 0.6163, 0.6153, 0.6829, 0.6217, 0.6390,\n",
      "        0.6550, 0.7079, 0.6886, 0.5741, 0.7036, 0.6002, 0.8106, 0.6646, 0.6672,\n",
      "        0.5416, 0.6780, 0.6257, 0.5590, 0.5680, 0.6087, 0.6532, 0.5211, 0.6469,\n",
      "        0.4742, 0.6772, 0.6050, 0.6900, 0.6237, 0.6758, 0.7527, 0.5692, 0.6804,\n",
      "        0.6639, 0.5354, 0.6381, 0.5807, 0.5855, 0.6362, 0.6154, 0.6844, 0.5652,\n",
      "        0.6463, 0.6187, 0.6250, 0.6899], device='cuda:0'))\n",
      "('conv3.1.bias', Parameter containing:\n",
      "tensor([-0.2247, -0.3131, -0.2904, -0.2946, -0.2507, -0.3315, -0.3222, -0.3032,\n",
      "        -0.2574, -0.2324, -0.2896, -0.2679, -0.2575, -0.3330, -0.2450, -0.2815,\n",
      "        -0.3024, -0.2720, -0.2072, -0.2422, -0.1642, -0.2971, -0.2131, -0.2239,\n",
      "        -0.2649, -0.2558, -0.3586, -0.3194, -0.3039, -0.2971, -0.2208, -0.2673,\n",
      "        -0.3372, -0.2674, -0.2745, -0.2493, -0.2567, -0.3432, -0.3151, -0.2480,\n",
      "        -0.2591, -0.2532, -0.3350, -0.3162, -0.3213, -0.2769, -0.2654, -0.2576,\n",
      "        -0.1995, -0.2540, -0.2810, -0.3547, -0.2257, -0.3348, -0.3407, -0.3029,\n",
      "        -0.3202, -0.3308, -0.2525, -0.2887, -0.2111, -0.2430, -0.2203, -0.3142,\n",
      "        -0.3050, -0.3566, -0.2186, -0.2925, -0.3180, -0.2204, -0.2398, -0.2475,\n",
      "        -0.2799, -0.2729, -0.3221, -0.2859, -0.3037, -0.2792, -0.3069, -0.2734,\n",
      "        -0.2855, -0.2904, -0.3002, -0.2807, -0.2897, -0.2511, -0.2533, -0.2314,\n",
      "        -0.3393, -0.3317, -0.2213, -0.2402, -0.2672, -0.3078, -0.2762, -0.2846,\n",
      "        -0.2766, -0.2607, -0.2988, -0.2520, -0.2995, -0.2737, -0.3465, -0.2583,\n",
      "        -0.3249, -0.3214, -0.3272, -0.2753, -0.2959, -0.2288, -0.2658, -0.2152,\n",
      "        -0.1982, -0.2706, -0.2862, -0.2774, -0.2703, -0.2950, -0.2266, -0.2196,\n",
      "        -0.3094, -0.2676, -0.2362, -0.2649, -0.3398, -0.2904, -0.3278, -0.3282,\n",
      "        -0.2765, -0.2488, -0.2751, -0.3036, -0.2836, -0.2068, -0.2772, -0.2802,\n",
      "        -0.2136, -0.2977, -0.2564, -0.1933, -0.2320, -0.2931, -0.2752, -0.2871,\n",
      "        -0.2923, -0.3051, -0.2514, -0.2649, -0.2861, -0.2725, -0.2792, -0.2490,\n",
      "        -0.3003, -0.2586, -0.2243, -0.1989, -0.3277, -0.2327, -0.2767, -0.2894,\n",
      "        -0.2720, -0.2775, -0.2928, -0.2748, -0.2887, -0.2782, -0.2922, -0.2874,\n",
      "        -0.2917, -0.3375, -0.2564, -0.3045, -0.2618, -0.2542, -0.2825, -0.2540,\n",
      "        -0.3104, -0.2712, -0.2221, -0.3067, -0.1891, -0.2968, -0.3236, -0.3336,\n",
      "        -0.2613, -0.2869, -0.2589, -0.3067, -0.2401, -0.2907, -0.2709, -0.2682,\n",
      "        -0.3053, -0.2464, -0.3049, -0.3045, -0.3090, -0.2495, -0.2434, -0.2673,\n",
      "        -0.2973, -0.3054, -0.3170, -0.2394, -0.2847, -0.1824, -0.2878, -0.2265,\n",
      "        -0.3095, -0.3217, -0.2939, -0.3243, -0.2366, -0.2944, -0.3428, -0.2838,\n",
      "        -0.2755, -0.2944, -0.2991, -0.2709, -0.2710, -0.2676, -0.2690, -0.3071,\n",
      "        -0.2619, -0.2078, -0.1499, -0.3271, -0.2179, -0.2513, -0.2422, -0.2997,\n",
      "        -0.2619, -0.2891, -0.2841, -0.3252, -0.2838, -0.3356, -0.2950, -0.2188,\n",
      "        -0.2567, -0.2143, -0.3101, -0.3022, -0.2457, -0.2456, -0.3397, -0.2932,\n",
      "        -0.2796, -0.2663, -0.3169, -0.2645, -0.2840, -0.2571, -0.2485, -0.2969],\n",
      "       device='cuda:0'))\n",
      "('conv4.0.weight', Parameter containing:\n",
      "tensor([[[[-0.0047, -0.0118,  0.0046],\n",
      "          [-0.0035, -0.0026, -0.0159],\n",
      "          [ 0.0097, -0.0084, -0.0271]],\n",
      "\n",
      "         [[-0.0230, -0.0284, -0.0100],\n",
      "          [-0.0329, -0.0279, -0.0234],\n",
      "          [-0.0238, -0.0170, -0.0155]],\n",
      "\n",
      "         [[ 0.0022, -0.0187, -0.0036],\n",
      "          [-0.0234, -0.0277, -0.0189],\n",
      "          [-0.0251, -0.0180,  0.0019]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0166, -0.0076, -0.0226],\n",
      "          [ 0.0155,  0.0008, -0.0042],\n",
      "          [-0.0101, -0.0250, -0.0254]],\n",
      "\n",
      "         [[-0.0092, -0.0118, -0.0074],\n",
      "          [ 0.0353,  0.0389,  0.0103],\n",
      "          [ 0.0092,  0.0098, -0.0008]],\n",
      "\n",
      "         [[ 0.0162, -0.0053, -0.0046],\n",
      "          [-0.0087, -0.0232, -0.0154],\n",
      "          [-0.0253, -0.0374, -0.0370]]],\n",
      "\n",
      "\n",
      "        [[[-0.0174,  0.0088,  0.0024],\n",
      "          [-0.0093,  0.0280,  0.0543],\n",
      "          [-0.0029,  0.0123,  0.0254]],\n",
      "\n",
      "         [[ 0.0461, -0.0099,  0.0041],\n",
      "          [ 0.0149,  0.0026,  0.0147],\n",
      "          [-0.0134, -0.0028, -0.0101]],\n",
      "\n",
      "         [[-0.0203,  0.0037, -0.0162],\n",
      "          [ 0.0277,  0.0135, -0.0151],\n",
      "          [ 0.0153, -0.0051, -0.0176]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0054,  0.0088,  0.0221],\n",
      "          [-0.0103,  0.0059,  0.0207],\n",
      "          [-0.0207, -0.0202, -0.0230]],\n",
      "\n",
      "         [[-0.0045,  0.0288,  0.0446],\n",
      "          [ 0.0016, -0.0012,  0.0098],\n",
      "          [ 0.0295, -0.0184, -0.0319]],\n",
      "\n",
      "         [[-0.0187, -0.0219,  0.0134],\n",
      "          [-0.0276, -0.0039,  0.0018],\n",
      "          [-0.0094,  0.0048, -0.0137]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0234,  0.0001,  0.0079],\n",
      "          [ 0.0184,  0.0232,  0.0261],\n",
      "          [ 0.0062,  0.0360, -0.0073]],\n",
      "\n",
      "         [[ 0.0321, -0.0043,  0.0036],\n",
      "          [ 0.0049, -0.0246,  0.0034],\n",
      "          [-0.0015,  0.0066, -0.0085]],\n",
      "\n",
      "         [[ 0.0021, -0.0076, -0.0093],\n",
      "          [-0.0071,  0.0199,  0.0044],\n",
      "          [-0.0018, -0.0139, -0.0328]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0139,  0.0271,  0.0176],\n",
      "          [-0.0023,  0.0027, -0.0191],\n",
      "          [ 0.0224,  0.0022, -0.0265]],\n",
      "\n",
      "         [[ 0.0176,  0.0219, -0.0134],\n",
      "          [ 0.0104,  0.0393, -0.0058],\n",
      "          [-0.0142, -0.0068, -0.0191]],\n",
      "\n",
      "         [[-0.0158, -0.0133,  0.0044],\n",
      "          [-0.0293, -0.0081, -0.0143],\n",
      "          [ 0.0005,  0.0155, -0.0048]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0074,  0.0131,  0.0154],\n",
      "          [-0.0015, -0.0086,  0.0024],\n",
      "          [-0.0103, -0.0077, -0.0201]],\n",
      "\n",
      "         [[ 0.0024,  0.0106, -0.0256],\n",
      "          [ 0.0082,  0.0276, -0.0063],\n",
      "          [ 0.0140,  0.0258, -0.0007]],\n",
      "\n",
      "         [[ 0.0091, -0.0123,  0.0053],\n",
      "          [ 0.0133,  0.0047,  0.0118],\n",
      "          [ 0.0056, -0.0191,  0.0077]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0044, -0.0122,  0.0195],\n",
      "          [ 0.0218, -0.0050,  0.0223],\n",
      "          [ 0.0134,  0.0207,  0.0143]],\n",
      "\n",
      "         [[ 0.0148,  0.0195,  0.0077],\n",
      "          [-0.0020,  0.0188,  0.0188],\n",
      "          [ 0.0066, -0.0031,  0.0218]],\n",
      "\n",
      "         [[-0.0016, -0.0205, -0.0096],\n",
      "          [ 0.0025,  0.0121,  0.0293],\n",
      "          [ 0.0092,  0.0190,  0.0165]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0228,  0.0291,  0.0053],\n",
      "          [ 0.0048, -0.0087, -0.0150],\n",
      "          [ 0.0032, -0.0070, -0.0138]],\n",
      "\n",
      "         [[-0.0062, -0.0415, -0.0063],\n",
      "          [ 0.0079,  0.0024,  0.0200],\n",
      "          [ 0.0075,  0.0001,  0.0054]],\n",
      "\n",
      "         [[-0.0283, -0.0107,  0.0202],\n",
      "          [-0.0076, -0.0029,  0.0270],\n",
      "          [-0.0183, -0.0045,  0.0215]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.0405, -0.0079, -0.0253],\n",
      "          [-0.0213,  0.0069,  0.0025],\n",
      "          [-0.0205,  0.0077,  0.0270]],\n",
      "\n",
      "         [[ 0.0144,  0.0207,  0.0196],\n",
      "          [-0.0111,  0.0076,  0.0224],\n",
      "          [-0.0246, -0.0214,  0.0147]],\n",
      "\n",
      "         [[ 0.0450,  0.0371,  0.0256],\n",
      "          [-0.0034,  0.0249,  0.0215],\n",
      "          [-0.0161, -0.0146, -0.0121]]],\n",
      "\n",
      "\n",
      "        [[[-0.0195, -0.0151, -0.0060],\n",
      "          [ 0.0053, -0.0016, -0.0008],\n",
      "          [ 0.0127, -0.0053, -0.0163]],\n",
      "\n",
      "         [[-0.0062,  0.0052,  0.0032],\n",
      "          [-0.0038,  0.0099, -0.0053],\n",
      "          [ 0.0147, -0.0004, -0.0170]],\n",
      "\n",
      "         [[ 0.0105,  0.0010, -0.0010],\n",
      "          [-0.0022,  0.0243,  0.0391],\n",
      "          [-0.0144,  0.0127,  0.0427]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0059, -0.0016, -0.0364],\n",
      "          [ 0.0049, -0.0202, -0.0249],\n",
      "          [ 0.0167, -0.0036, -0.0175]],\n",
      "\n",
      "         [[ 0.0213,  0.0102,  0.0126],\n",
      "          [-0.0060, -0.0032,  0.0007],\n",
      "          [-0.0196, -0.0286, -0.0168]],\n",
      "\n",
      "         [[ 0.0140, -0.0134,  0.0059],\n",
      "          [-0.0131, -0.0278, -0.0214],\n",
      "          [-0.0305, -0.0458, -0.0217]]]], device='cuda:0'))\n",
      "('conv4.0.bias', Parameter containing:\n",
      "tensor([ 1.4477e-07,  5.5904e-09, -7.6618e-08, -2.4607e-08, -2.2330e-08,\n",
      "        -5.9712e-08, -7.8025e-08, -2.2776e-08,  2.8458e-08, -5.3319e-08,\n",
      "        -3.5984e-08, -4.4085e-08,  2.6599e-09,  6.0501e-08,  3.9359e-08,\n",
      "        -2.6157e-08, -2.2452e-08, -1.2302e-07, -1.7070e-07,  8.9293e-09,\n",
      "        -8.0356e-08,  8.8428e-09, -4.4280e-08, -2.1943e-08,  2.1204e-08,\n",
      "        -9.2088e-08, -3.7511e-08,  4.8857e-08, -2.3353e-08, -6.2306e-09,\n",
      "        -1.7942e-07,  8.0053e-09, -5.8515e-08, -2.9998e-08,  2.9839e-08,\n",
      "         2.2524e-08,  2.5349e-08, -4.6535e-09, -4.4359e-08,  3.8274e-08,\n",
      "         4.3872e-08,  7.7750e-09,  9.0354e-09,  1.3395e-08,  5.8766e-08,\n",
      "        -4.0562e-08,  3.0537e-08,  2.4294e-08,  6.8154e-08,  9.2649e-08,\n",
      "         2.4274e-08, -3.1182e-08, -5.7526e-08, -9.1170e-08, -3.3834e-08,\n",
      "        -1.0476e-07,  9.4059e-09,  4.1897e-08,  6.5110e-08,  2.3552e-08,\n",
      "        -5.3627e-08, -2.9944e-08, -5.7665e-08, -8.5749e-08, -2.6415e-08,\n",
      "         7.7559e-10,  1.5739e-08,  5.2199e-08, -3.6649e-08,  4.0495e-08,\n",
      "        -4.8908e-08, -3.9445e-08, -7.2729e-09,  1.2583e-07, -3.4364e-08,\n",
      "        -1.0760e-09,  1.1958e-07, -7.1559e-08, -7.4969e-08,  5.2728e-08,\n",
      "         1.5261e-07, -3.6037e-11, -1.8157e-08,  1.3863e-08, -6.5455e-09,\n",
      "        -2.9778e-08,  7.0946e-08,  2.9762e-08,  2.8038e-08,  4.4642e-08,\n",
      "         5.3543e-08, -1.1048e-08, -3.8295e-08,  2.3234e-07,  1.2563e-08,\n",
      "         8.9854e-09, -4.2983e-08,  3.0513e-08, -2.2850e-08, -4.3140e-08,\n",
      "        -1.4167e-08, -7.3242e-09, -4.9481e-08,  2.1946e-08, -1.3373e-08,\n",
      "         6.8891e-08,  6.8087e-09, -3.2771e-08,  1.4311e-07, -4.5523e-08,\n",
      "        -2.6362e-08,  3.0977e-08, -3.9715e-08, -3.9857e-08, -4.7830e-08,\n",
      "         8.8704e-08, -4.8981e-08,  2.6303e-09,  7.8338e-09, -4.0577e-08,\n",
      "         4.1926e-09, -1.2397e-08, -2.0545e-08,  3.2122e-08,  8.4238e-08,\n",
      "         5.5411e-08,  7.1493e-08,  3.6208e-08, -2.6586e-08, -1.2460e-09,\n",
      "        -1.8284e-08, -2.2431e-08, -4.5749e-08,  7.6039e-08,  3.6964e-08,\n",
      "         9.9712e-08, -4.3354e-08,  2.7002e-08,  1.0000e-08, -4.0988e-08,\n",
      "         3.8557e-08,  2.8767e-08, -3.4604e-08, -7.3157e-09, -5.7015e-08,\n",
      "         6.0184e-08,  4.4977e-08,  4.7810e-08, -2.1239e-08, -7.0631e-08,\n",
      "        -4.5094e-08,  1.5506e-08, -3.9033e-08,  9.1979e-08, -1.0080e-08,\n",
      "         1.0993e-08,  4.6376e-08,  2.0848e-08,  2.3042e-08,  6.6549e-08,\n",
      "         2.0582e-09, -1.2424e-08,  1.8956e-08,  9.3946e-08,  2.7529e-08,\n",
      "        -4.8031e-08,  2.4589e-08,  3.7020e-08, -8.9646e-09, -2.1503e-08,\n",
      "        -2.3360e-09,  7.8160e-08,  1.0032e-09,  2.9962e-08,  6.5805e-08,\n",
      "         4.1087e-09,  5.4536e-08, -7.4820e-08,  5.4353e-08, -2.1462e-08,\n",
      "        -1.1508e-07, -1.8333e-08,  2.0077e-08,  1.8957e-08, -2.2767e-08,\n",
      "        -7.5526e-08,  2.5749e-09,  4.2297e-08, -2.5955e-08,  5.9240e-08,\n",
      "         2.3299e-08, -4.1769e-08, -1.3204e-07, -2.0181e-08, -9.3396e-08,\n",
      "        -6.3650e-09,  5.3407e-08,  1.8036e-08,  4.9197e-08,  5.8792e-08,\n",
      "        -3.1842e-08,  8.5188e-09, -5.9274e-08, -1.1856e-08, -3.8714e-09,\n",
      "         1.6248e-09,  3.4931e-08, -4.0262e-08, -2.5017e-08, -1.9693e-09,\n",
      "        -9.9452e-09, -8.9485e-09,  1.0911e-08, -3.1185e-08,  7.3419e-09,\n",
      "        -2.6164e-08, -1.7202e-08,  2.4519e-08,  5.7021e-08, -5.3446e-08,\n",
      "        -3.2034e-09, -2.6840e-08,  4.6654e-08, -5.3533e-08, -5.0092e-08,\n",
      "         3.0497e-08, -2.5904e-08, -8.5254e-09, -2.4930e-08, -1.3857e-08,\n",
      "        -4.2157e-08,  2.4694e-08, -5.4139e-09,  3.7711e-09, -1.1187e-08,\n",
      "         5.5575e-09,  9.2728e-08, -8.0270e-08, -6.0100e-09,  1.0652e-07,\n",
      "         7.8270e-08, -1.2759e-08,  2.7100e-08, -3.7822e-08, -2.7481e-08,\n",
      "         5.8937e-08, -2.0951e-08, -2.8254e-08, -4.8712e-08,  9.1474e-08,\n",
      "        -1.1411e-07, -2.8263e-09,  9.7801e-09,  5.4554e-08, -2.2150e-09,\n",
      "         7.0732e-08,  3.8030e-08,  2.3450e-08,  2.3615e-08,  5.2886e-08,\n",
      "         9.4976e-09,  8.9925e-08, -4.9019e-08, -7.3461e-08,  1.5011e-08,\n",
      "         2.0032e-08,  4.6093e-11,  3.2162e-08, -3.0581e-08,  1.1238e-08,\n",
      "        -5.6423e-08,  7.2714e-09, -3.2496e-08, -1.1010e-08,  1.9259e-08,\n",
      "         1.9567e-09,  3.9227e-08,  2.4617e-08,  4.3805e-09, -2.6780e-08,\n",
      "        -6.9402e-08,  1.1824e-07, -1.0130e-07, -5.2814e-08,  7.5225e-09,\n",
      "         9.2573e-09, -2.4875e-08,  3.7916e-09, -3.6078e-08,  6.7248e-08,\n",
      "         7.3653e-08,  3.2462e-08,  3.6924e-08, -2.1817e-08,  5.4044e-08,\n",
      "         6.1986e-08,  4.3484e-08,  5.7479e-09, -7.0598e-08,  3.7055e-09,\n",
      "        -5.2364e-08, -1.8106e-07, -9.5610e-08, -5.8023e-09,  2.8602e-08,\n",
      "        -1.8418e-08, -6.7578e-08,  6.6206e-08,  7.4964e-08, -4.0623e-08,\n",
      "         2.9227e-08,  5.7049e-08,  1.1105e-08,  1.1047e-07, -1.6066e-08,\n",
      "        -8.9676e-08, -2.6793e-07, -1.1179e-08,  1.3929e-08, -3.6641e-08,\n",
      "         2.4201e-08,  1.7016e-08,  1.8895e-09, -3.7888e-08,  2.0865e-08,\n",
      "         2.2067e-08, -9.4741e-09, -6.2820e-09,  2.8995e-08, -2.3127e-09,\n",
      "        -3.4702e-08, -6.4433e-09,  1.1905e-07, -3.2675e-08, -3.8406e-09,\n",
      "        -2.5442e-10,  3.1929e-08,  8.9996e-08, -7.9016e-08,  3.1891e-08,\n",
      "        -3.8928e-08,  3.5819e-08, -3.2714e-08, -2.2853e-08, -1.2701e-08,\n",
      "        -4.3426e-08,  1.4335e-08, -9.3114e-08, -5.3037e-08, -9.5625e-08,\n",
      "        -3.8089e-08,  4.1071e-08,  1.2477e-08,  5.2747e-08,  5.3803e-08,\n",
      "        -8.2449e-09, -2.0979e-08,  3.6245e-08,  4.9580e-08,  7.2944e-08,\n",
      "        -3.8572e-08,  5.5344e-09, -1.0707e-09,  5.8726e-08,  7.5722e-08,\n",
      "        -3.6505e-08,  1.1159e-07, -1.7953e-08, -2.9246e-08, -1.6159e-08,\n",
      "         5.0687e-08, -1.4779e-09,  1.7571e-08, -5.0019e-08,  7.5447e-08,\n",
      "        -9.3370e-08,  7.2580e-09,  3.8950e-08,  2.7725e-08,  1.6413e-08,\n",
      "        -3.8539e-08,  4.2307e-08, -6.0986e-09,  1.0273e-08, -9.4068e-09,\n",
      "        -3.0976e-08,  9.6568e-08, -3.5206e-08,  1.3269e-08, -2.2466e-08,\n",
      "        -1.3218e-07, -4.3101e-08,  5.6676e-08,  1.5047e-08, -1.0217e-07,\n",
      "         9.5523e-09,  6.1575e-08, -3.1174e-09, -6.8165e-08, -1.2998e-07,\n",
      "         3.7956e-08,  5.4439e-08, -3.5150e-08,  8.2510e-08,  6.6440e-09,\n",
      "         4.9913e-08, -2.1239e-08,  9.1498e-08, -1.3515e-08, -6.0786e-08,\n",
      "        -9.1852e-08, -4.6281e-08, -7.5185e-08,  1.4307e-08, -3.6884e-08,\n",
      "        -1.3827e-07, -2.7750e-08,  1.2225e-08, -1.0431e-08,  7.4519e-08,\n",
      "        -1.1791e-07, -8.9953e-08, -4.5395e-08,  4.1096e-08,  1.3364e-08,\n",
      "         4.0063e-09,  6.4578e-08,  5.0119e-08,  1.9926e-08, -5.2792e-10,\n",
      "         6.1887e-08,  1.4195e-07,  8.3968e-09,  1.3596e-08,  1.3859e-08,\n",
      "         1.8751e-08, -2.7153e-08,  3.9153e-08,  4.8844e-08,  1.1218e-07,\n",
      "         1.5704e-08,  7.2943e-08, -6.0776e-08,  2.9943e-09,  3.5444e-08,\n",
      "        -3.4632e-08, -4.3073e-08, -4.6471e-08, -1.7754e-09,  4.1297e-08,\n",
      "        -6.3354e-08, -6.6254e-09, -6.7798e-08, -2.6458e-08, -1.6500e-07,\n",
      "         3.8125e-08, -8.0782e-10, -1.9031e-08,  1.9993e-08, -1.9663e-08,\n",
      "         4.2825e-08,  2.5084e-08,  1.2522e-09,  4.7689e-08, -1.2381e-07,\n",
      "         6.2033e-09, -6.0486e-08,  2.8231e-08,  9.9750e-08,  3.4546e-08,\n",
      "        -1.1957e-09, -4.5059e-08,  5.2562e-08,  3.9594e-08,  1.3158e-08,\n",
      "        -2.2959e-08, -1.6928e-08,  3.4791e-08,  2.3562e-08,  8.6511e-08,\n",
      "         1.0879e-08,  9.6069e-08, -2.2067e-08,  4.3245e-08, -4.2205e-08,\n",
      "        -2.4803e-08, -5.3713e-08,  1.2954e-08, -1.2689e-07,  5.2566e-09,\n",
      "        -5.1399e-08, -9.2804e-09,  1.5342e-08, -4.8972e-08,  1.8094e-08,\n",
      "         4.1637e-08,  3.3903e-08,  4.9427e-08,  1.2060e-08, -6.7537e-08,\n",
      "         6.8448e-08, -4.2428e-08, -4.9414e-09, -3.7962e-08,  2.3404e-08,\n",
      "        -1.4536e-08,  4.4283e-08, -4.8965e-08, -2.1671e-08, -3.9404e-09,\n",
      "         1.1616e-07,  3.7838e-08], device='cuda:0'))\n",
      "('conv4.1.weight', Parameter containing:\n",
      "tensor([0.4113, 0.4189, 0.4067, 0.4879, 0.4796, 0.4102, 0.4321, 0.4018, 0.4195,\n",
      "        0.4359, 0.4756, 0.4869, 0.3733, 0.4787, 0.4288, 0.4301, 0.6099, 0.5376,\n",
      "        0.3764, 0.4435, 0.5352, 0.4665, 0.5277, 0.4876, 0.4020, 0.4308, 0.4422,\n",
      "        0.4423, 0.4185, 0.5854, 0.4853, 0.4305, 0.5334, 0.4504, 0.5304, 0.4297,\n",
      "        0.4238, 0.3844, 0.4875, 0.4392, 0.5351, 0.4682, 0.5062, 0.4880, 0.4232,\n",
      "        0.4360, 0.4227, 0.4091, 0.4349, 0.6265, 0.4841, 0.3693, 0.4879, 0.4665,\n",
      "        0.5357, 0.4266, 0.5069, 0.4973, 0.3929, 0.4098, 0.4435, 0.4483, 0.4529,\n",
      "        0.4387, 0.4748, 0.5240, 0.3834, 0.4937, 0.4309, 0.3672, 0.4674, 0.4489,\n",
      "        0.4523, 0.6142, 0.4696, 0.4380, 0.4803, 0.5147, 0.4729, 0.5174, 0.4425,\n",
      "        0.4344, 0.4519, 0.5696, 0.4785, 0.4591, 0.4623, 0.5415, 0.4307, 0.4282,\n",
      "        0.4606, 0.4144, 0.4842, 0.4584, 0.5060, 0.4715, 0.4459, 0.4624, 0.4498,\n",
      "        0.4766, 0.4725, 0.4985, 0.5539, 0.3887, 0.5283, 0.5852, 0.4722, 0.3649,\n",
      "        0.4739, 0.4712, 0.4832, 0.4318, 0.4983, 0.4077, 0.4573, 0.4686, 0.4782,\n",
      "        0.4617, 0.5669, 0.3788, 0.4806, 0.4660, 0.5086, 0.4725, 0.4661, 0.5207,\n",
      "        0.4504, 0.4772, 0.4366, 0.4786, 0.3820, 0.4388, 0.4710, 0.5058, 0.4682,\n",
      "        0.6287, 0.5064, 0.3908, 0.4415, 0.4088, 0.4092, 0.4598, 0.3802, 0.4331,\n",
      "        0.5167, 0.4565, 0.3751, 0.4621, 0.3869, 0.4611, 0.4448, 0.5009, 0.3971,\n",
      "        0.3786, 0.4279, 0.4660, 0.4814, 0.4836, 0.4682, 0.4278, 0.4696, 0.4722,\n",
      "        0.4438, 0.4223, 0.4987, 0.4010, 0.4278, 0.4233, 0.4287, 0.5286, 0.4570,\n",
      "        0.4527, 0.4671, 0.4383, 0.4485, 0.4497, 0.4900, 0.5543, 0.5673, 0.4460,\n",
      "        0.4895, 0.5103, 0.3808, 0.4690, 0.4091, 0.3939, 0.3773, 0.3946, 0.4921,\n",
      "        0.5109, 0.4549, 0.4982, 0.5001, 0.4091, 0.4551, 0.5167, 0.4631, 0.4447,\n",
      "        0.4174, 0.4272, 0.5153, 0.4104, 0.4657, 0.4066, 0.4541, 0.4798, 0.4761,\n",
      "        0.4541, 0.3530, 0.4677, 0.4180, 0.5110, 0.4977, 0.4024, 0.5019, 0.4246,\n",
      "        0.5497, 0.3610, 0.4109, 0.5324, 0.4476, 0.3871, 0.4540, 0.4164, 0.4596,\n",
      "        0.4440, 0.4764, 0.3907, 0.4454, 0.4529, 0.3571, 0.4738, 0.4514, 0.4945,\n",
      "        0.4524, 0.3741, 0.5313, 0.5873, 0.4151, 0.4572, 0.5836, 0.4447, 0.4561,\n",
      "        0.4847, 0.4304, 0.4474, 0.4407, 0.4272, 0.4294, 0.4372, 0.4211, 0.5272,\n",
      "        0.4894, 0.4563, 0.4072, 0.4657, 0.5155, 0.4328, 0.4072, 0.4655, 0.5764,\n",
      "        0.4098, 0.3505, 0.5080, 0.4883, 0.4216, 0.5283, 0.4955, 0.4229, 0.4333,\n",
      "        0.4430, 0.4003, 0.4361, 0.4911, 0.4528, 0.3878, 0.4018, 0.3923, 0.5034,\n",
      "        0.4542, 0.3878, 0.5086, 0.4278, 0.4496, 0.4657, 0.4769, 0.4129, 0.3961,\n",
      "        0.4613, 0.3772, 0.4854, 0.4308, 0.5118, 0.4342, 0.4091, 0.4841, 0.3506,\n",
      "        0.4270, 0.4606, 0.5531, 0.4901, 0.4917, 0.4220, 0.3767, 0.4160, 0.3817,\n",
      "        0.5053, 0.3774, 0.4517, 0.5067, 0.3981, 0.4517, 0.3755, 0.4801, 0.3929,\n",
      "        0.4472, 0.4580, 0.5121, 0.4067, 0.3604, 0.4082, 0.4344, 0.3862, 0.4806,\n",
      "        0.4613, 0.4711, 0.4337, 0.4851, 0.5400, 0.4946, 0.3874, 0.4834, 0.4518,\n",
      "        0.4269, 0.5690, 0.4460, 0.4151, 0.4374, 0.5258, 0.4611, 0.4073, 0.4134,\n",
      "        0.4931, 0.4822, 0.4201, 0.4373, 0.4776, 0.3799, 0.4136, 0.4628, 0.4379,\n",
      "        0.3707, 0.4664, 0.4562, 0.5228, 0.5756, 0.4477, 0.4497, 0.4452, 0.4738,\n",
      "        0.5122, 0.4439, 0.4402, 0.3145, 0.3702, 0.3893, 0.4518, 0.4594, 0.4415,\n",
      "        0.4467, 0.4561, 0.3992, 0.4962, 0.4174, 0.4498, 0.4739, 0.4563, 0.4378,\n",
      "        0.4144, 0.4868, 0.5259, 0.3993, 0.4900, 0.4695, 0.4218, 0.4914, 0.5029,\n",
      "        0.4826, 0.4810, 0.4364, 0.4465, 0.4207, 0.5360, 0.4149, 0.4678, 0.4954,\n",
      "        0.4052, 0.5096, 0.4086, 0.4990, 0.5380, 0.4569, 0.4705, 0.4350, 0.3995,\n",
      "        0.4597, 0.3944, 0.4386, 0.3798, 0.4795, 0.4701, 0.4459, 0.5128, 0.4537,\n",
      "        0.5491, 0.4139, 0.4606, 0.3421, 0.4903, 0.4334, 0.3544, 0.4327, 0.3794,\n",
      "        0.4377, 0.4371, 0.4737, 0.4587, 0.5020, 0.5127, 0.4326, 0.5761, 0.4350,\n",
      "        0.3453, 0.4668, 0.4953, 0.4571, 0.3991, 0.5140, 0.4529, 0.4146, 0.4938,\n",
      "        0.4492, 0.4266, 0.4411, 0.3719, 0.5583, 0.4156, 0.4629, 0.5039, 0.4566,\n",
      "        0.4229, 0.4459, 0.4622, 0.4617, 0.4720, 0.4058, 0.4418, 0.5232, 0.4951,\n",
      "        0.4529, 0.4478, 0.4595, 0.4443, 0.4687, 0.4960, 0.3859, 0.4307, 0.4284,\n",
      "        0.3919, 0.4336, 0.4486, 0.4817, 0.4914, 0.4364, 0.4463, 0.4302, 0.4011,\n",
      "        0.5090, 0.4775, 0.5317, 0.4858, 0.5032, 0.3911, 0.4478, 0.4297, 0.4446,\n",
      "        0.5167, 0.4157, 0.5615, 0.4311, 0.5237, 0.5321, 0.5229, 0.4413, 0.4601,\n",
      "        0.4603, 0.3980, 0.5229, 0.4355, 0.4504, 0.3977, 0.5049, 0.4501, 0.4010,\n",
      "        0.3962, 0.5081, 0.3894, 0.4548, 0.4036, 0.4387, 0.4235, 0.4234],\n",
      "       device='cuda:0'))\n",
      "('conv4.1.bias', Parameter containing:\n",
      "tensor([-0.3724, -0.3816, -0.3993, -0.4119, -0.4437, -0.3859, -0.3674, -0.4053,\n",
      "        -0.4128, -0.3797, -0.3605, -0.3445, -0.3478, -0.4183, -0.3863, -0.4043,\n",
      "        -0.3874, -0.4352, -0.3873, -0.3707, -0.4979, -0.4353, -0.4495, -0.4442,\n",
      "        -0.3836, -0.4005, -0.3696, -0.3635, -0.3746, -0.4762, -0.3879, -0.3044,\n",
      "        -0.3765, -0.4801, -0.3181, -0.3612, -0.4158, -0.4239, -0.4114, -0.3652,\n",
      "        -0.3814, -0.4320, -0.5096, -0.4642, -0.3997, -0.3895, -0.3895, -0.3552,\n",
      "        -0.4120, -0.5221, -0.3446, -0.4100, -0.4218, -0.4218, -0.4551, -0.3373,\n",
      "        -0.4700, -0.4572, -0.3590, -0.3624, -0.4340, -0.4077, -0.4530, -0.3755,\n",
      "        -0.4054, -0.4417, -0.3673, -0.4302, -0.3957, -0.3477, -0.4243, -0.4122,\n",
      "        -0.3950, -0.4438, -0.3935, -0.4172, -0.3242, -0.4306, -0.4423, -0.4876,\n",
      "        -0.4321, -0.3831, -0.4013, -0.4510, -0.3941, -0.4105, -0.3877, -0.4221,\n",
      "        -0.4263, -0.3958, -0.4517, -0.3698, -0.4127, -0.3531, -0.3549, -0.4328,\n",
      "        -0.4386, -0.4303, -0.4393, -0.4453, -0.3815, -0.4038, -0.3560, -0.3494,\n",
      "        -0.4783, -0.4696, -0.4257, -0.4416, -0.3237, -0.4054, -0.3739, -0.4031,\n",
      "        -0.4411, -0.3229, -0.4174, -0.3946, -0.4261, -0.4399, -0.4216, -0.3760,\n",
      "        -0.3850, -0.4448, -0.3711, -0.4111, -0.4165, -0.4560, -0.4341, -0.4526,\n",
      "        -0.4554, -0.4349, -0.3730, -0.3780, -0.4454, -0.4516, -0.3975, -0.4372,\n",
      "        -0.4358, -0.3694, -0.4017, -0.3449, -0.4605, -0.4219, -0.3536, -0.4338,\n",
      "        -0.4147, -0.3750, -0.4017, -0.4792, -0.3453, -0.4195, -0.4128, -0.4682,\n",
      "        -0.4178, -0.3811, -0.3783, -0.3980, -0.4326, -0.4378, -0.4473, -0.4166,\n",
      "        -0.4325, -0.4585, -0.3573, -0.4074, -0.4244, -0.3591, -0.4095, -0.3991,\n",
      "        -0.4110, -0.3617, -0.4263, -0.3428, -0.3988, -0.4056, -0.4472, -0.3691,\n",
      "        -0.4295, -0.4614, -0.4970, -0.4426, -0.4556, -0.4339, -0.3445, -0.3510,\n",
      "        -0.3687, -0.3861, -0.3207, -0.3658, -0.4308, -0.4239, -0.4018, -0.4550,\n",
      "        -0.4059, -0.3863, -0.4029, -0.4411, -0.4020, -0.3796, -0.4031, -0.3975,\n",
      "        -0.4325, -0.3932, -0.3990, -0.3702, -0.4023, -0.3848, -0.3794, -0.4299,\n",
      "        -0.3429, -0.4019, -0.3825, -0.4037, -0.4013, -0.3385, -0.4681, -0.3711,\n",
      "        -0.3255, -0.3429, -0.3784, -0.3847, -0.4008, -0.3938, -0.3710, -0.3302,\n",
      "        -0.4632, -0.4258, -0.3894, -0.3714, -0.4103, -0.4073, -0.3423, -0.4311,\n",
      "        -0.4096, -0.4003, -0.3909, -0.3156, -0.4275, -0.4526, -0.4120, -0.4549,\n",
      "        -0.4981, -0.4301, -0.4309, -0.4063, -0.3618, -0.4173, -0.3946, -0.3899,\n",
      "        -0.3803, -0.4582, -0.3810, -0.4262, -0.4677, -0.4375, -0.3627, -0.3986,\n",
      "        -0.4186, -0.3746, -0.3997, -0.4108, -0.4502, -0.3405, -0.3314, -0.4543,\n",
      "        -0.4830, -0.3949, -0.4330, -0.4407, -0.3696, -0.3908, -0.3676, -0.3562,\n",
      "        -0.4097, -0.4054, -0.4101, -0.3927, -0.3531, -0.3964, -0.4412, -0.4148,\n",
      "        -0.3944, -0.3909, -0.4183, -0.3663, -0.4396, -0.4418, -0.3893, -0.3715,\n",
      "        -0.4474, -0.3581, -0.4512, -0.3928, -0.4338, -0.4116, -0.3756, -0.4483,\n",
      "        -0.3473, -0.3930, -0.3912, -0.4939, -0.4161, -0.4499, -0.3661, -0.4084,\n",
      "        -0.3851, -0.3470, -0.4373, -0.3462, -0.4317, -0.4422, -0.4011, -0.4004,\n",
      "        -0.3920, -0.4194, -0.3307, -0.3950, -0.4378, -0.4049, -0.3540, -0.3636,\n",
      "        -0.3881, -0.3638, -0.3666, -0.4411, -0.3988, -0.4409, -0.4115, -0.4544,\n",
      "        -0.4327, -0.4473, -0.3124, -0.3875, -0.4014, -0.3940, -0.4732, -0.3447,\n",
      "        -0.3868, -0.3766, -0.4571, -0.4644, -0.4102, -0.4002, -0.4669, -0.3975,\n",
      "        -0.3877, -0.4486, -0.3919, -0.3431, -0.4226, -0.3749, -0.3644, -0.3615,\n",
      "        -0.4348, -0.4240, -0.4625, -0.3473, -0.4658, -0.4392, -0.4446, -0.4097,\n",
      "        -0.5026, -0.3581, -0.4507, -0.2938, -0.3751, -0.3621, -0.4103, -0.4160,\n",
      "        -0.4144, -0.4205, -0.3818, -0.3771, -0.3741, -0.3468, -0.3943, -0.3948,\n",
      "        -0.3966, -0.4149, -0.3663, -0.4232, -0.3443, -0.3490, -0.4551, -0.4110,\n",
      "        -0.3889, -0.3720, -0.3365, -0.3674, -0.4348, -0.4073, -0.3655, -0.3868,\n",
      "        -0.4388, -0.3273, -0.4339, -0.4211, -0.3978, -0.4542, -0.3926, -0.4452,\n",
      "        -0.5104, -0.3702, -0.3731, -0.4462, -0.3913, -0.4153, -0.3679, -0.3900,\n",
      "        -0.3376, -0.3664, -0.4298, -0.3905, -0.4205, -0.3800, -0.3640, -0.4208,\n",
      "        -0.3486, -0.3078, -0.3764, -0.3989, -0.3209, -0.4068, -0.3599, -0.4305,\n",
      "        -0.4435, -0.4427, -0.4288, -0.3214, -0.3594, -0.3554, -0.4441, -0.4152,\n",
      "        -0.3542, -0.4270, -0.4602, -0.4079, -0.4031, -0.3432, -0.3988, -0.4142,\n",
      "        -0.4399, -0.4380, -0.4169, -0.4377, -0.2992, -0.4124, -0.4043, -0.4291,\n",
      "        -0.4655, -0.4199, -0.3920, -0.3110, -0.3622, -0.4195, -0.4085, -0.4057,\n",
      "        -0.4412, -0.3484, -0.3422, -0.4009, -0.4249, -0.4193, -0.4127, -0.4494,\n",
      "        -0.5016, -0.3452, -0.3860, -0.3873, -0.3566, -0.4392, -0.4126, -0.4379,\n",
      "        -0.3597, -0.4544, -0.3872, -0.3575, -0.3739, -0.3862, -0.4244, -0.4055,\n",
      "        -0.4541, -0.3790, -0.3387, -0.4283, -0.4070, -0.4074, -0.4531, -0.3768,\n",
      "        -0.4490, -0.4222, -0.4006, -0.4109, -0.3612, -0.4096, -0.4144, -0.4122,\n",
      "        -0.3112, -0.4539, -0.3941, -0.4176, -0.3890, -0.3946, -0.4646, -0.4101,\n",
      "        -0.4023, -0.4551, -0.3898, -0.4136, -0.3733, -0.4139, -0.4448, -0.3786],\n",
      "       device='cuda:0'))\n",
      "('res2.0.0.weight', Parameter containing:\n",
      "tensor([[[[-1.5404e-02,  1.2151e-02,  2.3705e-02],\n",
      "          [-1.0218e-02,  3.3836e-02,  4.0423e-02],\n",
      "          [ 2.0618e-04,  2.0667e-02,  3.1181e-02]],\n",
      "\n",
      "         [[-9.2230e-03, -7.7148e-03, -4.8724e-03],\n",
      "          [-1.3986e-02, -6.0717e-03,  5.1700e-03],\n",
      "          [ 7.2429e-04,  6.0914e-03,  6.7276e-03]],\n",
      "\n",
      "         [[-2.4975e-03, -8.8884e-03,  4.5083e-03],\n",
      "          [-1.1855e-02, -6.2873e-03, -4.9808e-03],\n",
      "          [-5.5864e-03,  8.9157e-03,  9.5724e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.4724e-02, -1.2340e-02, -4.7555e-03],\n",
      "          [-2.4640e-02, -8.0678e-03, -1.9450e-02],\n",
      "          [-3.1569e-02, -2.5689e-02, -3.0864e-02]],\n",
      "\n",
      "         [[ 1.4293e-03,  1.4202e-02,  1.6284e-02],\n",
      "          [-2.1144e-03,  1.0494e-03, -4.0255e-03],\n",
      "          [-3.7926e-03, -7.3540e-03, -2.1359e-02]],\n",
      "\n",
      "         [[ 1.2893e-02,  5.4393e-03,  1.0319e-02],\n",
      "          [ 3.4814e-03,  1.0199e-02,  6.3591e-03],\n",
      "          [ 9.3543e-04,  3.2331e-03,  1.1318e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 9.3631e-03, -1.7324e-02, -1.8816e-02],\n",
      "          [ 9.5917e-05, -1.8184e-02, -1.3607e-02],\n",
      "          [ 1.6431e-02, -1.6541e-02, -1.3800e-02]],\n",
      "\n",
      "         [[-1.7204e-02,  7.0571e-03,  5.2446e-03],\n",
      "          [-7.9688e-03,  1.6186e-03, -8.0257e-03],\n",
      "          [-1.8421e-02, -6.0749e-03, -6.5917e-03]],\n",
      "\n",
      "         [[ 2.2715e-02,  4.7177e-03, -1.1601e-02],\n",
      "          [-1.5458e-03,  1.2256e-02,  3.8955e-03],\n",
      "          [ 7.7616e-03,  1.6223e-02,  1.0047e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0062e-03, -4.3164e-03, -6.9169e-03],\n",
      "          [-2.0582e-02, -1.4556e-03,  9.1307e-03],\n",
      "          [ 3.1501e-03,  1.0669e-02,  1.5438e-02]],\n",
      "\n",
      "         [[-2.7062e-02, -3.7525e-02, -4.0303e-02],\n",
      "          [-2.1281e-02, -1.4102e-02, -4.9667e-03],\n",
      "          [-1.0965e-02, -5.7008e-03, -2.7418e-03]],\n",
      "\n",
      "         [[-3.8311e-03, -5.8075e-03, -7.8288e-03],\n",
      "          [ 7.3359e-03,  5.7009e-03,  3.7316e-04],\n",
      "          [ 1.5980e-03, -7.8029e-03, -2.9855e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0958e-02,  1.4122e-03, -3.0467e-03],\n",
      "          [ 1.6948e-02,  1.1090e-02, -8.8353e-04],\n",
      "          [ 1.5427e-02,  4.3750e-03, -2.9968e-03]],\n",
      "\n",
      "         [[-1.1149e-02, -1.0652e-02,  2.4603e-03],\n",
      "          [-7.5945e-03, -4.1384e-03, -5.7504e-03],\n",
      "          [ 1.2022e-02,  2.6752e-02,  3.0088e-02]],\n",
      "\n",
      "         [[ 8.7909e-04, -5.2436e-03, -1.3757e-02],\n",
      "          [-1.4013e-02, -2.8504e-02, -2.0972e-02],\n",
      "          [-6.7422e-03, -3.2678e-02, -1.4873e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.9598e-04,  6.1998e-04, -1.5639e-02],\n",
      "          [-7.9836e-03, -2.3042e-02, -9.1302e-03],\n",
      "          [ 1.7236e-02,  1.9936e-02,  2.1407e-02]],\n",
      "\n",
      "         [[ 5.9531e-03, -6.2782e-03, -7.3307e-03],\n",
      "          [-1.0681e-02, -5.7663e-03,  6.9749e-03],\n",
      "          [-5.6668e-03,  8.0432e-03,  3.7970e-03]],\n",
      "\n",
      "         [[-8.9857e-03, -1.0258e-02,  4.5079e-03],\n",
      "          [ 1.3418e-02,  1.9291e-02,  2.4953e-02],\n",
      "          [ 3.4754e-02,  3.5535e-02,  3.5720e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-5.1716e-03,  1.2740e-02,  4.5228e-04],\n",
      "          [-4.6442e-02, -1.2378e-02,  8.2514e-03],\n",
      "          [-2.9679e-02, -2.8815e-02,  5.0659e-03]],\n",
      "\n",
      "         [[ 6.1238e-03,  1.3852e-02,  1.4681e-04],\n",
      "          [-3.1333e-02, -2.7401e-02, -1.5988e-02],\n",
      "          [-1.2962e-02, -4.3992e-03, -1.8203e-02]],\n",
      "\n",
      "         [[-3.8223e-03,  5.7154e-03, -4.8158e-04],\n",
      "          [-1.5050e-02, -1.4829e-02, -3.2036e-02],\n",
      "          [-1.7271e-02, -5.8238e-03, -1.9884e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.0192e-02, -8.3638e-03, -1.0518e-02],\n",
      "          [-6.5113e-03,  5.2503e-04,  3.7589e-03],\n",
      "          [ 8.5663e-03,  1.6411e-02,  1.3831e-02]],\n",
      "\n",
      "         [[ 4.4374e-03,  7.8820e-03, -8.1149e-03],\n",
      "          [-9.4414e-03, -6.0531e-03, -1.0696e-02],\n",
      "          [-1.5322e-02, -8.3003e-03, -1.3987e-02]],\n",
      "\n",
      "         [[ 3.1569e-02,  2.1738e-02,  9.1663e-03],\n",
      "          [ 2.4048e-02,  1.4914e-02,  2.6829e-03],\n",
      "          [-1.6763e-02, -3.7048e-02, -1.8150e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.9149e-03,  1.7638e-04,  1.2026e-02],\n",
      "          [-4.4268e-03, -3.1106e-03, -4.3402e-03],\n",
      "          [ 3.4857e-03,  1.6195e-03,  7.3129e-03]],\n",
      "\n",
      "         [[-8.5348e-03, -1.8333e-03,  3.7472e-03],\n",
      "          [ 7.1719e-03, -2.7567e-04, -1.2123e-03],\n",
      "          [-6.4137e-03,  9.8588e-03, -5.9642e-03]],\n",
      "\n",
      "         [[-1.5488e-03, -1.1074e-02, -1.1464e-03],\n",
      "          [-1.0213e-02, -1.1571e-02, -3.4662e-03],\n",
      "          [ 4.2347e-03,  7.7947e-03,  2.8771e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5750e-02,  1.3494e-02, -5.7186e-03],\n",
      "          [-1.0701e-02, -2.5485e-02, -2.4094e-02],\n",
      "          [-4.2056e-03, -2.0863e-02, -1.2637e-02]],\n",
      "\n",
      "         [[-1.3376e-02, -3.4152e-03, -5.7469e-04],\n",
      "          [ 8.2992e-03, -1.9168e-03, -7.6782e-03],\n",
      "          [-2.0174e-03, -2.8936e-03, -9.2910e-03]],\n",
      "\n",
      "         [[ 5.6882e-03,  1.6473e-03,  7.5047e-03],\n",
      "          [-2.0017e-03,  2.5732e-03,  9.8633e-03],\n",
      "          [ 9.0716e-03, -1.0057e-03, -4.2856e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5264e-02,  1.1714e-02,  1.1979e-02],\n",
      "          [ 4.2049e-02,  1.2400e-02, -2.0169e-03],\n",
      "          [-1.0338e-02, -5.1485e-03, -3.0402e-03]],\n",
      "\n",
      "         [[ 3.0440e-02,  9.7346e-03, -2.5335e-03],\n",
      "          [ 1.7957e-02,  1.5028e-02,  2.4801e-03],\n",
      "          [-4.3918e-03,  6.6649e-03,  6.5652e-03]],\n",
      "\n",
      "         [[-1.1265e-02, -1.6208e-02, -3.0098e-03],\n",
      "          [ 2.8935e-02, -7.0393e-03,  1.4753e-02],\n",
      "          [ 2.5754e-02, -1.3119e-02, -1.7128e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.6213e-02, -2.2137e-02, -1.3527e-04],\n",
      "          [-2.5990e-02, -2.5178e-02, -1.9146e-02],\n",
      "          [-9.1445e-03, -1.9102e-02, -4.8260e-03]],\n",
      "\n",
      "         [[ 3.4968e-02, -1.0314e-02, -6.6928e-03],\n",
      "          [-3.4642e-03, -5.9528e-03,  5.4940e-03],\n",
      "          [-2.3637e-02, -1.8366e-03,  4.3602e-03]],\n",
      "\n",
      "         [[ 6.1545e-03,  2.7458e-04,  9.1104e-03],\n",
      "          [-7.3913e-03, -2.4641e-02, -1.5486e-02],\n",
      "          [-1.0919e-02, -1.9878e-02, -2.6229e-02]]]], device='cuda:0'))\n",
      "('res2.0.0.bias', Parameter containing:\n",
      "tensor([ 6.4923e-08, -8.3491e-08,  1.4354e-07, -3.9921e-09, -2.8874e-08,\n",
      "         2.9235e-08,  8.9691e-08, -6.7475e-08, -2.5694e-08, -1.0757e-07,\n",
      "         1.2199e-08,  1.5850e-07,  1.3816e-07,  1.4256e-07, -2.2103e-07,\n",
      "         6.1809e-08,  3.6006e-08, -1.1270e-08, -9.2266e-08, -2.7802e-08,\n",
      "        -4.6363e-08,  4.0611e-08, -9.6841e-08, -6.4260e-08,  8.1971e-08,\n",
      "         3.4977e-08, -1.0468e-07,  3.2287e-08,  9.3227e-08, -1.0275e-07,\n",
      "        -3.5296e-08, -4.4350e-08, -8.1062e-08,  3.5187e-08,  5.9310e-08,\n",
      "        -2.1340e-07,  5.7729e-08,  7.0458e-08,  1.6068e-07, -1.0968e-07,\n",
      "        -5.3493e-08,  4.8763e-08, -4.3644e-08, -1.0703e-07,  6.1887e-08,\n",
      "        -1.9357e-07, -2.7481e-08,  5.8194e-09, -1.1494e-07, -1.2966e-07,\n",
      "         1.9389e-08, -1.2444e-07, -3.5901e-09, -3.1174e-08, -1.5958e-07,\n",
      "         1.7609e-08,  8.4774e-08, -1.1968e-08, -1.5454e-09, -9.0131e-08,\n",
      "         2.4730e-08,  1.0658e-07,  3.5885e-08, -3.7829e-08, -1.5121e-08,\n",
      "         2.3427e-08,  2.0949e-07, -1.5202e-08, -2.2326e-08,  1.2307e-07,\n",
      "        -6.4935e-08, -5.6954e-08, -1.6798e-07,  4.7282e-09,  2.2973e-08,\n",
      "        -3.3795e-08, -3.6127e-08, -1.8188e-08,  5.9490e-08,  4.0291e-09,\n",
      "         6.9608e-08, -3.7028e-08, -7.9059e-08, -1.5940e-08, -2.9586e-08,\n",
      "         3.2988e-08, -2.8244e-08,  1.0024e-07,  1.4403e-07,  6.5276e-08,\n",
      "        -6.3083e-08, -3.2835e-08,  7.4447e-09,  2.0411e-08,  4.9228e-08,\n",
      "         1.3604e-07,  2.2501e-08, -4.4996e-08,  9.2408e-08, -3.1487e-08,\n",
      "        -5.5032e-08, -2.7457e-08, -7.9651e-08, -4.6134e-08, -3.4080e-09,\n",
      "         2.7155e-08,  8.4125e-08, -3.5424e-08, -5.0475e-08,  4.4681e-08,\n",
      "        -8.8725e-10,  4.7597e-08, -1.0029e-07,  3.5367e-08,  8.4491e-09,\n",
      "        -7.1835e-09,  2.0584e-08,  1.0691e-07,  1.2633e-07,  4.9987e-09,\n",
      "         4.7765e-08, -3.6589e-08, -5.7732e-08, -2.6051e-08,  1.3270e-08,\n",
      "         1.5371e-07, -7.2417e-08,  4.3705e-08, -1.5412e-07, -2.8008e-08,\n",
      "         1.5505e-07,  1.4749e-07,  9.3852e-08,  4.5318e-08, -1.2664e-07,\n",
      "        -6.4684e-08,  5.2805e-08,  1.4650e-07, -1.7042e-07,  1.8125e-08,\n",
      "         1.3030e-08, -7.7584e-09, -3.8933e-08,  6.1246e-08, -9.5289e-08,\n",
      "         6.8506e-10,  8.5119e-08,  1.0322e-07, -5.4675e-08,  1.0917e-07,\n",
      "         2.6212e-09,  2.0557e-08, -4.2701e-08,  4.1131e-08, -8.9068e-09,\n",
      "        -3.9375e-08, -1.9362e-08, -3.7867e-08,  6.9712e-08,  2.2700e-08,\n",
      "         1.3855e-08,  7.9857e-08,  7.2715e-08,  9.9744e-08, -2.9268e-08,\n",
      "         4.6365e-08, -1.2406e-09,  4.2070e-08, -5.3073e-08,  1.0036e-07,\n",
      "        -3.8385e-08,  9.0844e-09,  3.9476e-08,  2.2455e-08,  6.8871e-09,\n",
      "         6.7561e-08,  1.1492e-08, -1.0530e-07,  6.3549e-09, -1.7484e-08,\n",
      "         5.1362e-09,  1.0319e-07,  1.4865e-07, -2.7734e-08, -8.1674e-08,\n",
      "         7.2432e-08,  2.0040e-07,  8.1225e-08, -1.0766e-07,  1.0660e-07,\n",
      "         1.4662e-08, -2.3774e-08, -2.3950e-07, -5.3704e-08, -9.2244e-08,\n",
      "         2.6342e-08, -1.0555e-08,  1.1776e-07,  1.8640e-08, -3.5232e-08,\n",
      "        -3.7616e-08,  8.9769e-08, -1.0971e-07,  3.5361e-08,  4.2709e-08,\n",
      "         1.5941e-08,  1.2015e-08, -7.5887e-08, -2.6906e-08,  8.8838e-08,\n",
      "        -3.5682e-08,  1.5893e-07,  9.3670e-09, -1.6226e-08, -1.6150e-08,\n",
      "         9.0058e-08, -2.4196e-08,  2.1029e-08,  1.9218e-08, -1.7122e-08,\n",
      "         2.6321e-08, -1.4699e-07, -3.6293e-08, -8.1975e-08, -1.0312e-07,\n",
      "        -3.8796e-08, -1.9434e-07, -1.3640e-08, -2.4127e-08, -2.0195e-08,\n",
      "         1.5567e-07,  5.8574e-09, -4.0594e-08,  8.7991e-08, -8.7202e-08,\n",
      "        -8.2753e-09,  1.7821e-08,  1.1929e-07,  1.2992e-07, -1.0227e-08,\n",
      "        -2.1223e-08, -8.5819e-08, -1.4776e-08, -1.0713e-08, -7.8497e-08,\n",
      "         9.3647e-09,  5.5044e-08,  1.4094e-07,  5.6920e-08, -1.8871e-08,\n",
      "        -1.0351e-07,  1.0194e-07,  3.2773e-08, -3.6891e-08,  1.2149e-08,\n",
      "         1.7981e-07,  1.0374e-07, -4.6783e-08, -1.2318e-07, -1.2301e-07,\n",
      "         3.1675e-08, -1.7309e-07, -4.4460e-08, -7.2340e-08, -2.7336e-08,\n",
      "         6.3689e-08, -7.4018e-08,  1.2962e-07,  1.8604e-08,  4.3145e-08,\n",
      "         6.7406e-08,  2.6600e-08,  1.7166e-08, -3.7469e-08,  7.3001e-08,\n",
      "        -1.6144e-07, -4.6053e-08, -5.9009e-08,  7.8655e-09,  5.1417e-08,\n",
      "        -7.2826e-08, -1.0525e-07,  1.3823e-07, -6.6413e-08,  1.8682e-08,\n",
      "         9.3636e-08,  3.5688e-08, -8.1108e-08, -2.6695e-07,  5.4743e-08,\n",
      "         1.0926e-08,  1.9593e-09, -1.1943e-07, -4.8856e-08, -1.6301e-07,\n",
      "        -6.5526e-08,  2.2611e-08, -1.2904e-10,  3.1179e-08, -2.0009e-08,\n",
      "        -6.6589e-08,  4.2361e-08,  2.2147e-08, -2.0098e-07, -7.9476e-08,\n",
      "        -6.4255e-08,  5.5359e-08,  8.9616e-09,  1.0420e-07, -1.9535e-07,\n",
      "        -5.6340e-08, -1.1989e-07,  1.5333e-08,  1.4322e-07,  1.3284e-08,\n",
      "         3.3479e-08, -1.1058e-07,  1.5301e-07, -9.6255e-09,  5.4556e-08,\n",
      "        -2.0359e-08, -1.0475e-07, -5.7971e-09,  9.8156e-09,  8.1682e-08,\n",
      "        -1.2255e-08,  1.4635e-08, -4.6269e-08,  4.1426e-09,  4.0081e-08,\n",
      "         1.3960e-07,  1.3987e-08,  2.3598e-08, -4.5480e-08, -1.4490e-07,\n",
      "         4.8428e-08, -6.3747e-08, -1.6407e-07, -1.0846e-07, -5.4887e-08,\n",
      "        -3.4581e-08,  1.3413e-07,  1.3589e-08,  3.9561e-08,  9.6133e-08,\n",
      "         3.6113e-08,  6.9075e-08, -2.9750e-09,  2.5152e-07, -4.6850e-08,\n",
      "        -4.5393e-08,  8.1259e-08,  5.4048e-08, -9.0192e-08,  1.0523e-08,\n",
      "        -2.5066e-08, -3.8855e-08,  2.1838e-08, -1.9864e-09, -8.2412e-08,\n",
      "         2.1469e-08, -1.1199e-07,  6.8662e-08,  8.6613e-08, -1.5240e-07,\n",
      "        -1.2459e-08,  2.2757e-08,  1.9627e-08, -6.9159e-08, -5.0277e-08,\n",
      "        -1.2988e-07,  5.3278e-08, -5.7078e-08,  2.1075e-08,  1.4066e-08,\n",
      "        -2.7934e-08,  9.8836e-08,  4.7736e-08,  2.1552e-08, -3.9501e-09,\n",
      "         9.1133e-08,  1.9040e-07,  1.9326e-08,  3.4010e-08, -5.5538e-08,\n",
      "        -7.0346e-08,  2.4751e-08, -7.0413e-08,  4.4769e-08,  4.1861e-08,\n",
      "        -4.8476e-08,  5.6845e-08, -7.6875e-08,  4.0811e-08,  2.6493e-08,\n",
      "        -1.1914e-07,  8.2443e-08, -5.9331e-08, -1.0854e-07,  1.0664e-07,\n",
      "         9.7552e-09, -4.2595e-08,  4.4710e-08, -7.2551e-08,  3.6159e-07,\n",
      "         8.1220e-08, -5.5561e-08,  4.5277e-08, -1.8862e-07,  7.0549e-10,\n",
      "         1.0128e-07,  8.0180e-08, -3.7805e-08,  1.0620e-07, -8.2225e-08,\n",
      "         3.6396e-08,  4.2731e-08, -5.9263e-08, -8.0939e-08, -6.5684e-08,\n",
      "         5.9953e-08, -7.9979e-08, -1.6802e-08,  9.6996e-08, -1.4728e-07,\n",
      "         2.9230e-08, -2.9560e-07,  1.2530e-08,  4.2747e-08, -1.0175e-07,\n",
      "        -7.0929e-08, -1.0896e-07,  6.6022e-08, -1.2634e-08, -4.7013e-08,\n",
      "         2.3611e-09, -1.5803e-08,  9.3363e-08, -2.3633e-07, -1.6712e-07,\n",
      "         2.5041e-07,  1.4750e-07,  5.6569e-08, -1.6434e-07,  1.7221e-08,\n",
      "        -2.4433e-08,  1.3898e-07,  1.0278e-07, -1.4517e-08,  8.9252e-08,\n",
      "         8.0171e-09, -1.2065e-07, -3.4383e-08,  2.0573e-08, -4.2634e-08,\n",
      "        -1.1077e-07, -5.2850e-08, -6.9801e-08, -1.1843e-07, -6.9709e-08,\n",
      "         3.1661e-08, -3.9693e-08,  5.5406e-08,  1.1346e-08,  6.8104e-08,\n",
      "        -1.1170e-07, -4.7065e-08, -1.8288e-07, -4.2418e-08, -8.7754e-08,\n",
      "         5.3987e-08,  1.4909e-08, -9.0986e-08, -4.0394e-08,  4.7362e-08,\n",
      "        -1.5050e-08, -3.0529e-07,  9.7376e-08,  6.8941e-08,  1.6244e-07,\n",
      "         5.5269e-08, -4.5697e-08, -4.9440e-08, -9.1215e-08, -5.6687e-09,\n",
      "        -3.7733e-08, -2.5389e-08,  6.6381e-08, -1.5090e-07, -4.9844e-08,\n",
      "        -7.2752e-08, -7.0177e-08,  2.1646e-08,  2.4218e-08,  1.6426e-08,\n",
      "        -2.1249e-08, -4.3806e-08,  2.9196e-08, -4.0555e-08, -8.0983e-08,\n",
      "         2.5776e-08, -3.5896e-08,  1.0397e-07, -1.4760e-07, -4.0061e-08,\n",
      "         1.6698e-08, -5.3291e-08, -6.9137e-08, -1.7659e-08, -1.4383e-08,\n",
      "        -2.6298e-08, -7.8359e-08], device='cuda:0'))\n",
      "('res2.0.1.weight', Parameter containing:\n",
      "tensor([0.3204, 0.4178, 0.4974, 0.3299, 0.4231, 0.5421, 0.5408, 0.4386, 0.5441,\n",
      "        0.4506, 0.2702, 0.3609, 0.3512, 0.4412, 0.5079, 0.5117, 0.5658, 0.6319,\n",
      "        0.4249, 0.4210, 0.5149, 0.4096, 0.5054, 0.2638, 0.3889, 0.2823, 0.4751,\n",
      "        0.4648, 0.3675, 0.4617, 0.4097, 0.4937, 0.4448, 0.4220, 0.4894, 0.5709,\n",
      "        0.3827, 0.4254, 0.3741, 0.4193, 0.4743, 0.5495, 0.4079, 0.4077, 0.4781,\n",
      "        0.6030, 0.3375, 0.2756, 0.5259, 0.4568, 0.4487, 0.4180, 0.4140, 0.4002,\n",
      "        0.4045, 0.4018, 0.5062, 0.3452, 0.3755, 0.4631, 0.4592, 0.3174, 0.3657,\n",
      "        0.4657, 0.6075, 0.4601, 0.3257, 0.5264, 0.4106, 0.3235, 0.4770, 0.2965,\n",
      "        0.5251, 0.3296, 0.3851, 0.3841, 0.3936, 0.3822, 0.3683, 0.2792, 0.5104,\n",
      "        0.4782, 0.5544, 0.3699, 0.4822, 0.4701, 0.4156, 0.3637, 0.4280, 0.4673,\n",
      "        0.4286, 0.4033, 0.4096, 0.5242, 0.4055, 0.3176, 0.3700, 0.3154, 0.3652,\n",
      "        0.2986, 0.6150, 0.3364, 0.4901, 0.4399, 0.5223, 0.4225, 0.5919, 0.4075,\n",
      "        0.3554, 0.3678, 0.5304, 0.2440, 0.3998, 0.2874, 0.2797, 0.3749, 0.4155,\n",
      "        0.3982, 0.5699, 0.4318, 0.4320, 0.3977, 0.4113, 0.5306, 0.4759, 0.4505,\n",
      "        0.5303, 0.4468, 0.4975, 0.4606, 0.4560, 0.4923, 0.4894, 0.4184, 0.3390,\n",
      "        0.4538, 0.3826, 0.4073, 0.4297, 0.4247, 0.4348, 0.3512, 0.5471, 0.4803,\n",
      "        0.6582, 0.4318, 0.4912, 0.4163, 0.2744, 0.3328, 0.4570, 0.3952, 0.3376,\n",
      "        0.4999, 0.4786, 0.4956, 0.3847, 0.4026, 0.5230, 0.3257, 0.4226, 0.4668,\n",
      "        0.4727, 0.3784, 0.4107, 0.3732, 0.5050, 0.4942, 0.4792, 0.5821, 0.3503,\n",
      "        0.3076, 0.5105, 0.4750, 0.4298, 0.3879, 0.4676, 0.3498, 0.2554, 0.2655,\n",
      "        0.4676, 0.4630, 0.5365, 0.3332, 0.3435, 0.3862, 0.4412, 0.4880, 0.4348,\n",
      "        0.5632, 0.4605, 0.4323, 0.4649, 0.2915, 0.3906, 0.4292, 0.4223, 0.4184,\n",
      "        0.3133, 0.5198, 0.3749, 0.3310, 0.3864, 0.2925, 0.3961, 0.4656, 0.5115,\n",
      "        0.4677, 0.4147, 0.5985, 0.4488, 0.4492, 0.4685, 0.4158, 0.5480, 0.3101,\n",
      "        0.3534, 0.3177, 0.3671, 0.3962, 0.4749, 0.3813, 0.4720, 0.4902, 0.5325,\n",
      "        0.4306, 0.4975, 0.2691, 0.3138, 0.3783, 0.4187, 0.4305, 0.3551, 0.2961,\n",
      "        0.3845, 0.3301, 0.4703, 0.4754, 0.4337, 0.4438, 0.3649, 0.3371, 0.4262,\n",
      "        0.3558, 0.4459, 0.3464, 0.3324, 0.3883, 0.4035, 0.3982, 0.4294, 0.5722,\n",
      "        0.4545, 0.5687, 0.5459, 0.5111, 0.4767, 0.4038, 0.4331, 0.3553, 0.3673,\n",
      "        0.6591, 0.3992, 0.4072, 0.5166, 0.3824, 0.5755, 0.4107, 0.4077, 0.4587,\n",
      "        0.4943, 0.4224, 0.3253, 0.4494, 0.4728, 0.5682, 0.5623, 0.4195, 0.4399,\n",
      "        0.5729, 0.5257, 0.5422, 0.3843, 0.5502, 0.3359, 0.5140, 0.3742, 0.4785,\n",
      "        0.4231, 0.5341, 0.4260, 0.3514, 0.5133, 0.4539, 0.5171, 0.4416, 0.4535,\n",
      "        0.3931, 0.4624, 0.3814, 0.4961, 0.5193, 0.3902, 0.4857, 0.4911, 0.5190,\n",
      "        0.4770, 0.5056, 0.4755, 0.5213, 0.3102, 0.3949, 0.4974, 0.4273, 0.4731,\n",
      "        0.3041, 0.5172, 0.4311, 0.4013, 0.4775, 0.5147, 0.3814, 0.4535, 0.3777,\n",
      "        0.6070, 0.4016, 0.4140, 0.3379, 0.5471, 0.3869, 0.3789, 0.3246, 0.4955,\n",
      "        0.4358, 0.4756, 0.3783, 0.4600, 0.4990, 0.4218, 0.3683, 0.3286, 0.4241,\n",
      "        0.4538, 0.2059, 0.4021, 0.4889, 0.3187, 0.3949, 0.4053, 0.5502, 0.4597,\n",
      "        0.4231, 0.4142, 0.3481, 0.4126, 0.2564, 0.4344, 0.2797, 0.4750, 0.4533,\n",
      "        0.5083, 0.4152, 0.3626, 0.4746, 0.5144, 0.3713, 0.5378, 0.4059, 0.3412,\n",
      "        0.3493, 0.5683, 0.5009, 0.3980, 0.4512, 0.5818, 0.4417, 0.4952, 0.4743,\n",
      "        0.2904, 0.4931, 0.4137, 0.5489, 0.5128, 0.4922, 0.4658, 0.5491, 0.4815,\n",
      "        0.3928, 0.5307, 0.4630, 0.5514, 0.3450, 0.4237, 0.3804, 0.4284, 0.5566,\n",
      "        0.3704, 0.3607, 0.5475, 0.2930, 0.5580, 0.4227, 0.4133, 0.3321, 0.4781,\n",
      "        0.5231, 0.4188, 0.2224, 0.4513, 0.4409, 0.3623, 0.3973, 0.4155, 0.5339,\n",
      "        0.4994, 0.4657, 0.2310, 0.4711, 0.4678, 0.4594, 0.4548, 0.5519, 0.3809,\n",
      "        0.2958, 0.3279, 0.3787, 0.5400, 0.5827, 0.4415, 0.5607, 0.4310, 0.5958,\n",
      "        0.4250, 0.4341, 0.4916, 0.4795, 0.3735, 0.3493, 0.5869, 0.5401, 0.4770,\n",
      "        0.3744, 0.4011, 0.3657, 0.4663, 0.4087, 0.4634, 0.4141, 0.5261, 0.3774,\n",
      "        0.3615, 0.5676, 0.3515, 0.3343, 0.4067, 0.4066, 0.4159, 0.4780, 0.4084,\n",
      "        0.4464, 0.3798, 0.3656, 0.4100, 0.3736, 0.5121, 0.3972, 0.5356, 0.4825,\n",
      "        0.3445, 0.4210, 0.4655, 0.3975, 0.5331, 0.4135, 0.3795, 0.5204, 0.5196,\n",
      "        0.5333, 0.3571, 0.2753, 0.3733, 0.3353, 0.4363, 0.5754, 0.3769, 0.3504,\n",
      "        0.4348, 0.5196, 0.4818, 0.4659, 0.5275, 0.2797, 0.4014, 0.4905, 0.3027,\n",
      "        0.4278, 0.3767, 0.5476, 0.3484, 0.4427, 0.5602, 0.4079, 0.4932, 0.4718,\n",
      "        0.4052, 0.4767, 0.4119, 0.4037, 0.4007, 0.5690, 0.3287, 0.4707],\n",
      "       device='cuda:0'))\n",
      "('res2.0.1.bias', Parameter containing:\n",
      "tensor([-0.3040, -0.3018, -0.2702, -0.2485, -0.3248, -0.3218, -0.3319, -0.3225,\n",
      "        -0.3514, -0.2600, -0.2206, -0.2986, -0.2396, -0.3057, -0.3587, -0.3432,\n",
      "        -0.2172, -0.2806, -0.2883, -0.3007, -0.3582, -0.2804, -0.3457, -0.1639,\n",
      "        -0.2495, -0.2102, -0.3367, -0.3012, -0.2589, -0.2628, -0.3208, -0.3009,\n",
      "        -0.2885, -0.2829, -0.3673, -0.3865, -0.2604, -0.3175, -0.2689, -0.3121,\n",
      "        -0.3536, -0.3344, -0.2887, -0.2689, -0.2994, -0.3956, -0.2561, -0.2271,\n",
      "        -0.3010, -0.2889, -0.2753, -0.2716, -0.2224, -0.3221, -0.2831, -0.2741,\n",
      "        -0.2944, -0.2772, -0.3236, -0.3141, -0.2907, -0.2368, -0.2508, -0.3257,\n",
      "        -0.3007, -0.3045, -0.2379, -0.3161, -0.3280, -0.2725, -0.3587, -0.2252,\n",
      "        -0.3100, -0.2819, -0.2795, -0.2923, -0.2974, -0.2486, -0.2706, -0.1921,\n",
      "        -0.3350, -0.3332, -0.3590, -0.2784, -0.2808, -0.3569, -0.3386, -0.2491,\n",
      "        -0.2611, -0.3026, -0.3588, -0.2674, -0.2903, -0.3142, -0.2670, -0.2127,\n",
      "        -0.2804, -0.2333, -0.2354, -0.2225, -0.4172, -0.2649, -0.3546, -0.2967,\n",
      "        -0.3119, -0.2961, -0.3355, -0.2801, -0.3102, -0.2734, -0.3655, -0.2063,\n",
      "        -0.2508, -0.2254, -0.2239, -0.2556, -0.2985, -0.2593, -0.3007, -0.2777,\n",
      "        -0.2731, -0.2712, -0.2883, -0.3225, -0.2673, -0.3094, -0.3113, -0.2894,\n",
      "        -0.3679, -0.2964, -0.3327, -0.3267, -0.3015, -0.3087, -0.2442, -0.3222,\n",
      "        -0.2687, -0.2778, -0.3330, -0.2917, -0.3291, -0.2596, -0.3096, -0.3367,\n",
      "        -0.3571, -0.3471, -0.4078, -0.2702, -0.2190, -0.2569, -0.2436, -0.2824,\n",
      "        -0.2959, -0.3271, -0.2939, -0.2978, -0.2942, -0.3000, -0.3856, -0.2632,\n",
      "        -0.3155, -0.3118, -0.3134, -0.2218, -0.3399, -0.3089, -0.3653, -0.3499,\n",
      "        -0.3297, -0.2769, -0.2606, -0.2021, -0.3161, -0.3198, -0.3186, -0.2908,\n",
      "        -0.3037, -0.2653, -0.1844, -0.1980, -0.3008, -0.2554, -0.3993, -0.2130,\n",
      "        -0.2662, -0.2608, -0.2998, -0.3368, -0.3006, -0.3443, -0.3138, -0.2835,\n",
      "        -0.3332, -0.2049, -0.2572, -0.2912, -0.2731, -0.2874, -0.2357, -0.3372,\n",
      "        -0.2722, -0.2802, -0.3232, -0.1985, -0.2920, -0.3204, -0.2798, -0.2984,\n",
      "        -0.2905, -0.3339, -0.3105, -0.2877, -0.3141, -0.2668, -0.3490, -0.2425,\n",
      "        -0.2350, -0.2453, -0.2866, -0.2868, -0.3476, -0.2864, -0.3047, -0.3098,\n",
      "        -0.2808, -0.2899, -0.2888, -0.2309, -0.1983, -0.2668, -0.2889, -0.2974,\n",
      "        -0.2545, -0.2302, -0.2613, -0.2438, -0.3085, -0.3393, -0.2749, -0.2923,\n",
      "        -0.2764, -0.2803, -0.2769, -0.2531, -0.3124, -0.2394, -0.2555, -0.2711,\n",
      "        -0.3076, -0.3064, -0.3122, -0.3417, -0.2866, -0.3074, -0.3276, -0.4103,\n",
      "        -0.3219, -0.2887, -0.2974, -0.2695, -0.2878, -0.4265, -0.2674, -0.2900,\n",
      "        -0.3603, -0.2727, -0.3423, -0.2775, -0.2859, -0.3076, -0.3840, -0.2844,\n",
      "        -0.2434, -0.3471, -0.3140, -0.3046, -0.3382, -0.3017, -0.2882, -0.3519,\n",
      "        -0.2839, -0.3022, -0.2591, -0.2897, -0.2836, -0.3509, -0.2773, -0.3300,\n",
      "        -0.2835, -0.3522, -0.2765, -0.2846, -0.3103, -0.3198, -0.3212, -0.3262,\n",
      "        -0.3308, -0.2952, -0.2921, -0.3159, -0.3165, -0.3382, -0.2528, -0.2406,\n",
      "        -0.3599, -0.3478, -0.3502, -0.2795, -0.2655, -0.2530, -0.2222, -0.2921,\n",
      "        -0.3834, -0.3058, -0.3298, -0.2381, -0.3238, -0.2717, -0.3085, -0.3035,\n",
      "        -0.3849, -0.2936, -0.3236, -0.2576, -0.3937, -0.2794, -0.2949, -0.2138,\n",
      "        -0.2738, -0.2958, -0.2752, -0.2359, -0.3240, -0.2906, -0.3308, -0.2691,\n",
      "        -0.3489, -0.3301, -0.2854, -0.2657, -0.2578, -0.2884, -0.3219, -0.1952,\n",
      "        -0.2786, -0.2975, -0.2585, -0.2585, -0.2929, -0.3150, -0.2899, -0.3088,\n",
      "        -0.3078, -0.2564, -0.3179, -0.2058, -0.3157, -0.2508, -0.3780, -0.3253,\n",
      "        -0.3193, -0.2806, -0.2725, -0.2847, -0.3440, -0.2742, -0.3592, -0.2512,\n",
      "        -0.2493, -0.2687, -0.3214, -0.3799, -0.2759, -0.3412, -0.3314, -0.2926,\n",
      "        -0.2926, -0.2904, -0.2521, -0.3450, -0.2803, -0.3029, -0.3380, -0.3436,\n",
      "        -0.3481, -0.3423, -0.3273, -0.2957, -0.3337, -0.3769, -0.3616, -0.3156,\n",
      "        -0.2943, -0.2918, -0.2953, -0.2900, -0.2182, -0.2719, -0.3552, -0.2354,\n",
      "        -0.2966, -0.3318, -0.2894, -0.2169, -0.3122, -0.3440, -0.2928, -0.1719,\n",
      "        -0.3339, -0.3652, -0.2850, -0.2711, -0.2984, -0.3706, -0.2923, -0.2896,\n",
      "        -0.2050, -0.2878, -0.3162, -0.2808, -0.3377, -0.2568, -0.2523, -0.2732,\n",
      "        -0.2291, -0.2672, -0.3232, -0.3138, -0.2989, -0.3450, -0.2821, -0.3651,\n",
      "        -0.3458, -0.2980, -0.3181, -0.3131, -0.2598, -0.2709, -0.3040, -0.3451,\n",
      "        -0.3459, -0.2657, -0.3106, -0.3012, -0.3002, -0.2967, -0.3279, -0.3135,\n",
      "        -0.3128, -0.2286, -0.2448, -0.3648, -0.2389, -0.2901, -0.3256, -0.2770,\n",
      "        -0.3132, -0.2981, -0.2990, -0.3056, -0.2457, -0.2834, -0.2499, -0.2620,\n",
      "        -0.3827, -0.2811, -0.3676, -0.3410, -0.2874, -0.3012, -0.3033, -0.2963,\n",
      "        -0.3873, -0.2834, -0.2527, -0.3410, -0.2627, -0.3162, -0.2780, -0.2442,\n",
      "        -0.2670, -0.2779, -0.2923, -0.2810, -0.2439, -0.2454, -0.3052, -0.3200,\n",
      "        -0.3249, -0.3464, -0.3716, -0.2541, -0.3038, -0.3313, -0.1873, -0.3080,\n",
      "        -0.2782, -0.2964, -0.2745, -0.3182, -0.3276, -0.2985, -0.2898, -0.3231,\n",
      "        -0.3295, -0.3151, -0.3091, -0.2636, -0.2933, -0.3416, -0.2302, -0.3116],\n",
      "       device='cuda:0'))\n",
      "('res2.1.0.weight', Parameter containing:\n",
      "tensor([[[[-2.4057e-02, -2.0789e-02, -1.7930e-02],\n",
      "          [-2.9466e-02, -2.7426e-02, -1.9017e-02],\n",
      "          [-1.4290e-02, -1.8147e-02, -1.5430e-02]],\n",
      "\n",
      "         [[ 5.7023e-03,  1.4463e-02,  1.5782e-02],\n",
      "          [-1.1504e-02,  8.4534e-03,  5.5925e-03],\n",
      "          [-2.9605e-02, -2.0093e-02, -1.4158e-02]],\n",
      "\n",
      "         [[ 1.4113e-02,  1.7347e-02,  5.6488e-03],\n",
      "          [ 6.8934e-03,  1.6863e-02, -1.0537e-03],\n",
      "          [-2.3402e-02, -1.3192e-02, -1.5597e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.5260e-03, -1.6677e-02, -1.1518e-02],\n",
      "          [-2.0029e-02, -2.0861e-02, -9.0501e-03],\n",
      "          [-1.3608e-02, -1.3671e-02,  4.2035e-03]],\n",
      "\n",
      "         [[-5.0367e-03, -6.1155e-03,  1.1889e-03],\n",
      "          [-6.9943e-03, -2.0391e-04, -1.3099e-03],\n",
      "          [-4.4578e-03,  1.1029e-03, -3.4696e-03]],\n",
      "\n",
      "         [[-1.2270e-02, -8.6024e-03, -1.7352e-02],\n",
      "          [-5.5763e-03,  6.5091e-03,  2.1796e-03],\n",
      "          [-2.1594e-04,  7.8200e-03,  6.8536e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 3.6213e-03,  8.8240e-03,  1.1722e-02],\n",
      "          [ 3.4791e-03,  2.4926e-03, -6.4426e-04],\n",
      "          [-1.5214e-03, -1.4779e-02, -1.0352e-02]],\n",
      "\n",
      "         [[-1.9405e-03, -3.4642e-03, -8.2929e-03],\n",
      "          [-1.2742e-02, -8.8130e-03, -1.8626e-02],\n",
      "          [ 1.6675e-04, -8.0638e-03, -1.9853e-02]],\n",
      "\n",
      "         [[ 2.7800e-02,  1.1566e-02,  1.6432e-02],\n",
      "          [ 1.7080e-02,  1.5255e-02,  5.2798e-03],\n",
      "          [-9.5031e-03,  8.6293e-03, -2.4678e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 8.7545e-03, -7.5515e-03, -1.8843e-03],\n",
      "          [-2.1231e-02, -1.0103e-02, -1.2157e-02],\n",
      "          [-2.0103e-02, -1.1065e-02, -1.0072e-02]],\n",
      "\n",
      "         [[ 1.0948e-02,  4.3113e-03,  1.1997e-02],\n",
      "          [-9.6768e-03, -1.0989e-02, -4.4525e-03],\n",
      "          [-3.3513e-04, -2.9292e-03, -3.2129e-04]],\n",
      "\n",
      "         [[-6.7792e-03, -1.4820e-02, -3.5390e-04],\n",
      "          [ 1.3204e-02, -1.9166e-03, -1.2856e-02],\n",
      "          [ 1.6397e-02,  2.8121e-04, -1.4997e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.0371e-04, -5.5169e-03, -1.1373e-02],\n",
      "          [ 1.3636e-03,  1.4284e-03, -7.6857e-03],\n",
      "          [ 4.5595e-05,  2.2726e-04, -4.4209e-03]],\n",
      "\n",
      "         [[-3.2283e-02, -3.3052e-02, -2.0635e-02],\n",
      "          [-2.3409e-02, -2.8425e-02, -1.9872e-02],\n",
      "          [ 2.8385e-03,  6.8192e-03,  6.0929e-03]],\n",
      "\n",
      "         [[-7.8902e-03, -1.1125e-02, -5.4681e-03],\n",
      "          [-6.3488e-03, -1.5927e-02, -8.1112e-03],\n",
      "          [-3.3479e-03, -9.6275e-03, -6.6298e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.2981e-03,  6.8701e-03,  1.0812e-02],\n",
      "          [-3.8885e-02, -2.9436e-02, -2.0746e-02],\n",
      "          [-3.1726e-02, -2.7316e-02, -1.8887e-02]],\n",
      "\n",
      "         [[ 9.6957e-03,  7.1954e-03,  2.8534e-03],\n",
      "          [ 1.3027e-02,  7.4670e-03,  2.7450e-03],\n",
      "          [ 1.6456e-02,  1.0319e-02,  4.1868e-03]],\n",
      "\n",
      "         [[ 1.8160e-03,  1.4785e-02,  1.4848e-02],\n",
      "          [-5.4375e-03,  1.7822e-03,  1.6583e-02],\n",
      "          [-7.3927e-03, -6.7796e-03,  4.9309e-03]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.5786e-03, -9.6318e-03, -1.0193e-02],\n",
      "          [-1.8314e-03, -1.3648e-02, -1.9287e-02],\n",
      "          [ 8.8930e-03, -8.6950e-03, -1.3043e-02]],\n",
      "\n",
      "         [[ 2.1972e-02,  3.8020e-02,  3.8968e-02],\n",
      "          [ 1.4560e-02,  2.8081e-02,  4.1630e-02],\n",
      "          [ 2.3571e-02,  1.2002e-02,  2.5809e-02]],\n",
      "\n",
      "         [[ 1.5042e-02,  2.9613e-03,  1.6966e-02],\n",
      "          [ 1.9638e-03, -7.8169e-03, -5.9737e-05],\n",
      "          [-6.0536e-03, -9.9788e-03,  3.9525e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-9.1194e-03,  3.6791e-03,  8.5933e-03],\n",
      "          [-1.7138e-02,  1.1874e-02,  2.3825e-02],\n",
      "          [-4.7488e-03,  4.1994e-03,  1.5691e-02]],\n",
      "\n",
      "         [[ 5.0020e-03,  6.7511e-03,  1.0872e-02],\n",
      "          [-7.7117e-03, -1.2156e-02, -9.4664e-03],\n",
      "          [-7.3374e-03, -4.7545e-03, -2.5272e-03]],\n",
      "\n",
      "         [[-1.0017e-02,  3.5285e-03,  9.6181e-04],\n",
      "          [ 7.6249e-03,  1.8579e-02,  1.4270e-02],\n",
      "          [ 7.8328e-03,  2.7995e-02,  3.3726e-02]]],\n",
      "\n",
      "\n",
      "        [[[-3.2528e-03, -6.8687e-03, -7.8309e-03],\n",
      "          [-3.0660e-03, -1.0811e-03,  3.1363e-03],\n",
      "          [ 2.1635e-03,  2.4456e-03,  3.8137e-03]],\n",
      "\n",
      "         [[-3.7026e-03, -5.0084e-04, -1.2947e-03],\n",
      "          [ 4.3892e-03,  2.0737e-02,  1.6828e-02],\n",
      "          [-4.2637e-03, -2.8050e-03,  9.0224e-04]],\n",
      "\n",
      "         [[-3.7191e-03, -1.2404e-02, -4.7504e-03],\n",
      "          [ 1.3928e-03,  1.5225e-03,  4.0837e-03],\n",
      "          [ 1.3033e-03, -1.7403e-02, -4.6193e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4246e-02, -2.3797e-03, -6.2182e-03],\n",
      "          [-1.6565e-02, -1.4100e-02, -4.7774e-03],\n",
      "          [ 1.5267e-02,  3.0146e-02,  1.7860e-02]],\n",
      "\n",
      "         [[-5.7479e-03, -7.2338e-04, -5.4678e-03],\n",
      "          [-8.7029e-03, -3.0218e-03,  3.6453e-03],\n",
      "          [ 1.4176e-03,  8.0990e-03,  1.1341e-02]],\n",
      "\n",
      "         [[ 9.1977e-03,  4.4122e-04, -9.9309e-03],\n",
      "          [-1.4594e-02,  1.0776e-03,  1.6725e-03],\n",
      "          [-1.5525e-02, -8.0434e-03, -8.4420e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.0187e-03,  9.6941e-04, -2.9572e-03],\n",
      "          [ 9.9373e-03,  9.4804e-03,  6.8575e-03],\n",
      "          [ 1.0348e-03, -7.3568e-04,  3.2619e-04]],\n",
      "\n",
      "         [[ 1.6846e-02,  7.7462e-03,  1.9973e-03],\n",
      "          [-3.7913e-03, -5.1463e-03, -8.7610e-03],\n",
      "          [-1.3113e-02, -1.4233e-03, -1.0594e-03]],\n",
      "\n",
      "         [[ 1.1342e-02,  6.2187e-03,  3.3544e-03],\n",
      "          [-1.7322e-02, -1.6994e-02, -8.2636e-03],\n",
      "          [-2.9291e-02, -3.2492e-02, -1.9357e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.5201e-02,  9.1121e-03,  1.3273e-02],\n",
      "          [ 2.9235e-02,  2.2004e-02,  3.9897e-02],\n",
      "          [ 9.1264e-03,  7.8921e-03,  2.2450e-02]],\n",
      "\n",
      "         [[-2.3760e-03,  7.8475e-03,  1.0977e-02],\n",
      "          [-7.1661e-03, -2.4792e-03,  1.0594e-02],\n",
      "          [-1.1491e-02, -1.1617e-02, -5.1040e-03]],\n",
      "\n",
      "         [[-6.2952e-03, -1.4130e-02, -1.3919e-02],\n",
      "          [-1.1549e-03,  4.6019e-03, -8.6548e-03],\n",
      "          [-6.4789e-03, -3.7835e-03, -4.0115e-03]]]], device='cuda:0'))\n",
      "('res2.1.0.bias', Parameter containing:\n",
      "tensor([-7.6325e-08, -1.9074e-08, -1.3636e-08,  2.0148e-08, -6.4052e-08,\n",
      "        -4.9429e-08,  2.6707e-07, -3.7156e-08, -9.3608e-08, -4.2443e-08,\n",
      "        -7.9334e-08,  3.6447e-08, -3.0978e-08,  7.8013e-10, -4.0259e-08,\n",
      "        -2.9810e-08,  7.7228e-08, -9.5806e-08, -5.7321e-08, -3.4999e-08,\n",
      "         2.7592e-07,  2.1545e-07, -4.2414e-08, -5.9912e-08,  1.3031e-08,\n",
      "        -9.4915e-08,  1.5534e-07, -1.6559e-08, -3.0145e-09, -9.3079e-09,\n",
      "        -1.3313e-07,  1.5932e-07,  1.9724e-08, -2.7978e-07,  1.8033e-09,\n",
      "         1.1417e-07, -6.9906e-09, -2.4704e-08, -6.1679e-08,  8.6081e-08,\n",
      "        -2.4369e-08,  1.1532e-07, -2.4675e-08, -2.4117e-08, -5.3469e-08,\n",
      "         1.7526e-07,  5.1255e-08,  2.4683e-08, -5.4771e-08, -7.5243e-08,\n",
      "         9.5622e-09,  6.1803e-08, -4.0250e-07, -9.8578e-08,  3.4948e-07,\n",
      "         5.2106e-08, -6.1548e-08, -4.7461e-08,  1.4597e-08,  2.6793e-08,\n",
      "         6.2681e-08, -7.4111e-08, -4.6351e-08,  6.1129e-08, -2.6202e-08,\n",
      "         7.0528e-09,  5.8200e-07,  1.3979e-07,  7.3370e-08,  4.8988e-08,\n",
      "         1.5338e-07, -1.1524e-09, -7.4888e-08, -9.4173e-09, -9.0257e-08,\n",
      "        -2.5400e-08, -3.2464e-08,  1.7675e-08,  1.9145e-08,  6.0486e-08,\n",
      "        -9.0110e-08,  3.9385e-08,  9.2362e-08,  3.3502e-08,  4.9686e-08,\n",
      "         4.3079e-08,  1.7497e-08,  8.7659e-08,  6.7248e-08,  4.3437e-08,\n",
      "         1.3340e-09,  1.4318e-08,  7.1653e-08, -3.0074e-07,  1.7155e-08,\n",
      "         1.3168e-08,  1.2410e-08, -2.9175e-08,  1.5362e-07, -6.1938e-08,\n",
      "         7.8338e-08,  1.8236e-08,  7.3778e-08,  4.5852e-08,  3.1656e-08,\n",
      "         3.4040e-08,  4.1727e-08,  1.5145e-08, -9.3759e-08,  2.0448e-08,\n",
      "        -1.0054e-07,  1.1177e-08, -1.4244e-07, -8.4782e-08, -8.8128e-08,\n",
      "        -2.6825e-08,  3.2319e-08,  3.5327e-08, -1.0228e-07,  6.1318e-08,\n",
      "        -2.1191e-07,  2.2412e-08,  7.3986e-08,  4.6252e-08,  1.4440e-08,\n",
      "        -2.5950e-08, -4.3956e-08,  1.3123e-07,  6.1717e-08,  2.9324e-08,\n",
      "        -1.3612e-07, -3.9875e-08, -7.2582e-09,  1.3831e-07, -3.0006e-09,\n",
      "         1.1513e-07,  1.4949e-08, -8.1655e-08,  5.3104e-08, -6.1272e-08,\n",
      "         3.4329e-08,  5.3160e-09,  1.6652e-08, -2.6434e-08,  1.3630e-07,\n",
      "         6.6726e-08,  4.1482e-08, -6.8067e-08,  1.1253e-08,  3.6722e-08,\n",
      "         6.2767e-08,  3.3451e-08,  6.5239e-09,  4.8130e-08, -1.3671e-07,\n",
      "         9.3816e-08, -7.3461e-08, -3.8168e-08, -9.6951e-09, -1.9400e-08,\n",
      "         6.2746e-08, -4.8237e-08,  2.3783e-08, -1.5280e-08, -1.7050e-07,\n",
      "         2.2816e-08, -7.0930e-08, -2.2833e-07, -5.3657e-08, -3.1117e-08,\n",
      "        -4.2754e-09,  4.0304e-09,  9.9047e-08, -2.4897e-09, -9.0082e-08,\n",
      "        -4.3793e-07,  9.3322e-09,  7.2626e-08,  6.4926e-09,  2.2864e-07,\n",
      "         6.4179e-08,  2.0125e-07, -1.9872e-08, -7.0181e-08,  1.2871e-08,\n",
      "         2.1475e-08, -7.5211e-08, -1.1095e-07,  8.5410e-08, -7.6571e-08,\n",
      "        -2.0905e-07,  1.6056e-07,  6.1712e-08,  1.9527e-07, -6.4128e-09,\n",
      "        -1.9036e-07,  1.1165e-07, -1.3332e-08, -1.3935e-07,  8.8661e-08,\n",
      "         2.7718e-09,  1.6635e-08,  4.0487e-08, -4.7379e-08,  2.7774e-09,\n",
      "        -1.3586e-08,  3.0116e-08,  3.0309e-08, -3.0029e-08, -1.6521e-07,\n",
      "         1.7381e-07, -1.0981e-07,  3.7716e-08, -8.8196e-08,  2.2940e-07,\n",
      "         1.4851e-07, -1.1722e-08,  1.5719e-07, -3.7660e-08,  1.1564e-07,\n",
      "         6.0923e-08,  1.8054e-07, -2.3821e-07, -1.2378e-07,  1.2458e-09,\n",
      "        -7.0998e-08, -6.8110e-09,  8.2422e-08, -6.8094e-08,  7.2351e-08,\n",
      "         5.8340e-09, -5.2320e-08,  2.8320e-08, -1.6505e-07, -5.8452e-08,\n",
      "        -7.9969e-09,  9.4570e-08,  5.8343e-08, -1.7539e-08, -6.6922e-08,\n",
      "         1.8078e-07,  8.1696e-08, -1.5411e-07, -1.1868e-07, -3.0878e-08,\n",
      "         7.4848e-09,  1.7299e-07, -4.9329e-08, -1.3129e-07, -7.4426e-08,\n",
      "         2.2462e-08,  1.1362e-07,  4.3287e-08,  6.4953e-08, -2.9208e-07,\n",
      "         2.6287e-08,  3.3221e-08,  3.5045e-08,  2.8232e-08, -4.0597e-08,\n",
      "         1.3325e-07, -6.5611e-08, -7.6072e-08,  1.0806e-08,  4.6562e-09,\n",
      "        -8.0727e-09, -7.5391e-08, -2.0675e-08, -5.5976e-08, -1.6249e-07,\n",
      "        -4.7046e-08, -1.4446e-08,  1.0761e-07, -1.0037e-07, -7.2927e-08,\n",
      "        -1.2368e-07,  1.2003e-07,  4.0105e-08, -2.6453e-07, -5.7346e-08,\n",
      "         6.3178e-09,  3.9687e-08,  2.1849e-08, -3.4015e-08,  5.4117e-08,\n",
      "        -3.3204e-09, -7.0078e-08, -1.2544e-07,  1.5544e-07, -8.4882e-08,\n",
      "         3.8132e-08, -9.5165e-08,  1.2435e-08,  1.6876e-07,  2.2208e-08,\n",
      "        -4.7339e-08, -2.5992e-08, -2.8342e-07,  2.0066e-07, -1.8374e-07,\n",
      "         3.5004e-08, -7.5143e-08, -2.4415e-08, -1.6921e-08,  1.8756e-07,\n",
      "         3.5135e-08,  2.2321e-07,  3.2126e-08,  1.1816e-08,  5.7991e-08,\n",
      "        -5.4853e-08, -6.3062e-08,  9.2338e-08, -7.6236e-08,  1.3037e-07,\n",
      "         2.5538e-07,  5.9282e-09,  6.3523e-08,  6.6141e-08, -2.7670e-08,\n",
      "         2.3176e-08, -2.3387e-08,  2.7741e-07,  6.1834e-08,  1.7566e-08,\n",
      "         7.1822e-08, -7.2516e-09, -1.0869e-07,  6.7724e-08, -6.0198e-08,\n",
      "         1.1626e-07,  8.1744e-08,  5.4184e-08,  6.4058e-09, -7.5322e-09,\n",
      "        -7.4042e-09,  7.9467e-08,  9.2434e-08,  2.0451e-07, -6.2301e-10,\n",
      "        -1.2204e-07, -6.8518e-08, -4.4893e-08,  3.7509e-08,  2.8133e-08,\n",
      "         3.0086e-08, -6.4943e-09, -5.4553e-08,  1.6841e-08,  2.1352e-08,\n",
      "         1.0338e-08, -2.7031e-09,  1.5240e-07, -2.3191e-08,  3.9101e-08,\n",
      "         2.6160e-08,  9.7965e-09, -4.6277e-08, -3.4053e-07, -1.9870e-09,\n",
      "        -3.0462e-08,  1.9695e-08, -5.9893e-08,  9.1499e-09, -4.3766e-08,\n",
      "        -1.1121e-07, -8.5717e-08,  1.3329e-08,  4.2415e-08, -1.9228e-09,\n",
      "        -5.1758e-08, -3.2188e-08,  9.3326e-08, -1.1461e-08,  9.3231e-08,\n",
      "         2.8308e-08, -8.2645e-08, -5.8273e-08,  1.9458e-07,  2.1325e-09,\n",
      "        -2.2358e-08,  6.6378e-08,  2.0106e-07, -4.3292e-08,  6.7724e-08,\n",
      "         1.1082e-07, -1.2659e-08,  1.1470e-08,  8.9042e-08,  1.5785e-08,\n",
      "         9.5972e-09, -5.6825e-08,  4.0899e-08, -5.3838e-08,  3.6381e-08,\n",
      "        -1.8212e-08,  2.8368e-08, -3.5079e-09,  3.1306e-08, -1.1849e-08,\n",
      "         1.0478e-07,  3.7975e-08,  9.5479e-11, -5.7689e-08,  6.5698e-09,\n",
      "         1.4634e-08, -7.4949e-08, -2.2490e-08, -4.2678e-08, -3.0317e-08,\n",
      "         1.1513e-07,  7.9446e-08, -4.1127e-08, -3.9622e-08,  8.0047e-08,\n",
      "        -1.0808e-07, -2.1004e-07,  1.2084e-08,  1.0611e-07,  4.7931e-08,\n",
      "         4.5353e-08, -7.2946e-08, -3.3534e-08,  7.6249e-08, -3.5502e-08,\n",
      "         4.6734e-07,  1.7578e-08,  5.8421e-08,  1.0391e-08, -2.8133e-09,\n",
      "        -3.6156e-08,  5.1173e-08,  7.5921e-08,  5.8753e-08, -7.5146e-08,\n",
      "         1.3188e-08, -5.4581e-09,  2.7904e-08,  1.0769e-07,  3.3303e-09,\n",
      "         3.2068e-09,  1.2286e-07, -1.3941e-07, -1.9548e-08, -2.2054e-08,\n",
      "         9.9460e-08, -6.3047e-08, -7.9653e-08, -4.1868e-08,  2.9532e-07,\n",
      "        -3.7823e-08,  7.9278e-08, -3.9456e-08, -1.0638e-07,  8.9985e-08,\n",
      "        -2.3539e-08,  8.2215e-08, -5.4838e-08, -8.9651e-08,  6.9617e-08,\n",
      "        -1.4224e-07,  9.0408e-08,  4.5011e-08, -1.6975e-08,  3.0212e-08,\n",
      "        -8.8336e-08,  6.3400e-09, -1.3836e-07, -9.9151e-08,  1.0616e-08,\n",
      "        -7.2681e-08,  1.1002e-08, -7.9347e-08,  3.6737e-08, -7.1732e-08,\n",
      "         4.1303e-08,  1.8359e-08,  3.9981e-08,  5.9652e-08, -1.8726e-08,\n",
      "         6.2868e-08, -4.8743e-08,  9.4520e-08, -4.5767e-08,  1.2920e-07,\n",
      "        -1.3063e-07, -7.1453e-08, -5.0254e-08,  3.8287e-08, -7.6303e-10,\n",
      "         1.3859e-08,  1.0184e-07,  5.2627e-08,  1.0143e-08,  1.8433e-08,\n",
      "        -2.0120e-08,  5.0188e-08,  5.0173e-08, -3.4522e-08,  5.2987e-08,\n",
      "        -9.2133e-08, -1.1314e-07, -1.5932e-07,  3.2877e-08,  1.3015e-08,\n",
      "         1.1710e-07, -5.8500e-08, -2.1589e-08,  4.0764e-08,  3.0849e-08,\n",
      "         1.4233e-07, -7.3130e-08], device='cuda:0'))\n",
      "('res2.1.1.weight', Parameter containing:\n",
      "tensor([0.4800, 0.4196, 0.5150, 0.6066, 0.4792, 0.5413, 0.4236, 0.4779, 0.6789,\n",
      "        0.4342, 0.5906, 0.4334, 0.4952, 0.4500, 0.4448, 0.4641, 0.5227, 0.5211,\n",
      "        0.4577, 0.4831, 0.5917, 0.5061, 0.5697, 0.5391, 0.4822, 0.3755, 0.5205,\n",
      "        0.4910, 0.4407, 0.4311, 0.5296, 0.5151, 0.4573, 0.5352, 0.5353, 0.4839,\n",
      "        0.4719, 0.5712, 0.4678, 0.5217, 0.5389, 0.4995, 0.4885, 0.4822, 0.4435,\n",
      "        0.5412, 0.4257, 0.6054, 0.4682, 0.6128, 0.6334, 0.3259, 0.5369, 0.4806,\n",
      "        0.5983, 0.3364, 0.5157, 0.4116, 0.4841, 0.4777, 0.5936, 0.4838, 0.5611,\n",
      "        0.4689, 0.4401, 0.5617, 0.4957, 0.5558, 0.4822, 0.6448, 0.5369, 0.4147,\n",
      "        0.4492, 0.5898, 0.4421, 0.4340, 0.4061, 0.5387, 0.4458, 0.4793, 0.5088,\n",
      "        0.4542, 0.4551, 0.5530, 0.5835, 0.3988, 0.4501, 0.5144, 0.3555, 0.4854,\n",
      "        0.4599, 0.3915, 0.4885, 0.5661, 0.5417, 0.4206, 0.4050, 0.4904, 0.4398,\n",
      "        0.4864, 0.4228, 0.5308, 0.6139, 0.3450, 0.4452, 0.4798, 0.4897, 0.5368,\n",
      "        0.5447, 0.5035, 0.6737, 0.4643, 0.4789, 0.4327, 0.4457, 0.5587, 0.3856,\n",
      "        0.4521, 0.4506, 0.3387, 0.4732, 0.4275, 0.4893, 0.5032, 0.5156, 0.6023,\n",
      "        0.4921, 0.4692, 0.5316, 0.4310, 0.4983, 0.5213, 0.4210, 0.4817, 0.2929,\n",
      "        0.6266, 0.4004, 0.4984, 0.4020, 0.4673, 0.4224, 0.5077, 0.3887, 0.4468,\n",
      "        0.3936, 0.5361, 0.4255, 0.5726, 0.3752, 0.4813, 0.4923, 0.5131, 0.3827,\n",
      "        0.3745, 0.5183, 0.4495, 0.4944, 0.5972, 0.4412, 0.5070, 0.4536, 0.3581,\n",
      "        0.5884, 0.3353, 0.5165, 0.5072, 0.4560, 0.5568, 0.5146, 0.6112, 0.5010,\n",
      "        0.5072, 0.4800, 0.4818, 0.4580, 0.5979, 0.4594, 0.5381, 0.4385, 0.6291,\n",
      "        0.5403, 0.6286, 0.4966, 0.4847, 0.3421, 0.4903, 0.4565, 0.5348, 0.5465,\n",
      "        0.5370, 0.5678, 0.4718, 0.4111, 0.5015, 0.4591, 0.5422, 0.5515, 0.4469,\n",
      "        0.4318, 0.4759, 0.5086, 0.3369, 0.4910, 0.3669, 0.5494, 0.4311, 0.4602,\n",
      "        0.4930, 0.4746, 0.5096, 0.5871, 0.5707, 0.4197, 0.4400, 0.5731, 0.4877,\n",
      "        0.5614, 0.4388, 0.5578, 0.4639, 0.4470, 0.5417, 0.6460, 0.5892, 0.4266,\n",
      "        0.3927, 0.4089, 0.4828, 0.4015, 0.6151, 0.4073, 0.4008, 0.5103, 0.5870,\n",
      "        0.5999, 0.4597, 0.5157, 0.5831, 0.4198, 0.5619, 0.4768, 0.4873, 0.5664,\n",
      "        0.5272, 0.4445, 0.3793, 0.5642, 0.4140, 0.4772, 0.5018, 0.4890, 0.5728,\n",
      "        0.3826, 0.4608, 0.4936, 0.4521, 0.5839, 0.4294, 0.3729, 0.4738, 0.6176,\n",
      "        0.5375, 0.4981, 0.4010, 0.4368, 0.4755, 0.5057, 0.4651, 0.4764, 0.5761,\n",
      "        0.5860, 0.4993, 0.5201, 0.5572, 0.5119, 0.4134, 0.4424, 0.4433, 0.5671,\n",
      "        0.3978, 0.4784, 0.4794, 0.4620, 0.4299, 0.4538, 0.4598, 0.5235, 0.3843,\n",
      "        0.5988, 0.4514, 0.5304, 0.4605, 0.5137, 0.4819, 0.5706, 0.5142, 0.3955,\n",
      "        0.4667, 0.5907, 0.5848, 0.5612, 0.3759, 0.5704, 0.5885, 0.4501, 0.4727,\n",
      "        0.3662, 0.3912, 0.4390, 0.5047, 0.4529, 0.4116, 0.5046, 0.6503, 0.5276,\n",
      "        0.4532, 0.5466, 0.4064, 0.4871, 0.4448, 0.4629, 0.3504, 0.5125, 0.4955,\n",
      "        0.5303, 0.4955, 0.4973, 0.4341, 0.4875, 0.6263, 0.5901, 0.5498, 0.4595,\n",
      "        0.4244, 0.6275, 0.5288, 0.5217, 0.5320, 0.4855, 0.4750, 0.5851, 0.4668,\n",
      "        0.4838, 0.5650, 0.4590, 0.3550, 0.4092, 0.4377, 0.4525, 0.5339, 0.3759,\n",
      "        0.2572, 0.5735, 0.4660, 0.3816, 0.5917, 0.4637, 0.5755, 0.3517, 0.6311,\n",
      "        0.4577, 0.3808, 0.5299, 0.4994, 0.4493, 0.5573, 0.5596, 0.5062, 0.4895,\n",
      "        0.3838, 0.5961, 0.4606, 0.5528, 0.3849, 0.5591, 0.3887, 0.4682, 0.4674,\n",
      "        0.5093, 0.4039, 0.5866, 0.5508, 0.4544, 0.4709, 0.3676, 0.4942, 0.4808,\n",
      "        0.4764, 0.5077, 0.5467, 0.5015, 0.4369, 0.5197, 0.4485, 0.4352, 0.3996,\n",
      "        0.4252, 0.4948, 0.4719, 0.4453, 0.4626, 0.4847, 0.5008, 0.5290, 0.4315,\n",
      "        0.5252, 0.4442, 0.4511, 0.5138, 0.5050, 0.5248, 0.5315, 0.4895, 0.5075,\n",
      "        0.4216, 0.5046, 0.5478, 0.4354, 0.4804, 0.4449, 0.4066, 0.4992, 0.4107,\n",
      "        0.3808, 0.5496, 0.5253, 0.5020, 0.6430, 0.4522, 0.4351, 0.4853, 0.5117,\n",
      "        0.4201, 0.4249, 0.5688, 0.5115, 0.4788, 0.5918, 0.4463, 0.4842, 0.5285,\n",
      "        0.5017, 0.4652, 0.3838, 0.4249, 0.4628, 0.4436, 0.5185, 0.5862, 0.5714,\n",
      "        0.3350, 0.4899, 0.6619, 0.4997, 0.5238, 0.4348, 0.5230, 0.5917, 0.5008,\n",
      "        0.5033, 0.5356, 0.5165, 0.4874, 0.3962, 0.4768, 0.4856, 0.5061, 0.5325,\n",
      "        0.4716, 0.3941, 0.5481, 0.4643, 0.5640, 0.3895, 0.4643, 0.3984, 0.4177,\n",
      "        0.5122, 0.3910, 0.4855, 0.4428, 0.4715, 0.5193, 0.4095, 0.5213, 0.3857,\n",
      "        0.4112, 0.3881, 0.5933, 0.5384, 0.5507, 0.5085, 0.5782, 0.3953, 0.4283,\n",
      "        0.4637, 0.4202, 0.4683, 0.4001, 0.4011, 0.4022, 0.4915, 0.5527, 0.3426,\n",
      "        0.4285, 0.6139, 0.4038, 0.4151, 0.4467, 0.4939, 0.3462, 0.3697],\n",
      "       device='cuda:0'))\n",
      "('res2.1.1.bias', Parameter containing:\n",
      "tensor([-0.2732, -0.2666, -0.3127, -0.3449, -0.2664, -0.2579, -0.2145, -0.2687,\n",
      "        -0.2877, -0.2202, -0.2540, -0.2087, -0.2851, -0.1923, -0.3321, -0.2439,\n",
      "        -0.2960, -0.3270, -0.2644, -0.2754, -0.3751, -0.2441, -0.3534, -0.3299,\n",
      "        -0.2734, -0.1558, -0.2423, -0.2597, -0.2339, -0.2326, -0.2556, -0.2886,\n",
      "        -0.2922, -0.3211, -0.3112, -0.2639, -0.2785, -0.3124, -0.1799, -0.2599,\n",
      "        -0.2689, -0.3217, -0.2580, -0.2806, -0.2452, -0.2864, -0.2299, -0.3389,\n",
      "        -0.2874, -0.3136, -0.2888, -0.1825, -0.3002, -0.3047, -0.2954, -0.2056,\n",
      "        -0.2270, -0.2103, -0.2733, -0.2703, -0.3372, -0.2165, -0.2901, -0.2474,\n",
      "        -0.2087, -0.2978, -0.3279, -0.3693, -0.2701, -0.3286, -0.3344, -0.2240,\n",
      "        -0.2302, -0.2445, -0.2352, -0.2370, -0.2503, -0.3386, -0.2747, -0.2493,\n",
      "        -0.2809, -0.2303, -0.2775, -0.3389, -0.2882, -0.2435, -0.2854, -0.3032,\n",
      "        -0.1852, -0.2723, -0.2211, -0.2302, -0.2458, -0.3140, -0.2807, -0.1839,\n",
      "        -0.2137, -0.2473, -0.2119, -0.2270, -0.2526, -0.2651, -0.2768, -0.2487,\n",
      "        -0.2251, -0.2321, -0.2443, -0.3292, -0.3065, -0.2991, -0.3895, -0.2753,\n",
      "        -0.3158, -0.2715, -0.2471, -0.3397, -0.1810, -0.2663, -0.2456, -0.1842,\n",
      "        -0.2459, -0.2101, -0.2603, -0.3128, -0.2824, -0.3396, -0.2249, -0.2669,\n",
      "        -0.2436, -0.2433, -0.2628, -0.2592, -0.2152, -0.2414, -0.1109, -0.3509,\n",
      "        -0.2017, -0.2951, -0.2328, -0.2688, -0.2155, -0.2829, -0.2234, -0.2330,\n",
      "        -0.2055, -0.2987, -0.2344, -0.3058, -0.2258, -0.2528, -0.2559, -0.2747,\n",
      "        -0.2308, -0.2227, -0.2565, -0.2846, -0.2819, -0.2563, -0.2489, -0.3288,\n",
      "        -0.2934, -0.2067, -0.3213, -0.1834, -0.2648, -0.2657, -0.2292, -0.2980,\n",
      "        -0.2995, -0.3228, -0.2066, -0.2593, -0.2785, -0.2364, -0.2742, -0.3323,\n",
      "        -0.1817, -0.3055, -0.2096, -0.3676, -0.2502, -0.3537, -0.2468, -0.2736,\n",
      "        -0.1694, -0.2754, -0.2802, -0.2904, -0.2991, -0.2833, -0.2910, -0.2974,\n",
      "        -0.2195, -0.3010, -0.2371, -0.2988, -0.3156, -0.3011, -0.2733, -0.2841,\n",
      "        -0.2496, -0.2057, -0.2883, -0.2373, -0.3293, -0.2488, -0.2770, -0.2711,\n",
      "        -0.2428, -0.3097, -0.3860, -0.3009, -0.1969, -0.2509, -0.2849, -0.2288,\n",
      "        -0.3032, -0.2666, -0.3215, -0.2214, -0.2508, -0.2925, -0.2650, -0.3471,\n",
      "        -0.1902, -0.2122, -0.2354, -0.2993, -0.1964, -0.3208, -0.2350, -0.2277,\n",
      "        -0.2744, -0.2980, -0.3826, -0.1741, -0.2846, -0.3248, -0.2271, -0.3520,\n",
      "        -0.2798, -0.2381, -0.2993, -0.3446, -0.2719, -0.2034, -0.3302, -0.2271,\n",
      "        -0.2289, -0.3017, -0.2436, -0.3282, -0.2334, -0.3036, -0.2865, -0.2889,\n",
      "        -0.2820, -0.2363, -0.2039, -0.3049, -0.3273, -0.2826, -0.3373, -0.2470,\n",
      "        -0.2371, -0.2884, -0.2096, -0.2681, -0.2894, -0.2874, -0.2897, -0.2997,\n",
      "        -0.2492, -0.2827, -0.2789, -0.2412, -0.2492, -0.2860, -0.3345, -0.2589,\n",
      "        -0.2666, -0.2425, -0.2688, -0.2507, -0.2548, -0.2349, -0.2691, -0.2149,\n",
      "        -0.3753, -0.2900, -0.3034, -0.2557, -0.2947, -0.2789, -0.2991, -0.2950,\n",
      "        -0.1817, -0.2645, -0.2663, -0.3610, -0.2667, -0.2369, -0.3425, -0.3325,\n",
      "        -0.2214, -0.2769, -0.2375, -0.2236, -0.2299, -0.2901, -0.2293, -0.2727,\n",
      "        -0.3276, -0.3290, -0.3330, -0.2156, -0.3475, -0.1863, -0.2581, -0.2691,\n",
      "        -0.2174, -0.2152, -0.2908, -0.2856, -0.2828, -0.2589, -0.2946, -0.2902,\n",
      "        -0.2580, -0.3337, -0.2968, -0.3068, -0.2368, -0.2747, -0.3812, -0.2774,\n",
      "        -0.2802, -0.2692, -0.2635, -0.2639, -0.3188, -0.2677, -0.2619, -0.2973,\n",
      "        -0.2674, -0.1915, -0.2685, -0.2792, -0.2725, -0.2922, -0.1693, -0.1401,\n",
      "        -0.2997, -0.2701, -0.2184, -0.3073, -0.2346, -0.2827, -0.1627, -0.3148,\n",
      "        -0.2307, -0.2373, -0.2958, -0.2750, -0.2586, -0.3163, -0.2952, -0.2791,\n",
      "        -0.2444, -0.1902, -0.3299, -0.2526, -0.2796, -0.2296, -0.3026, -0.2574,\n",
      "        -0.2206, -0.2402, -0.3032, -0.2160, -0.2914, -0.3310, -0.2471, -0.2655,\n",
      "        -0.1847, -0.2667, -0.2868, -0.3001, -0.2883, -0.3048, -0.2822, -0.2535,\n",
      "        -0.3022, -0.2273, -0.2434, -0.2169, -0.2128, -0.2585, -0.2189, -0.2906,\n",
      "        -0.2214, -0.2874, -0.2144, -0.3432, -0.2479, -0.3458, -0.2835, -0.2811,\n",
      "        -0.3098, -0.3218, -0.2833, -0.3394, -0.2419, -0.2757, -0.1855, -0.2968,\n",
      "        -0.2953, -0.2105, -0.2666, -0.2836, -0.2253, -0.3189, -0.3032, -0.2076,\n",
      "        -0.2729, -0.2999, -0.3105, -0.3102, -0.2111, -0.2705, -0.2506, -0.2736,\n",
      "        -0.2596, -0.2836, -0.2905, -0.2850, -0.2522, -0.2203, -0.2550, -0.3376,\n",
      "        -0.2940, -0.2253, -0.2701, -0.2075, -0.2127, -0.2956, -0.2708, -0.2601,\n",
      "        -0.3424, -0.3473, -0.1857, -0.2040, -0.3183, -0.2635, -0.2932, -0.2387,\n",
      "        -0.2496, -0.3093, -0.2711, -0.2911, -0.2900, -0.2635, -0.3110, -0.1899,\n",
      "        -0.2739, -0.2950, -0.2713, -0.3713, -0.2749, -0.2251, -0.3187, -0.2389,\n",
      "        -0.2988, -0.2586, -0.2647, -0.2510, -0.2402, -0.3066, -0.2068, -0.2485,\n",
      "        -0.2341, -0.2163, -0.2902, -0.2453, -0.3127, -0.2295, -0.1723, -0.2740,\n",
      "        -0.3737, -0.2538, -0.2226, -0.2916, -0.2753, -0.2056, -0.1577, -0.2336,\n",
      "        -0.2312, -0.2628, -0.2347, -0.1611, -0.2108, -0.2665, -0.2819, -0.1870,\n",
      "        -0.2240, -0.3304, -0.2162, -0.2030, -0.2795, -0.2942, -0.1883, -0.2353],\n",
      "       device='cuda:0'))\n",
      "('conv5.0.weight', Parameter containing:\n",
      "tensor([[[[ 6.5713e-03,  1.2785e-02,  1.6471e-02],\n",
      "          [ 6.9746e-04,  7.8814e-03,  1.8634e-03],\n",
      "          [ 9.9465e-03,  1.0779e-02,  8.2363e-04]],\n",
      "\n",
      "         [[-1.0186e-02, -3.3579e-03, -5.4698e-03],\n",
      "          [-1.9868e-02, -1.4121e-02, -1.2871e-02],\n",
      "          [ 1.2586e-03, -5.8603e-03, -9.4538e-03]],\n",
      "\n",
      "         [[ 3.8798e-03,  9.0443e-03,  1.1645e-02],\n",
      "          [-1.9270e-03, -7.9058e-03,  1.3919e-03],\n",
      "          [-1.1979e-02, -1.0673e-02, -2.9307e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.8721e-02,  3.8194e-03, -3.9463e-03],\n",
      "          [-6.0102e-04,  1.4186e-02,  1.5369e-02],\n",
      "          [ 4.7271e-03,  1.1158e-02,  1.5646e-02]],\n",
      "\n",
      "         [[-1.9049e-03,  2.8567e-03, -5.4335e-03],\n",
      "          [ 6.6335e-03,  1.2781e-02, -4.4335e-03],\n",
      "          [ 1.0765e-02,  4.6505e-03,  1.3099e-02]],\n",
      "\n",
      "         [[-5.0371e-03,  7.6867e-03,  2.4297e-02],\n",
      "          [ 9.0821e-03,  7.0327e-03,  3.4159e-03],\n",
      "          [ 6.6731e-03,  7.0708e-03,  2.1327e-03]]],\n",
      "\n",
      "\n",
      "        [[[-3.4763e-03,  1.1750e-02,  1.0761e-02],\n",
      "          [-1.4726e-02, -8.7479e-03,  5.1860e-03],\n",
      "          [-1.2598e-02, -3.1726e-02, -2.6426e-03]],\n",
      "\n",
      "         [[-7.5802e-03,  1.7590e-03, -4.3513e-03],\n",
      "          [ 1.0064e-02,  4.0753e-02,  1.5578e-02],\n",
      "          [-3.5922e-04,  1.4880e-02, -6.7679e-03]],\n",
      "\n",
      "         [[ 2.0983e-02,  1.7675e-02,  1.7981e-02],\n",
      "          [-1.3799e-02, -4.4046e-03, -1.5040e-03],\n",
      "          [-7.1769e-03, -1.1334e-02, -8.9623e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.7517e-02, -1.9148e-02, -2.1170e-02],\n",
      "          [ 2.0954e-03,  5.9974e-03,  3.2498e-03],\n",
      "          [ 1.3997e-02,  1.3002e-02,  1.4737e-02]],\n",
      "\n",
      "         [[ 1.0480e-02,  3.2111e-03, -1.1285e-02],\n",
      "          [-8.2914e-03,  8.7782e-03, -3.7062e-02],\n",
      "          [ 6.2394e-03,  7.2648e-03, -8.1556e-03]],\n",
      "\n",
      "         [[-9.0375e-03,  3.3091e-03,  7.4529e-03],\n",
      "          [ 1.5141e-02, -4.0991e-03,  1.1501e-02],\n",
      "          [ 1.5939e-03, -2.8529e-02, -1.2904e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4495e-03,  1.2533e-03,  4.4191e-03],\n",
      "          [ 7.7580e-03, -2.1501e-04,  1.0826e-02],\n",
      "          [-3.8279e-03, -1.9748e-03,  8.8807e-04]],\n",
      "\n",
      "         [[ 2.2699e-02,  2.9549e-02,  1.3886e-02],\n",
      "          [ 5.4097e-03, -1.0053e-02, -1.5511e-02],\n",
      "          [-6.9724e-04,  1.7360e-02, -4.8068e-04]],\n",
      "\n",
      "         [[ 4.0917e-03,  6.6180e-03, -1.0847e-02],\n",
      "          [-7.0232e-03, -4.5304e-03, -7.5696e-03],\n",
      "          [ 6.3126e-03,  1.0569e-02,  8.7276e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3769e-02, -1.8527e-02, -2.1785e-02],\n",
      "          [-2.2923e-03, -2.9519e-03, -9.8600e-03],\n",
      "          [ 7.8602e-03, -2.2642e-03,  7.4716e-03]],\n",
      "\n",
      "         [[ 3.1399e-03,  9.6900e-03,  7.4831e-03],\n",
      "          [-1.3093e-02, -4.9832e-03,  7.8944e-03],\n",
      "          [ 8.7359e-03, -9.7682e-03, -2.4407e-03]],\n",
      "\n",
      "         [[ 1.4183e-02,  1.7225e-02,  4.5390e-03],\n",
      "          [ 1.9248e-02, -2.5828e-03, -5.2485e-03],\n",
      "          [ 2.9658e-03, -9.3876e-03, -1.3580e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 1.0376e-02,  4.9630e-03,  7.3912e-03],\n",
      "          [ 6.8371e-03,  5.2042e-03,  4.7233e-03],\n",
      "          [-7.0098e-03, -1.0109e-02, -6.5657e-03]],\n",
      "\n",
      "         [[ 1.9095e-02,  8.8210e-03, -6.5772e-03],\n",
      "          [ 1.7637e-03, -1.0012e-02, -1.2572e-02],\n",
      "          [ 6.9141e-04, -5.6884e-03, -4.7578e-03]],\n",
      "\n",
      "         [[ 2.7412e-02,  1.5173e-02,  8.2449e-03],\n",
      "          [ 8.6321e-03,  4.7509e-03,  1.3933e-03],\n",
      "          [-3.6842e-03, -1.8234e-03,  2.4313e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-5.6750e-04, -3.8596e-03,  9.2003e-03],\n",
      "          [ 7.4285e-04, -3.5956e-03, -4.5497e-03],\n",
      "          [-8.6282e-03, -1.2793e-02, -2.2915e-02]],\n",
      "\n",
      "         [[ 3.1728e-03, -1.9452e-02, -8.0189e-03],\n",
      "          [ 1.2008e-03,  9.8601e-03,  1.0636e-02],\n",
      "          [ 8.3263e-05,  4.8015e-03,  8.2090e-03]],\n",
      "\n",
      "         [[-5.8984e-03,  5.3080e-03,  1.7221e-02],\n",
      "          [-2.7978e-03, -6.3360e-03, -2.2513e-03],\n",
      "          [ 6.2644e-03,  1.6097e-02, -4.8903e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1462e-03, -5.9834e-03,  2.5983e-03],\n",
      "          [ 2.8673e-03,  5.3338e-03, -7.8036e-03],\n",
      "          [-2.7154e-03,  7.1829e-03,  1.9509e-03]],\n",
      "\n",
      "         [[ 5.3122e-04, -7.4359e-03, -7.1855e-04],\n",
      "          [-9.7506e-03, -1.1083e-02,  6.5100e-03],\n",
      "          [-5.2016e-04, -8.5188e-03, -5.7655e-03]],\n",
      "\n",
      "         [[ 1.0011e-03,  9.1340e-04, -2.1197e-03],\n",
      "          [ 1.1561e-02,  5.2908e-03,  8.0127e-03],\n",
      "          [ 6.6329e-03,  1.8001e-02,  9.3542e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.6799e-03,  8.2511e-03,  5.4063e-03],\n",
      "          [-1.9580e-03, -4.1273e-03, -3.5213e-03],\n",
      "          [ 1.0083e-03, -1.8820e-03, -5.6399e-03]],\n",
      "\n",
      "         [[ 1.9916e-03,  1.5214e-03, -1.3312e-02],\n",
      "          [ 5.7508e-03, -8.5935e-03, -1.9870e-03],\n",
      "          [ 4.4221e-03,  8.0708e-04,  7.6199e-03]],\n",
      "\n",
      "         [[ 1.9545e-03,  9.0210e-03,  1.5775e-03],\n",
      "          [-4.2384e-03, -1.2103e-03,  1.3142e-03],\n",
      "          [ 7.1326e-03,  4.3901e-03, -7.7554e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7823e-02,  9.9714e-03, -2.6456e-04],\n",
      "          [ 2.1793e-02,  1.4655e-02,  9.2624e-03],\n",
      "          [-6.9057e-03,  5.1526e-03,  4.9095e-03]],\n",
      "\n",
      "         [[ 1.3217e-03, -2.7594e-03, -8.0356e-04],\n",
      "          [ 2.7147e-02,  1.0467e-02, -3.6114e-04],\n",
      "          [-8.5906e-03,  3.0268e-03, -4.0319e-03]],\n",
      "\n",
      "         [[-1.0916e-02,  2.3550e-03,  8.4645e-03],\n",
      "          [-2.5086e-02, -1.6393e-03,  1.3737e-02],\n",
      "          [-6.1743e-03, -1.0718e-02,  1.6567e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 6.0435e-04,  4.6672e-03, -2.1628e-03],\n",
      "          [ 1.2192e-02,  1.5007e-02, -6.5161e-03],\n",
      "          [-3.5563e-04, -3.0566e-03, -2.2096e-03]],\n",
      "\n",
      "         [[-5.8384e-04,  2.3197e-04, -7.5326e-03],\n",
      "          [ 3.2598e-03,  2.4072e-03, -1.0607e-05],\n",
      "          [ 7.3864e-03, -9.0201e-03,  7.6430e-04]],\n",
      "\n",
      "         [[-1.3658e-02, -1.7762e-02, -1.0541e-03],\n",
      "          [-5.3602e-03,  1.0787e-02,  8.4870e-03],\n",
      "          [-2.1447e-03,  1.1464e-02,  2.1066e-03]]]], device='cuda:0'))\n",
      "('conv5.0.bias', Parameter containing:\n",
      "tensor([-2.6659e-08, -2.2349e-08,  2.7177e-08,  ..., -1.3022e-08,\n",
      "         1.2731e-08, -3.3084e-08], device='cuda:0'))\n",
      "('conv5.1.weight', Parameter containing:\n",
      "tensor([0.7002, 0.7579, 0.7305,  ..., 0.6540, 0.5771, 0.6765], device='cuda:0'))\n",
      "('conv5.1.bias', Parameter containing:\n",
      "tensor([-0.0527, -0.0612, -0.0500,  ..., -0.0827, -0.0880, -0.0542],\n",
      "       device='cuda:0'))\n",
      "('res3.0.0.weight', Parameter containing:\n",
      "tensor([[[[ 6.0790e-04, -1.8107e-04, -4.4472e-04],\n",
      "          [-2.4739e-04, -1.6069e-03, -8.0740e-04],\n",
      "          [-3.0610e-03, -5.3077e-03, -1.3739e-03]],\n",
      "\n",
      "         [[-6.5016e-05, -7.6310e-04, -7.0667e-04],\n",
      "          [-4.0921e-03, -2.1459e-03, -1.5190e-03],\n",
      "          [-3.7253e-03, -5.4316e-03, -1.3698e-03]],\n",
      "\n",
      "         [[-7.3325e-04, -1.2233e-03, -6.4942e-04],\n",
      "          [-3.6871e-04, -1.6298e-03, -1.1782e-03],\n",
      "          [-9.2895e-04, -2.0674e-03, -5.6951e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.3952e-04, -2.1850e-04, -2.7675e-04],\n",
      "          [ 3.7092e-04,  2.0571e-03, -2.4521e-04],\n",
      "          [ 3.1036e-03,  4.2271e-03, -6.0718e-05]],\n",
      "\n",
      "         [[-2.0465e-03, -1.9496e-03, -9.1538e-04],\n",
      "          [ 7.6928e-04,  3.0399e-04, -2.5387e-05],\n",
      "          [ 1.4671e-03,  1.1803e-03, -1.3963e-04]],\n",
      "\n",
      "         [[-3.2000e-04, -2.3910e-03, -1.0701e-03],\n",
      "          [-2.2190e-03, -4.0913e-03, -2.2658e-03],\n",
      "          [-4.6396e-04, -1.4097e-03, -1.6155e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.0418e-03,  1.2345e-02,  1.0058e-02],\n",
      "          [ 1.0988e-02,  1.5654e-02, -3.8838e-03],\n",
      "          [ 3.6596e-03,  3.9622e-03, -6.3718e-03]],\n",
      "\n",
      "         [[ 1.4153e-03,  6.1189e-03, -3.6996e-03],\n",
      "          [-3.5228e-03, -4.2759e-04, -5.8080e-03],\n",
      "          [-4.6241e-03, -7.2593e-03, -2.1033e-03]],\n",
      "\n",
      "         [[-9.0923e-04,  3.4934e-03, -8.4190e-05],\n",
      "          [ 5.3116e-04, -3.2154e-03, -3.4684e-03],\n",
      "          [-1.5615e-03, -5.3087e-03, -8.7638e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.2274e-03,  9.1248e-03,  3.0291e-03],\n",
      "          [-3.3416e-03, -1.0982e-02, -4.0315e-03],\n",
      "          [ 1.5451e-03,  3.7563e-03,  2.3067e-03]],\n",
      "\n",
      "         [[-7.0526e-03, -9.6745e-03, -2.8369e-03],\n",
      "          [-4.2302e-03, -9.7092e-03, -2.7528e-03],\n",
      "          [ 3.5285e-04,  1.7558e-03,  2.0966e-03]],\n",
      "\n",
      "         [[ 5.3552e-03,  5.1602e-03, -2.7397e-03],\n",
      "          [-2.2018e-03, -1.2217e-02, -6.6055e-03],\n",
      "          [-2.3893e-03, -6.9285e-03, -2.5660e-03]]],\n",
      "\n",
      "\n",
      "        [[[-2.3904e-04,  5.7161e-04,  5.6582e-05],\n",
      "          [ 3.9897e-04,  1.1962e-03,  6.2346e-04],\n",
      "          [ 7.9367e-05,  6.3868e-04,  7.4384e-04]],\n",
      "\n",
      "         [[-3.1699e-04, -8.1738e-04, -3.6538e-04],\n",
      "          [-7.4154e-04, -1.3648e-03, -1.0760e-05],\n",
      "          [-9.9847e-05, -1.4368e-04, -2.3354e-04]],\n",
      "\n",
      "         [[-8.0784e-04,  5.1945e-04,  2.2836e-04],\n",
      "          [-7.7071e-04, -8.4638e-04,  4.2862e-04],\n",
      "          [-2.1204e-04,  4.1936e-05,  1.3228e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.7107e-05, -2.2227e-04, -2.4746e-04],\n",
      "          [-5.1745e-04, -6.1036e-04,  1.7662e-04],\n",
      "          [-1.6572e-04,  3.8395e-05,  1.2275e-04]],\n",
      "\n",
      "         [[ 1.6232e-05,  4.2914e-04,  4.6133e-05],\n",
      "          [ 3.3135e-04,  8.6349e-04,  2.6824e-04],\n",
      "          [-1.7874e-04, -1.2088e-04,  1.4887e-04]],\n",
      "\n",
      "         [[ 2.4079e-04,  3.9727e-04, -4.1044e-04],\n",
      "          [ 9.5914e-05, -1.2917e-04, -7.4801e-04],\n",
      "          [-1.1200e-04, -4.8264e-05,  1.6563e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 3.5625e-04,  3.7659e-04,  2.3434e-04],\n",
      "          [ 1.5413e-04,  1.7592e-04,  3.2534e-05],\n",
      "          [-4.8680e-06,  5.5389e-06,  2.5096e-05]],\n",
      "\n",
      "         [[ 1.5228e-04,  3.0195e-04,  1.7358e-04],\n",
      "          [ 2.4264e-04,  3.9574e-04,  6.5192e-05],\n",
      "          [-3.7983e-06,  1.6692e-04, -8.1424e-05]],\n",
      "\n",
      "         [[ 1.2490e-04,  2.2572e-04,  1.3301e-04],\n",
      "          [ 1.0480e-04, -2.0772e-05,  2.2368e-05],\n",
      "          [-7.9048e-05,  6.6274e-05,  9.9865e-05]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.9342e-04, -1.4351e-04, -6.7692e-05],\n",
      "          [ 7.0822e-05,  3.0716e-05,  8.8703e-05],\n",
      "          [ 6.9929e-06, -1.0304e-05, -6.3671e-05]],\n",
      "\n",
      "         [[ 1.2693e-05, -8.8634e-05, -4.3150e-05],\n",
      "          [-1.4717e-05, -1.2674e-04,  5.6538e-05],\n",
      "          [ 3.9164e-06,  2.3135e-05,  3.7847e-05]],\n",
      "\n",
      "         [[-1.6030e-04, -1.6847e-04,  1.3037e-04],\n",
      "          [-1.9424e-04,  4.0304e-05,  4.5116e-05],\n",
      "          [-7.5881e-06, -4.4618e-05,  1.5445e-05]]],\n",
      "\n",
      "\n",
      "        [[[-4.3359e-03, -2.3946e-03, -9.4128e-04],\n",
      "          [-4.5205e-03, -9.2157e-03, -5.5013e-04],\n",
      "          [-1.9254e-03, -3.1850e-03, -9.1484e-04]],\n",
      "\n",
      "         [[-5.0573e-03, -4.1794e-03, -1.5151e-03],\n",
      "          [-8.3876e-03, -5.2274e-03, -8.5820e-04],\n",
      "          [-8.9329e-03, -6.4116e-03, -2.3127e-03]],\n",
      "\n",
      "         [[-3.2004e-03, -2.1032e-03, -5.4088e-04],\n",
      "          [-6.9615e-03,  1.5370e-04, -8.6594e-04],\n",
      "          [-9.6714e-03,  7.8366e-04, -1.6539e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.1251e-03, -3.1602e-04, -1.6528e-04],\n",
      "          [ 7.3485e-04, -2.6765e-03, -1.7824e-03],\n",
      "          [-4.1580e-03, -2.0850e-03, -1.1464e-03]],\n",
      "\n",
      "         [[-1.8584e-03, -2.4887e-03, -2.2105e-04],\n",
      "          [-4.6743e-03, -2.5020e-03, -9.6729e-04],\n",
      "          [-1.2323e-03, -1.4928e-03, -2.9595e-04]],\n",
      "\n",
      "         [[ 1.3172e-03, -2.1919e-03, -1.1921e-03],\n",
      "          [ 2.0865e-03,  6.1437e-03,  1.1368e-03],\n",
      "          [-5.4062e-04,  2.6879e-03,  6.4912e-05]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5761e-05, -6.2185e-05, -1.1435e-04],\n",
      "          [-1.0017e-04,  1.7721e-04, -8.7241e-05],\n",
      "          [-1.2718e-04, -2.8045e-04, -2.5172e-04]],\n",
      "\n",
      "         [[ 1.8970e-04,  1.4976e-04,  1.5207e-04],\n",
      "          [ 5.6235e-05, -1.0692e-05, -5.0972e-04],\n",
      "          [-4.6310e-05, -5.0682e-05, -1.8151e-05]],\n",
      "\n",
      "         [[-3.5106e-05,  1.5158e-04,  1.1084e-04],\n",
      "          [ 1.9339e-04, -2.4451e-04, -7.1321e-05],\n",
      "          [ 1.4941e-04,  1.2085e-04, -1.1044e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.1876e-05,  1.1639e-06,  5.1542e-05],\n",
      "          [ 1.2759e-04,  3.4488e-04, -3.1157e-05],\n",
      "          [-8.6019e-05, -7.7565e-04, -6.7917e-04]],\n",
      "\n",
      "         [[-6.6066e-05, -1.7502e-04, -3.3617e-05],\n",
      "          [ 1.2307e-04,  5.9569e-04,  1.6155e-04],\n",
      "          [ 1.7765e-05,  3.7385e-04,  8.3582e-05]],\n",
      "\n",
      "         [[-9.0232e-05,  1.4301e-04,  1.8568e-04],\n",
      "          [-4.4066e-05,  4.5483e-04, -2.3107e-04],\n",
      "          [ 2.4504e-05, -2.4307e-04, -3.7853e-04]]]], device='cuda:0'))\n",
      "('res3.0.0.bias', Parameter containing:\n",
      "tensor([ 3.7030e-09, -2.0979e-09, -3.3576e-10,  ..., -7.7151e-10,\n",
      "         8.2714e-09,  2.3009e-10], device='cuda:0'))\n",
      "('res3.0.1.weight', Parameter containing:\n",
      "tensor([0.1131, 0.3184, 0.0497,  ..., 0.0235, 0.2134, 0.0450], device='cuda:0'))\n",
      "('res3.0.1.bias', Parameter containing:\n",
      "tensor([-0.1098, -0.1357, -0.0430,  ..., -0.0149, -0.1445, -0.0367],\n",
      "       device='cuda:0'))\n",
      "('res3.1.0.weight', Parameter containing:\n",
      "tensor([[[[-1.6911e-04, -3.0694e-03, -4.9951e-03],\n",
      "          [ 9.8021e-04, -1.3081e-03, -2.4188e-03],\n",
      "          [-3.8882e-04, -1.4715e-03, -5.2037e-05]],\n",
      "\n",
      "         [[ 1.4059e-03,  2.1192e-03, -1.4277e-03],\n",
      "          [-9.3016e-03,  1.4014e-02,  5.0726e-03],\n",
      "          [ 5.4473e-03,  1.7737e-02,  4.1231e-03]],\n",
      "\n",
      "         [[-9.3683e-04,  9.3303e-05,  1.1280e-04],\n",
      "          [-2.8101e-04, -1.0151e-03,  1.1336e-03],\n",
      "          [-5.3128e-04,  1.7997e-04,  2.7113e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-6.4420e-05,  1.7206e-03,  1.5092e-05],\n",
      "          [ 4.7597e-04, -4.1085e-04,  6.9376e-05],\n",
      "          [-2.2791e-04, -3.3469e-04, -9.5350e-05]],\n",
      "\n",
      "         [[ 1.0639e-03,  1.7593e-04, -1.6789e-03],\n",
      "          [ 2.2538e-03, -3.9344e-03, -3.8319e-03],\n",
      "          [-5.1478e-04, -2.0201e-03, -4.6400e-04]],\n",
      "\n",
      "         [[ 3.1886e-05,  1.5770e-03,  4.3803e-04],\n",
      "          [ 1.4952e-03,  9.9030e-05, -4.0901e-04],\n",
      "          [ 3.3923e-05, -1.2462e-06, -1.0428e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.9430e-03, -2.0377e-03, -9.3082e-04],\n",
      "          [-1.5577e-04,  5.6035e-05,  4.6868e-04],\n",
      "          [-4.4126e-04,  5.7966e-04, -3.0208e-04]],\n",
      "\n",
      "         [[-6.6820e-04, -2.4527e-03,  1.0934e-03],\n",
      "          [-7.7549e-03, -7.7163e-03, -8.6160e-04],\n",
      "          [-4.7644e-03,  3.1786e-03, -2.2871e-03]],\n",
      "\n",
      "         [[-4.9881e-04, -1.2395e-04,  1.9398e-05],\n",
      "          [-6.6110e-04, -1.0732e-03, -1.5586e-04],\n",
      "          [ 1.6542e-04, -9.0689e-04, -2.2873e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 1.0414e-04, -1.0658e-04,  9.8029e-07],\n",
      "          [ 1.4431e-05, -1.5013e-04,  4.3077e-07],\n",
      "          [-1.7125e-04, -7.2125e-04, -4.0423e-04]],\n",
      "\n",
      "         [[-5.0889e-04,  5.5831e-04,  2.4205e-04],\n",
      "          [-1.2660e-03, -4.0908e-03, -4.6019e-03],\n",
      "          [ 6.9595e-04, -2.8304e-03, -2.8638e-03]],\n",
      "\n",
      "         [[ 5.2945e-05,  7.8011e-04,  2.5743e-05],\n",
      "          [ 2.1216e-05, -6.6049e-04, -5.0353e-06],\n",
      "          [-5.1803e-05,  1.5712e-04, -7.9909e-06]]],\n",
      "\n",
      "\n",
      "        [[[-6.0575e-04,  4.8938e-03,  7.1973e-03],\n",
      "          [-1.1110e-03, -1.7868e-03,  2.7725e-03],\n",
      "          [ 7.8643e-05, -5.1659e-04, -5.6194e-04]],\n",
      "\n",
      "         [[-9.2305e-04, -6.7806e-03, -3.9684e-03],\n",
      "          [ 3.8778e-03, -8.5811e-03, -9.3475e-03],\n",
      "          [ 1.6499e-03,  3.4258e-03,  2.6242e-03]],\n",
      "\n",
      "         [[-6.3469e-04, -3.8081e-04,  7.0296e-05],\n",
      "          [-6.3430e-04, -1.5988e-04, -6.5677e-04],\n",
      "          [-3.6508e-04, -1.5228e-03, -1.2009e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-4.5742e-05,  4.0658e-04, -9.9072e-05],\n",
      "          [-3.9574e-04,  2.3761e-05, -2.5643e-04],\n",
      "          [-1.3360e-04, -2.4083e-04, -3.2049e-04]],\n",
      "\n",
      "         [[ 4.5449e-05, -3.4046e-03,  1.8799e-03],\n",
      "          [-1.2828e-03,  2.0998e-04,  6.3569e-03],\n",
      "          [-2.5661e-04, -1.6991e-03, -8.8628e-03]],\n",
      "\n",
      "         [[ 1.1607e-04,  1.3978e-03,  2.3391e-04],\n",
      "          [-9.2711e-04, -1.6536e-03,  1.3210e-04],\n",
      "          [-6.4713e-05,  6.6567e-05, -1.1796e-04]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-3.8725e-04,  2.0417e-04, -6.4099e-04],\n",
      "          [-7.4813e-04, -1.2191e-03, -2.5437e-03],\n",
      "          [ 2.7289e-04, -1.4742e-03, -4.3467e-04]],\n",
      "\n",
      "         [[-1.1602e-03, -8.7525e-04, -1.7778e-03],\n",
      "          [-3.9288e-03, -2.4299e-03, -2.7738e-03],\n",
      "          [-8.7176e-03, -2.0251e-03, -1.8305e-04]],\n",
      "\n",
      "         [[ 3.0097e-04, -6.8026e-04, -2.5102e-05],\n",
      "          [-4.0149e-04, -3.6358e-04,  2.9030e-04],\n",
      "          [-6.0472e-04,  5.0179e-04,  5.3299e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-2.3845e-05, -1.9136e-04, -7.0654e-08],\n",
      "          [-2.0436e-04, -9.7104e-05, -4.6395e-04],\n",
      "          [-5.5010e-05,  3.6111e-05,  2.1444e-05]],\n",
      "\n",
      "         [[ 2.3002e-06, -3.2511e-03,  1.1924e-03],\n",
      "          [-2.0323e-03, -3.0720e-03,  8.8278e-03],\n",
      "          [-1.2070e-03, -1.0335e-03, -9.9025e-04]],\n",
      "\n",
      "         [[-2.8760e-04, -1.7963e-05,  1.3376e-04],\n",
      "          [-5.9670e-04,  1.9605e-04, -5.2182e-05],\n",
      "          [ 6.7927e-07, -1.5675e-04, -1.1428e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.5287e-04, -1.9951e-04, -3.4440e-04],\n",
      "          [ 1.6551e-04,  4.1444e-03, -2.9589e-03],\n",
      "          [ 2.5464e-05,  1.4815e-04,  1.5565e-04]],\n",
      "\n",
      "         [[-2.2152e-03, -3.0278e-03,  5.0670e-04],\n",
      "          [-1.1408e-02, -1.0668e-02, -9.8128e-03],\n",
      "          [-5.5666e-03, -1.3408e-02, -4.1189e-03]],\n",
      "\n",
      "         [[-8.3552e-05, -3.2389e-04, -5.1836e-05],\n",
      "          [-4.5884e-04, -7.2057e-04, -3.3006e-04],\n",
      "          [-4.4270e-04, -8.3118e-04,  5.2910e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-7.4479e-05, -3.5404e-05,  4.8597e-06],\n",
      "          [-2.9699e-04,  9.8000e-05,  5.1185e-05],\n",
      "          [ 2.2952e-05, -5.4895e-04, -4.1434e-04]],\n",
      "\n",
      "         [[-1.5891e-04,  6.3890e-03, -2.4361e-03],\n",
      "          [-1.4363e-03,  2.1792e-03, -3.7158e-03],\n",
      "          [-7.1137e-04,  2.6748e-03,  2.7507e-04]],\n",
      "\n",
      "         [[-1.3824e-04,  3.8453e-05, -8.0611e-05],\n",
      "          [-6.3661e-04,  1.8611e-03,  2.2365e-04],\n",
      "          [ 6.0550e-06, -2.8776e-05, -4.2662e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.1233e-03,  6.8610e-04, -6.8448e-04],\n",
      "          [-2.6614e-05, -6.7734e-04, -3.5422e-03],\n",
      "          [-4.3129e-04, -7.2610e-04,  9.8661e-04]],\n",
      "\n",
      "         [[-2.7442e-03, -9.4713e-04, -3.2022e-03],\n",
      "          [-7.4592e-03,  1.0505e-03, -3.4467e-03],\n",
      "          [-7.0142e-04,  1.0883e-02,  1.9877e-03]],\n",
      "\n",
      "         [[-1.3047e-04, -1.1433e-04, -1.5074e-05],\n",
      "          [-1.6720e-03,  4.2365e-04,  1.3346e-04],\n",
      "          [-3.1892e-04,  3.1446e-04,  3.4661e-04]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.3722e-04, -1.1480e-05, -5.5102e-06],\n",
      "          [ 9.9227e-05, -2.5496e-04, -1.0259e-04],\n",
      "          [-2.7124e-05, -7.7823e-04, -2.9070e-04]],\n",
      "\n",
      "         [[ 2.3632e-04,  5.1670e-03, -2.8107e-03],\n",
      "          [ 5.1064e-04,  8.5327e-03, -6.1412e-04],\n",
      "          [-1.4532e-03, -9.1104e-04,  2.1072e-03]],\n",
      "\n",
      "         [[ 1.8809e-03, -9.6515e-05, -2.4187e-05],\n",
      "          [-6.8167e-04,  8.6445e-06, -1.6896e-04],\n",
      "          [-1.1889e-04,  1.0053e-04, -5.1748e-05]]]], device='cuda:0'))\n",
      "('res3.1.0.bias', Parameter containing:\n",
      "tensor([ 5.1046e-08,  2.3350e-08,  6.2172e-08,  ..., -2.6382e-08,\n",
      "         6.0656e-08, -1.0842e-08], device='cuda:0'))\n",
      "('res3.1.1.weight', Parameter containing:\n",
      "tensor([0.4780, 0.4386, 0.5648,  ..., 0.3062, 0.4088, 0.3805], device='cuda:0'))\n",
      "('res3.1.1.bias', Parameter containing:\n",
      "tensor([ 0.0052, -0.0207, -0.0075,  ..., -0.0199, -0.0034, -0.0114],\n",
      "       device='cuda:0'))\n",
      "('classifier_parent.2.weight', Parameter containing:\n",
      "tensor([[ 0.0959,  0.0232, -0.0501,  ...,  0.0311,  0.0740,  0.0713],\n",
      "        [ 0.0657, -0.0543,  0.0663,  ...,  0.0480,  0.0339,  0.0289],\n",
      "        [-0.0303,  0.0676,  0.0305,  ..., -0.0599, -0.0071, -0.0250],\n",
      "        ...,\n",
      "        [ 0.0505, -0.0517,  0.0673,  ...,  0.0809,  0.0334, -0.0635],\n",
      "        [-0.0156, -0.0298,  0.0444,  ...,  0.0632,  0.0450,  0.0275],\n",
      "        [ 0.0142,  0.0639,  0.1025,  ...,  0.0015,  0.0508,  0.0546]],\n",
      "       device='cuda:0'))\n",
      "('classifier_parent.2.bias', Parameter containing:\n",
      "tensor([-0.0049,  0.0076,  0.0190,  0.0306,  0.0223,  0.0341, -0.0018,  0.0177,\n",
      "         0.0275,  0.0082,  0.0171,  0.0145,  0.0355, -0.0042,  0.0362,  0.0177,\n",
      "        -0.0043, -0.0085, -0.0068,  0.0310], device='cuda:0'))\n",
      "('classifier_child.2.weight', Parameter containing:\n",
      "tensor([[ 1.6543e-04, -1.0488e-01, -1.6758e-02, -1.0493e-03, -6.5415e-02,\n",
      "          3.8477e-02,  1.9607e-01,  2.1096e-01,  8.9559e-02, -2.2106e-01,\n",
      "         -2.1690e-01, -4.7450e-02, -1.1205e-01, -1.5315e-01, -3.6036e-02,\n",
      "          1.4833e-02, -1.6603e-01,  1.6032e-01, -1.8250e-01,  1.1018e-01],\n",
      "        [-1.0822e-01,  3.1695e-02, -6.1687e-02, -7.3830e-02, -1.9011e-01,\n",
      "         -8.9468e-02,  7.4094e-02, -1.6529e-01, -1.7088e-01,  1.7846e-01,\n",
      "          1.5091e-01, -9.9682e-02, -1.7241e-01, -1.5619e-01, -3.3895e-02,\n",
      "         -7.5076e-02, -7.1236e-02,  1.6211e-02,  1.2172e-01,  2.0927e-01],\n",
      "        [-1.1020e-01, -7.7390e-02,  2.1096e-02, -1.5301e-01, -1.2371e-01,\n",
      "         -2.1668e-01,  8.7838e-02, -1.5364e-01,  9.2074e-03,  2.0107e-01,\n",
      "          2.1040e-02, -1.1338e-01, -1.5433e-01,  7.6639e-02, -4.1816e-02,\n",
      "         -1.5315e-01, -7.2782e-02, -1.9151e-01, -2.0602e-01, -1.8375e-01],\n",
      "        [ 1.6007e-01,  1.3000e-01, -1.3935e-01, -1.9216e-01, -1.4369e-01,\n",
      "         -7.2778e-02,  3.9803e-02,  8.0933e-02,  3.3379e-02, -2.9493e-02,\n",
      "         -7.4899e-02,  2.0473e-01,  1.5744e-01,  7.3922e-02, -5.8911e-02,\n",
      "          2.3199e-02, -3.3872e-02,  1.7216e-01, -7.2598e-04,  5.2472e-02],\n",
      "        [ 6.9885e-02, -1.2691e-01, -1.2969e-02, -1.0030e-01, -1.2744e-01,\n",
      "         -8.6178e-02, -1.2405e-01,  1.8852e-01,  2.7933e-02,  2.0687e-01,\n",
      "         -8.2545e-02, -1.6670e-02,  6.5653e-02, -6.8258e-02, -1.3363e-02,\n",
      "          1.3882e-01,  1.3966e-01, -2.7617e-02,  5.7001e-02, -1.5228e-01]],\n",
      "       device='cuda:0', requires_grad=True))\n",
      "('classifier_child.2.bias', Parameter containing:\n",
      "tensor([ 0.1147, -0.0745, -0.0151,  0.0413, -0.1722], device='cuda:0',\n",
      "       requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param in model20To100.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]           1,792\n",
      "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
      "              ReLU-3           [-1, 64, 32, 32]               0\n",
      "            Conv2d-4          [-1, 128, 32, 32]          73,856\n",
      "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
      "              ReLU-6          [-1, 128, 32, 32]               0\n",
      "         MaxPool2d-7          [-1, 128, 16, 16]               0\n",
      "            Conv2d-8          [-1, 128, 16, 16]         147,584\n",
      "       BatchNorm2d-9          [-1, 128, 16, 16]             256\n",
      "             ReLU-10          [-1, 128, 16, 16]               0\n",
      "           Conv2d-11          [-1, 128, 16, 16]         147,584\n",
      "      BatchNorm2d-12          [-1, 128, 16, 16]             256\n",
      "             ReLU-13          [-1, 128, 16, 16]               0\n",
      "           Conv2d-14          [-1, 256, 16, 16]         295,168\n",
      "      BatchNorm2d-15          [-1, 256, 16, 16]             512\n",
      "             ReLU-16          [-1, 256, 16, 16]               0\n",
      "        MaxPool2d-17            [-1, 256, 8, 8]               0\n",
      "           Conv2d-18            [-1, 512, 8, 8]       1,180,160\n",
      "      BatchNorm2d-19            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-20            [-1, 512, 8, 8]               0\n",
      "        MaxPool2d-21            [-1, 512, 4, 4]               0\n",
      "           Conv2d-22            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-23            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-24            [-1, 512, 4, 4]               0\n",
      "           Conv2d-25            [-1, 512, 4, 4]       2,359,808\n",
      "      BatchNorm2d-26            [-1, 512, 4, 4]           1,024\n",
      "             ReLU-27            [-1, 512, 4, 4]               0\n",
      "           Conv2d-28           [-1, 1028, 4, 4]       4,738,052\n",
      "      BatchNorm2d-29           [-1, 1028, 4, 4]           2,056\n",
      "             ReLU-30           [-1, 1028, 4, 4]               0\n",
      "        MaxPool2d-31           [-1, 1028, 2, 2]               0\n",
      "           Conv2d-32           [-1, 1028, 2, 2]       9,512,084\n",
      "      BatchNorm2d-33           [-1, 1028, 2, 2]           2,056\n",
      "             ReLU-34           [-1, 1028, 2, 2]               0\n",
      "           Conv2d-35           [-1, 1028, 2, 2]       9,512,084\n",
      "      BatchNorm2d-36           [-1, 1028, 2, 2]           2,056\n",
      "             ReLU-37           [-1, 1028, 2, 2]               0\n",
      "        MaxPool2d-38           [-1, 1028, 1, 1]               0\n",
      "        MaxPool2d-39           [-1, 1028, 1, 1]               0\n",
      "          Flatten-40                 [-1, 1028]               0\n",
      "          Flatten-41                 [-1, 1028]               0\n",
      "           Linear-42                   [-1, 20]          20,580\n",
      "           Linear-43                   [-1, 20]          20,580\n",
      "             ReLU-44                   [-1, 20]               0\n",
      "           Linear-45                    [-1, 5]             105\n",
      "================================================================\n",
      "Total params: 30,379,893\n",
      "Trainable params: 105\n",
      "Non-trainable params: 30,379,788\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 9.69\n",
      "Params size (MB): 115.89\n",
      "Estimated Total Size (MB): 125.59\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(summary(model20To100,(3, 32, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parent to children Network is more complicated now (20 to 10 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model20To100 = ResNet9(3, 100, True)\n",
    "model20To100.load_state_dict(torch.load('group_1028_to_parent_Loss_parent_and_child_pretrained_model.h5'))\n",
    "for param in model20To100.parameters():\n",
    "    param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet9(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res1): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res2): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (conv5): Sequential(\n",
       "    (0): Conv2d(512, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (res3): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): Conv2d(1028, 1028, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(1028, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier_parent): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Linear(in_features=1028, out_features=20, bias=True)\n",
       "  )\n",
       "  (classifier_child): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=1028, out_features=20, bias=True)\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=10, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model20To100.classifier_child = nn.Sequential(model20To100.classifier_parent,\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(20, 10),\n",
    "                                        nn.ReLU(),\n",
    "                                        nn.Linear(10, 5)\n",
    "                                        )\n",
    "                                        \n",
    "model20To100=to_device(model20To100, device)\n",
    "model20To100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 2.6931605339050293, 'val_acc': 0.27399998903274536}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial evaluation\n",
    "history = [evaluate(model20To100, testloader)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 2.6931605339050293, 'val_acc': 0.27399998903274536}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [evaluate(model20To100, testloader)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00004, train_loss: 0.6183, val_loss: 0.9019, val_acc: 0.6740\n",
      "Epoch [1], last_lr: 0.00005, train_loss: 0.6047, val_loss: 0.9017, val_acc: 0.6720\n",
      "Epoch [2], last_lr: 0.00005, train_loss: 0.6069, val_loss: 0.9028, val_acc: 0.6780\n",
      "Epoch [3], last_lr: 0.00007, train_loss: 0.6249, val_loss: 0.9006, val_acc: 0.6740\n",
      "Epoch [4], last_lr: 0.00008, train_loss: 0.6162, val_loss: 0.9009, val_acc: 0.6740\n",
      "Epoch [5], last_lr: 0.00010, train_loss: 0.6198, val_loss: 0.9001, val_acc: 0.6760\n",
      "Epoch [6], last_lr: 0.00012, train_loss: 0.6278, val_loss: 0.8994, val_acc: 0.6780\n",
      "Epoch [7], last_lr: 0.00015, train_loss: 0.6334, val_loss: 0.9001, val_acc: 0.6720\n",
      "Epoch [8], last_lr: 0.00018, train_loss: 0.6244, val_loss: 0.9007, val_acc: 0.6720\n",
      "Epoch [9], last_lr: 0.00021, train_loss: 0.6209, val_loss: 0.9016, val_acc: 0.6780\n",
      "Epoch [10], last_lr: 0.00024, train_loss: 0.6154, val_loss: 0.9019, val_acc: 0.6760\n",
      "Epoch [11], last_lr: 0.00028, train_loss: 0.6179, val_loss: 0.9033, val_acc: 0.6740\n",
      "Epoch [12], last_lr: 0.00031, train_loss: 0.6265, val_loss: 0.9017, val_acc: 0.6740\n",
      "Epoch [13], last_lr: 0.00035, train_loss: 0.6398, val_loss: 0.9044, val_acc: 0.6720\n",
      "Epoch [14], last_lr: 0.00039, train_loss: 0.6102, val_loss: 0.9068, val_acc: 0.6700\n",
      "Epoch [15], last_lr: 0.00043, train_loss: 0.6087, val_loss: 0.9038, val_acc: 0.6780\n",
      "Epoch [16], last_lr: 0.00048, train_loss: 0.6158, val_loss: 0.9020, val_acc: 0.6760\n",
      "Epoch [17], last_lr: 0.00052, train_loss: 0.6439, val_loss: 0.9013, val_acc: 0.6780\n",
      "Epoch [18], last_lr: 0.00056, train_loss: 0.6123, val_loss: 0.9026, val_acc: 0.6740\n",
      "Epoch [19], last_lr: 0.00060, train_loss: 0.6142, val_loss: 0.9069, val_acc: 0.6720\n",
      "Epoch [20], last_lr: 0.00064, train_loss: 0.6324, val_loss: 0.9015, val_acc: 0.6740\n",
      "Epoch [21], last_lr: 0.00068, train_loss: 0.6325, val_loss: 0.9011, val_acc: 0.6720\n",
      "Epoch [22], last_lr: 0.00072, train_loss: 0.6081, val_loss: 0.9026, val_acc: 0.6680\n",
      "Epoch [23], last_lr: 0.00076, train_loss: 0.6178, val_loss: 0.9033, val_acc: 0.6800\n",
      "Epoch [24], last_lr: 0.00079, train_loss: 0.6103, val_loss: 0.9003, val_acc: 0.6800\n",
      "Epoch [25], last_lr: 0.00083, train_loss: 0.6301, val_loss: 0.8996, val_acc: 0.6800\n",
      "Epoch [26], last_lr: 0.00086, train_loss: 0.6296, val_loss: 0.9043, val_acc: 0.6780\n",
      "Epoch [27], last_lr: 0.00089, train_loss: 0.6320, val_loss: 0.9071, val_acc: 0.6780\n",
      "Epoch [28], last_lr: 0.00091, train_loss: 0.6035, val_loss: 0.9021, val_acc: 0.6740\n",
      "Epoch [29], last_lr: 0.00094, train_loss: 0.6319, val_loss: 0.9074, val_acc: 0.6740\n",
      "Epoch [30], last_lr: 0.00095, train_loss: 0.6063, val_loss: 0.9005, val_acc: 0.6760\n",
      "Epoch [31], last_lr: 0.00097, train_loss: 0.6192, val_loss: 0.9055, val_acc: 0.6720\n",
      "Epoch [32], last_lr: 0.00098, train_loss: 0.6070, val_loss: 0.9022, val_acc: 0.6680\n",
      "Epoch [33], last_lr: 0.00099, train_loss: 0.6237, val_loss: 0.9116, val_acc: 0.6660\n",
      "Epoch [34], last_lr: 0.00100, train_loss: 0.6140, val_loss: 0.8994, val_acc: 0.6760\n",
      "Epoch [35], last_lr: 0.00100, train_loss: 0.6096, val_loss: 0.9094, val_acc: 0.6640\n",
      "Epoch [36], last_lr: 0.00100, train_loss: 0.6394, val_loss: 0.9058, val_acc: 0.6700\n",
      "Epoch [37], last_lr: 0.00100, train_loss: 0.6066, val_loss: 0.9033, val_acc: 0.6680\n",
      "Epoch [38], last_lr: 0.00100, train_loss: 0.6207, val_loss: 0.9038, val_acc: 0.6680\n",
      "Epoch [39], last_lr: 0.00099, train_loss: 0.6227, val_loss: 0.9006, val_acc: 0.6740\n",
      "Epoch [40], last_lr: 0.00099, train_loss: 0.6272, val_loss: 0.9074, val_acc: 0.6620\n",
      "Epoch [41], last_lr: 0.00099, train_loss: 0.6162, val_loss: 0.9007, val_acc: 0.6680\n",
      "Epoch [42], last_lr: 0.00098, train_loss: 0.6174, val_loss: 0.9058, val_acc: 0.6700\n",
      "Epoch [43], last_lr: 0.00098, train_loss: 0.6263, val_loss: 0.9036, val_acc: 0.6780\n",
      "Epoch [44], last_lr: 0.00097, train_loss: 0.6110, val_loss: 0.8991, val_acc: 0.6800\n",
      "Epoch [45], last_lr: 0.00097, train_loss: 0.6199, val_loss: 0.9086, val_acc: 0.6760\n",
      "Epoch [46], last_lr: 0.00096, train_loss: 0.6116, val_loss: 0.9000, val_acc: 0.6720\n",
      "Epoch [47], last_lr: 0.00095, train_loss: 0.6290, val_loss: 0.9127, val_acc: 0.6820\n",
      "Epoch [48], last_lr: 0.00094, train_loss: 0.6172, val_loss: 0.9016, val_acc: 0.6700\n",
      "Epoch [49], last_lr: 0.00093, train_loss: 0.5959, val_loss: 0.9103, val_acc: 0.6740\n",
      "Epoch [50], last_lr: 0.00092, train_loss: 0.6029, val_loss: 0.9025, val_acc: 0.6760\n",
      "Epoch [51], last_lr: 0.00091, train_loss: 0.6075, val_loss: 0.9111, val_acc: 0.6740\n",
      "Epoch [52], last_lr: 0.00090, train_loss: 0.6040, val_loss: 0.9019, val_acc: 0.6760\n",
      "Epoch [53], last_lr: 0.00089, train_loss: 0.6025, val_loss: 0.9027, val_acc: 0.6700\n",
      "Epoch [54], last_lr: 0.00088, train_loss: 0.6216, val_loss: 0.9044, val_acc: 0.6780\n",
      "Epoch [55], last_lr: 0.00087, train_loss: 0.6114, val_loss: 0.8974, val_acc: 0.6760\n",
      "Epoch [56], last_lr: 0.00085, train_loss: 0.6045, val_loss: 0.9011, val_acc: 0.6720\n",
      "Epoch [57], last_lr: 0.00084, train_loss: 0.5945, val_loss: 0.8994, val_acc: 0.6740\n",
      "Epoch [58], last_lr: 0.00083, train_loss: 0.6114, val_loss: 0.9105, val_acc: 0.6720\n",
      "Epoch [59], last_lr: 0.00081, train_loss: 0.6155, val_loss: 0.8985, val_acc: 0.6720\n",
      "Epoch [60], last_lr: 0.00080, train_loss: 0.6178, val_loss: 0.9076, val_acc: 0.6640\n",
      "Epoch [61], last_lr: 0.00078, train_loss: 0.6215, val_loss: 0.9020, val_acc: 0.6720\n",
      "Epoch [62], last_lr: 0.00077, train_loss: 0.6015, val_loss: 0.8996, val_acc: 0.6780\n",
      "Epoch [63], last_lr: 0.00075, train_loss: 0.6137, val_loss: 0.9020, val_acc: 0.6700\n",
      "Epoch [64], last_lr: 0.00073, train_loss: 0.6048, val_loss: 0.9045, val_acc: 0.6660\n",
      "Epoch [65], last_lr: 0.00072, train_loss: 0.6207, val_loss: 0.9031, val_acc: 0.6700\n",
      "Epoch [66], last_lr: 0.00070, train_loss: 0.6129, val_loss: 0.9007, val_acc: 0.6680\n",
      "Epoch [67], last_lr: 0.00068, train_loss: 0.5898, val_loss: 0.8984, val_acc: 0.6680\n",
      "Epoch [68], last_lr: 0.00067, train_loss: 0.6075, val_loss: 0.9069, val_acc: 0.6680\n",
      "Epoch [69], last_lr: 0.00065, train_loss: 0.5972, val_loss: 0.9020, val_acc: 0.6680\n",
      "Epoch [70], last_lr: 0.00063, train_loss: 0.5967, val_loss: 0.9025, val_acc: 0.6720\n",
      "Epoch [71], last_lr: 0.00061, train_loss: 0.6006, val_loss: 0.9038, val_acc: 0.6720\n",
      "Epoch [72], last_lr: 0.00059, train_loss: 0.6211, val_loss: 0.9007, val_acc: 0.6740\n",
      "Epoch [73], last_lr: 0.00057, train_loss: 0.6160, val_loss: 0.9008, val_acc: 0.6780\n",
      "Epoch [74], last_lr: 0.00056, train_loss: 0.5956, val_loss: 0.9025, val_acc: 0.6700\n",
      "Epoch [75], last_lr: 0.00054, train_loss: 0.5972, val_loss: 0.9026, val_acc: 0.6700\n",
      "Epoch [76], last_lr: 0.00052, train_loss: 0.6297, val_loss: 0.9024, val_acc: 0.6700\n",
      "Epoch [77], last_lr: 0.00050, train_loss: 0.5957, val_loss: 0.9015, val_acc: 0.6740\n",
      "Epoch [78], last_lr: 0.00048, train_loss: 0.6012, val_loss: 0.9042, val_acc: 0.6700\n",
      "Epoch [79], last_lr: 0.00046, train_loss: 0.6293, val_loss: 0.9031, val_acc: 0.6700\n",
      "Epoch [80], last_lr: 0.00044, train_loss: 0.6227, val_loss: 0.8981, val_acc: 0.6740\n",
      "Epoch [81], last_lr: 0.00043, train_loss: 0.6179, val_loss: 0.9004, val_acc: 0.6720\n",
      "Epoch [82], last_lr: 0.00041, train_loss: 0.6050, val_loss: 0.9034, val_acc: 0.6700\n",
      "Epoch [83], last_lr: 0.00039, train_loss: 0.6214, val_loss: 0.9032, val_acc: 0.6700\n",
      "Epoch [84], last_lr: 0.00037, train_loss: 0.5858, val_loss: 0.8982, val_acc: 0.6760\n",
      "Epoch [85], last_lr: 0.00035, train_loss: 0.6050, val_loss: 0.8973, val_acc: 0.6760\n",
      "Epoch [86], last_lr: 0.00033, train_loss: 0.6038, val_loss: 0.9005, val_acc: 0.6680\n",
      "Epoch [87], last_lr: 0.00032, train_loss: 0.5982, val_loss: 0.9038, val_acc: 0.6660\n",
      "Epoch [88], last_lr: 0.00030, train_loss: 0.6084, val_loss: 0.9049, val_acc: 0.6680\n",
      "Epoch [89], last_lr: 0.00028, train_loss: 0.5844, val_loss: 0.9038, val_acc: 0.6740\n",
      "Epoch [90], last_lr: 0.00027, train_loss: 0.6046, val_loss: 0.9030, val_acc: 0.6740\n",
      "Epoch [91], last_lr: 0.00025, train_loss: 0.6056, val_loss: 0.9009, val_acc: 0.6720\n",
      "Epoch [92], last_lr: 0.00023, train_loss: 0.6127, val_loss: 0.9012, val_acc: 0.6680\n",
      "Epoch [93], last_lr: 0.00022, train_loss: 0.6092, val_loss: 0.8993, val_acc: 0.6740\n",
      "Epoch [94], last_lr: 0.00020, train_loss: 0.6190, val_loss: 0.9005, val_acc: 0.6720\n",
      "Epoch [95], last_lr: 0.00019, train_loss: 0.6030, val_loss: 0.9004, val_acc: 0.6700\n",
      "Epoch [96], last_lr: 0.00017, train_loss: 0.5901, val_loss: 0.9017, val_acc: 0.6700\n",
      "Epoch [97], last_lr: 0.00016, train_loss: 0.6185, val_loss: 0.9027, val_acc: 0.6700\n",
      "Epoch [98], last_lr: 0.00015, train_loss: 0.5923, val_loss: 0.9012, val_acc: 0.6700\n",
      "Epoch [99], last_lr: 0.00013, train_loss: 0.6082, val_loss: 0.9006, val_acc: 0.6720\n",
      "Epoch [100], last_lr: 0.00012, train_loss: 0.6101, val_loss: 0.8987, val_acc: 0.6720\n",
      "Epoch [101], last_lr: 0.00011, train_loss: 0.5823, val_loss: 0.8991, val_acc: 0.6700\n",
      "Epoch [102], last_lr: 0.00010, train_loss: 0.6155, val_loss: 0.8987, val_acc: 0.6700\n",
      "Epoch [103], last_lr: 0.00009, train_loss: 0.5921, val_loss: 0.9006, val_acc: 0.6660\n",
      "Epoch [104], last_lr: 0.00008, train_loss: 0.5955, val_loss: 0.9015, val_acc: 0.6680\n",
      "Epoch [105], last_lr: 0.00007, train_loss: 0.5844, val_loss: 0.9002, val_acc: 0.6660\n",
      "Epoch [106], last_lr: 0.00006, train_loss: 0.5946, val_loss: 0.9005, val_acc: 0.6680\n",
      "Epoch [107], last_lr: 0.00005, train_loss: 0.5952, val_loss: 0.9003, val_acc: 0.6660\n",
      "Epoch [108], last_lr: 0.00004, train_loss: 0.6013, val_loss: 0.8997, val_acc: 0.6680\n",
      "Epoch [109], last_lr: 0.00003, train_loss: 0.6327, val_loss: 0.8993, val_acc: 0.6680\n",
      "Epoch [110], last_lr: 0.00003, train_loss: 0.5999, val_loss: 0.8994, val_acc: 0.6680\n",
      "Epoch [111], last_lr: 0.00002, train_loss: 0.6108, val_loss: 0.9000, val_acc: 0.6680\n",
      "Epoch [112], last_lr: 0.00002, train_loss: 0.6029, val_loss: 0.9014, val_acc: 0.6680\n",
      "Epoch [113], last_lr: 0.00001, train_loss: 0.6105, val_loss: 0.9009, val_acc: 0.6680\n",
      "Epoch [114], last_lr: 0.00001, train_loss: 0.6164, val_loss: 0.9012, val_acc: 0.6660\n",
      "Epoch [115], last_lr: 0.00001, train_loss: 0.5959, val_loss: 0.9009, val_acc: 0.6680\n",
      "Epoch [116], last_lr: 0.00000, train_loss: 0.6321, val_loss: 0.9009, val_acc: 0.6680\n",
      "Epoch [117], last_lr: 0.00000, train_loss: 0.6178, val_loss: 0.9010, val_acc: 0.6700\n",
      "Epoch [118], last_lr: 0.00000, train_loss: 0.6001, val_loss: 0.9001, val_acc: 0.6700\n",
      "Epoch [119], last_lr: 0.00000, train_loss: 0.5993, val_loss: 0.8995, val_acc: 0.6720\n"
     ]
    }
   ],
   "source": [
    "# Fitting the first 1/4 epochs\n",
    "#current_time=time.time()\n",
    "history += fit_one_cycle(int(epochs), max_lr, model20To100, trainloader, testloader, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
